# From RISC-V Cycles to Polynomial Equations: The Conceptual Bridge

**Document Purpose**: Show exactly how concrete CPU execution (registers, memory, instructions) becomes abstract polynomial equations in Jolt's verification.

**Read this if**: You've read [01_Jolt_Theory_Enhanced.md](01_Jolt_Theory_Enhanced.md) and want to see how those protocols (Spartan, Twist, Shout) actually connect to RISC-V execution.

**You should already know**:
- Basic RISC-V (PC, registers, instructions)
- What Spartan, Twist, and Shout protocols do (from doc 01)
- Multilinear extensions and sumcheck basics

**You will learn**: The exact transformation from `RISCVCycle` data structures to sumcheck equations, with concrete examples

**Position in doc series**:
- **[01_Jolt_Theory_Enhanced.md](01_Jolt_Theory_Enhanced.md)**: Deep math - protocol specifications, security proofs - **READ FIRST**
- **This doc (01.5)**: Conceptual bridge - execution to math - **YOU ARE HERE**
- **[02_Jolt_Complete_Guide.md](02_Jolt_Complete_Guide.md)**: Implementation - code paths, file organization
- **[03_Verifier_Mathematics_and_Code.md](03_Verifier_Mathematics_and_Code.md)**: Verifier-specific - line-by-line verification logic

---

## The Central Question

**When you see this in the emulator**:
```rust
RISCVCycle {
    pc: 0x80000014,
    instruction: MUL,
    rs1: (a0, 2),    // Read register a0, value 2
    rs2: (s0, 3),    // Read register s0, value 3
    rd: (a0, 6),     // Write register a0, value 6
}
```

**What equations are proven in the verification?**

This document answers that question with mathematical precision.

---

## Part 1: The Transformation Pipeline

### Step 1: Execution Trace (Concrete)

Emulator outputs a vector of cycles:

```rust
trace: Vec<RISCVCycle> = [
    // Cycle 0: ADDI sp, sp, -16  (adjust stack pointer)
    RISCVCycle {
        pc: 0x80000000,
        instruction: ADDI { opcode: 0x13, rs1: 2, imm: -16, rd: 2 },
        rs1_val: 0x7FFFFFE0,  // rs1 = "source register 1" (x2/sp before)
        rd_val: 0x7FFFFFD0,   // rd = "destination register" (x2/sp after = rs1_val + (-16))
        memory: None,          // No memory access (register-only operation)
    },

    // Cycle 1: SD ra, 8(sp)  (store return address to stack)
    RISCVCycle {
        pc: 0x80000004,
        instruction: SD { opcode: 0x23, rs1: 2, rs2: 1, offset: 8 },
        rs1_val: 0x7FFFFFD0,  // rs1 = "source register 1" (x2/sp = address base)
        rs2_val: 0x0,         // rs2 = "source register 2" (x1/ra = value to store)
        memory: Some(MemoryOp::Write {
            address: 0x7FFFFFD8,  // Store to: rs1_val + offset = 0x7FFFFFD0 + 8
            value: 0x0,           // Store value: rs2_val (return address = 0)
        }),
        // Note: SD doesn't write to rd (no rd_val)
    },

    // Cycle 2: ADDI a0, x0, 3  (load immediate 3 into a0)
    RISCVCycle {
        pc: 0x80000008,
        instruction: ADDI { opcode: 0x13, rs1: 10, imm: 3, rd: 10 },
        rs1_val: 0x0,  // rs1 = x10/a0 (before)
        rd_val: 0x3,   // rd = x10/a0 (after = rs1_val + 3)
        memory: None,
    },

    // Cycle 3: MUL a0, a0, s0  (multiply a0 by s0)
    RISCVCycle {
        pc: 0x80000014,
        instruction: MUL { opcode: 0x33, rs1: 10, rs2: 8, rd: 10 },
        rs1_val: 0x2,  // rs1 = x10/a0 (first operand)
        rs2_val: 0x3,  // rs2 = x8/s0 (second operand)
        rd_val: 0x6,   // rd = x10/a0 (result = rs1_val * rs2_val)
        memory: None,
    },

    // ... continues for T cycles ...
]
```

**RISC-V Register Naming Convention**:
- **rs1** = "source register 1" (first input, register **index** like 2, 10, 8)
- **rs2** = "source register 2" (second input, for R-type instructions)
- **rd** = "destination register" (where result is written)
- **rs1_val, rs2_val, rd_val** = actual **values** from those registers

**Register Index -> Name mapping**:
- x0 = zero (always 0)
- x1 = ra (return address)
- x2 = sp (stack pointer)
- x8 = s0 (saved register 0)
- x10 = a0 (argument/return value 0)

**Memory operations**:
- `None` = No memory access this cycle (register-only)
- `Some(Write { address, value })` = Store `value` to RAM at `address`
- `Some(Read { address, value })` = Load `value` from RAM at `address`

**Key observation**: This is concrete, sequential data - no math yet.

---

### Step 2: Decompose into Traces (Preprocessing)

The prover **separates** the monolithic trace into component-specific traces:

```rust
// PC trace (just the program counters)
pc_trace: Vec<u64> = [
    0x80000000,  // Cycle 0
    0x80000004,  // Cycle 1
    0x80000008,  // Cycle 2
    0x80000014,  // Cycle 3
    // ...
]

// Instruction bytes trace
instruction_trace: Vec<u32> = [
    0xFF010113,  // Cycle 0: ADDI sp, sp, -16
    0x00113423,  // Cycle 1: SD ra, 8(sp)
    0x00300513,  // Cycle 2: ADDI a0, x0, 3
    0x028502B3,  // Cycle 3: MUL a0, a0, s0
    // ...
]

// Register read trace (address, value pairs)
register_reads_rs1: Vec<(u8, u64)> = [
    (2, 0x7FFFFFE0),   // Cycle 0: read x2 (sp)
    (2, 0x7FFFFFD0),   // Cycle 1: read x2 (sp)
    (10, 0x0),         // Cycle 2: read x10 (a0)
    (10, 0x2),         // Cycle 3: read x10 (a0)
    // ...
]

register_reads_rs2: Vec<(u8, u64)> = [
    (0, 0x0),          // Cycle 0: no rs2 (immediate inst)
    (1, 0x0),          // Cycle 1: read x1 (ra)
    (0, 0x0),          // Cycle 2: no rs2
    (8, 0x3),          // Cycle 3: read x8 (s0)
    // ...
]

// Register write trace
register_writes: Vec<(u8, u64)> = [
    (2, 0x7FFFFFD0),   // Cycle 0: write x2
    (0, 0x0),          // Cycle 1: no write (SD doesn't write reg)
    (10, 0x3),         // Cycle 2: write x10
    (10, 0x6),         // Cycle 3: write x10
    // ...
]

// Memory operations trace
memory_trace: Vec<Option<MemoryOp>> = [
    None,                               // Cycle 0
    Some(Write(0x7FFFFFD8, 0x0)),      // Cycle 1
    None,                               // Cycle 2
    None,                               // Cycle 3
    // ...
]
```

**Key observation**: Still concrete vectors, but now **organized by component**.

---

### Step 3: Encode as Polynomials (Mathematical)

Each trace becomes a **multilinear polynomial** over the Boolean hypercube $\{0,1\}^{\log T}$.

#### Example: PC Trace -> PC Polynomial

**Trace (4 cycles, so $n = \log_2(4) = 2$ variables)**:
```
pc_trace = [0x80000000, 0x80000004, 0x80000008, 0x80000014]
```

**Function on Boolean hypercube**:
```
f: {0,1}² -> F

f(0, 0) = 0x80000000   [cycle 0 = binary 00]
f(0, 1) = 0x80000004   [cycle 1 = binary 01]
f(1, 0) = 0x80000008   [cycle 2 = binary 10]
f(1, 1) = 0x80000014   [cycle 3 = binary 11]
```

**Multilinear extension (MLE)**:

The unique multilinear polynomial $\widetilde{\text{PC}}(x_1, x_2)$ that agrees with $f$ on $\{0,1\}^2$:

$$
\widetilde{\text{PC}}(x_1, x_2) = \sum_{(b_1, b_2) \in \{0,1\}^2} \text{pc\_trace}[b_1 2 + b_2] \cdot \text{eq}((x_1, x_2), (b_1, b_2))
$$

where $\text{eq}((x_1, x_2), (b_1, b_2)) = (x_1 b_1 + (1-x_1)(1-b_1)) \cdot (x_2 b_2 + (1-x_2)(1-b_2))$

**Expanded form** (for 4-cycle example):
$$
\begin{align*}
\widetilde{\text{PC}}(x_1, x_2) = &\ 0x80000000 \cdot (1-x_1)(1-x_2) \\
&+ 0x80000004 \cdot (1-x_1) x_2 \\
&+ 0x80000008 \cdot x_1 (1-x_2) \\
&+ 0x80000014 \cdot x_1 x_2
\end{align*}
$$

**Verification**:
- $\widetilde{\text{PC}}(0, 0) = 0x80000000 \cdot 1 + \ldots = 0x80000000$ OK
- $\widetilde{\text{PC}}(0, 1) = 0x80000004 \cdot 1 + \ldots = 0x80000004$ OK
- Can evaluate at **any** point (e.g., $\widetilde{\text{PC}}(0.5, 0.7)$ is well-defined)

**Key observation**: We've moved from discrete data to **continuous polynomial**. This enables sumcheck.

---

#### All Traces -> All Polynomials

**Complete witness polynomials** (for T cycles, $n = \log T$ variables):

| Trace | MLE | Domain | Evaluation Example |
|-------|-----|--------|-------------------|
| `pc_trace[t]` | $\widetilde{\text{PC}}(x_1, \ldots, x_n)$ | $\{0,1\}^n \to \mathbb{F}$ | $\widetilde{\text{PC}}(0,0,1) = $ PC at cycle 1 |
| `instruction_trace[t]` | $\widetilde{\text{Instr}}(x_1, \ldots, x_n)$ | $\{0,1\}^n \to \mathbb{F}$ | $\widetilde{\text{Instr}}(1,1,0) = $ Instruction at cycle 6 |
| `register_reads_rs1[t]` | $\widetilde{\text{RegAddr}}_{rs1}(x_1, \ldots, x_n)$ | $\{0,1\}^n \to \{0, \ldots, 31\}$ | Register address for rs1 |
| | $\widetilde{\text{RegVal}}_{rs1}(x_1, \ldots, x_n)$ | $\{0,1\}^n \to \mathbb{F}$ | Register value for rs1 |
| `register_reads_rs2[t]` | $\widetilde{\text{RegAddr}}_{rs2}, \widetilde{\text{RegVal}}_{rs2}$ | Similar | |
| `register_writes[t]` | $\widetilde{\text{RegAddr}}_{rd}, \widetilde{\text{RegVal}}_{rd}$ | Similar | |
| `memory_trace[t]` | $\widetilde{\text{MemAddr}}(x_1, \ldots, x_n)$ | $\{0,1\}^n \to \mathbb{F}$ | Memory address (if accessed) |
| | $\widetilde{\text{MemVal}}(x_1, \ldots, x_n)$ | $\{0,1\}^n \to \mathbb{F}$ | Memory value (if accessed) |

**Additionally** (derived, not directly from trace):

| Virtual Polynomial | Construction | Purpose |
|-------------------|--------------|---------|
| $\widetilde{\text{Bytecode}}(i)$ | MLE of program bytecode | Verify instruction fetch |
| $\widetilde{\text{InstrOutput}}(x_1, \ldots, x_n)$ | From instruction lookups | Verify computation correctness |
| $\widetilde{\text{JumpFlag}}(x_1, \ldots, x_n)$ | From instruction opcode | R1CS constraints |

**Key observation**: Every piece of trace data now exists as a polynomial. Next: prove relationships between them.

---

#### Committed vs. Virtual Polynomials

**Critical distinction**: Not all polynomials are committed using Dory PCS!

**Committed polynomials** (explicitly bound via Dory PCS cryptographic commitment):

**Note**: These are the ONLY polynomials directly committed. Everything else is either public data or virtual (computed on-the-fly during sumcheck).

| Polynomial | What It Encodes | Component | Count |
|------------|-----------------|-----------|-------|
| $\widetilde{\text{RdInc}}(t)$ | Register value increments (written to rd each cycle) | Registers (Twist) | 1 |
| $\widetilde{\text{RamInc}}(t)$ | RAM value increments (written to memory each cycle) | RAM (Twist) | 1 |
| $\widetilde{\text{InstructionRa}}_i(t)$ | Instruction lookup addresses (one-hot, chunked into d pieces) | Instructions (Shout) | 16 (d=16 for 128-bit indices) |
| $\widetilde{\text{BytecodeRa}}_i(k, t)$ | Bytecode read address (one-hot, chunked) | Bytecode (Shout) | ~3-4 (d varies by program) |
| $\widetilde{\text{RamRa}}_i(k, t)$ | RAM address (one-hot, chunked) | RAM (Twist) | ~8 (d varies by RAM size) |

**Total**: Approximately 28-30 committed polynomials (exact count depends on dynamic `d` parameters for RAM and bytecode chunking).

**What is `d` (chunking parameter)?**
- **Purpose**: Splits large address spaces into smaller, independent chunks to keep polynomial commitment sizes manageable
- **How it works**: Instead of committing to one huge one-hot polynomial over K addresses, commit to d smaller polynomials over K^(1/d) addresses each
- **Example**: RAM with K = 2^24 addresses uses d = 3, so each chunk handles 2^8 = 256 addresses (instead of one polynomial over 16M addresses)
- **Formula**: Access matrix virtualized as $\widetilde{\mathbf{ra}}(x, j) = \prod_{i=1}^{d} \widetilde{\mathbf{ra}}_i(x_i, j)$ where each chunk $\widetilde{\mathbf{ra}}_i$ is independent
- **Target**: Keep K^(1/d) ≈ 2^8 = 256 for optimal commitment and sumcheck efficiency
- **Trade-off**: Larger d → more committed polynomials → more sumcheck rounds, but each polynomial is smaller
- **Dynamic values**:
  - Registers: d = 1 (K = 64 is already small)
  - Bytecode: d = 3-4 (depends on program size)
  - RAM: d = 3-4 (depends on memory usage, dynamically chosen)
- See theory doc lines 3829-3849, 4348-4354, 4463-4467 for full details

**Critical implementation note**:
- **Committed with Dory**: ONLY the 5 types above
- **Virtual (computed during sumcheck, never committed)**:
  - Register addresses: rs1_ra(k,t), rs2_ra(k,t), rd_wa(k,t)
  - Register values: rs1_val(t), rs2_val(t), rd_write_val(t)
  - RAM values: ram_read_val(t), ram_write_val(t)
  - Instruction operands: left_instruction_input(t), right_instruction_input(t)
  - All R1CS inputs from `JoltR1CSInputs` enum (36 different virtual polynomials)
  - Circuit flags and control flow signals

**Why register addresses aren't committed** (key optimization):
- Register indices (rs1, rs2, rd) are hardcoded in bytecode
- Soundness of bytecode read-checking already ensures correct register addresses
- Therefore rs1_ra/rs2_ra/rd_wa can be **virtualized**: computed from bytecode on-the-fly
- Formula: `rs1_ra(k, t) = Σ_j eq(t, j) · [bytecode[j].rs1 == k ? 1 : 0]`
- Saves 3 × d polynomial commitments (significant prover cost reduction)
- See CLAUDE.md lines 574-577 for full explanation

**Public data** (not committed, given to verifier in plain):

| Data | What It Contains | When Shared |
|------|------------------|-------------|
| $\text{Bytecode}[i]$ | Actual instruction bytes at index $i$ | Preprocessing (once per program) |
| $\text{RAM}_{\text{init}}$ | Initial RAM state (program code + static data) | Preprocessing |
| $\text{MemoryLayout}$ | Memory region boundaries (inputs, outputs, stack, etc.) | Preprocessing |

**Virtual polynomials** (computed on-the-fly during sumcheck, never committed):

**Spartan (R1CS) virtual polynomials**:
| Virtual Polynomial | Defined As | Purpose |
|-------------------|------------|---------|
| $(\widetilde{Az})(x)$ | $\sum_{y} \widetilde{A}(x,y) \cdot \widetilde{z}(y)$ | Intermediate product in Spartan sumcheck phase 1 |
| $(\widetilde{Bz})(x)$ | $\sum_{y} \widetilde{B}(x,y) \cdot \widetilde{z}(y)$ | Intermediate product in Spartan sumcheck phase 1 |
| $(\widetilde{Cz})(x)$ | $\sum_{y} \widetilde{C}(x,y) \cdot \widetilde{z}(y)$ | Intermediate product in Spartan sumcheck phase 1 |
| All 36 from `JoltR1CSInputs` enum | See `r1cs/inputs.rs` lines 33-58 | PC, register values, flags, operands, etc. |

**Register (Twist) virtual polynomials**:
| Virtual Polynomial | Defined As | Purpose |
|-------------------|------------|---------|
| $\widetilde{\text{rs1\_ra}}(k, t)$ | Virtualized from bytecode: which register is rs1 at cycle t | Register read address (one-hot) |
| $\widetilde{\text{rs2\_ra}}(k, t)$ | Virtualized from bytecode: which register is rs2 at cycle t | Register read address (one-hot) |
| $\widetilde{\text{rd\_wa}}(k, t)$ | Virtualized from bytecode: which register is rd at cycle t | Register write address (one-hot) |
| $\widetilde{\text{Val}}_{\text{reg}}(k, t)$ | Accumulated register state via increments | Register value at (k,t) |
| $\widetilde{\text{rs1\_val}}(t)$ | $\sum_{k} \text{rs1\_ra}(k,t) \cdot \text{Val}(k,t)$ | Actual value read from rs1 |
| $\widetilde{\text{rs2\_val}}(t)$ | $\sum_{k} \text{rs2\_ra}(k,t) \cdot \text{Val}(k,t)$ | Actual value read from rs2 |
| $\widetilde{\text{rd\_write\_val}}(t)$ | $\sum_{k} \text{rd\_wa}(k,t) \cdot [\text{Val}(k,t) + \text{RdInc}(t)]$ | Actual value written to rd |

**RAM (Twist) virtual polynomials**:
| Virtual Polynomial | Defined As | Purpose |
|-------------------|------------|---------|
| $\widetilde{\text{Val}}_{\text{mem}}(k, t)$ | Accumulated RAM state via increments | RAM value at address k before cycle t |
| $\widetilde{\text{ram\_read\_val}}(t)$ | Computed from RamRa and Val | Actual value read from memory |
| $\widetilde{\text{ram\_write\_val}}(t)$ | Computed from RamRa, Val, and RamInc | Actual value written to memory |

**Instruction (Shout) virtual polynomials**:
| Virtual Polynomial | Defined As | Purpose |
|-------------------|------------|---------|
| $\widetilde{\text{lookup\_output}}(t)$ | From Shout evaluation at InstructionRa indices | Result of instruction lookup table |
| $\widetilde{\text{left\_operand}}(t)$ | Extracted from trace | Left operand for instruction |
| $\widetilde{\text{right\_operand}}(t)$ | Extracted from trace | Right operand for instruction |

**Bytecode (Shout) virtual polynomials**:
| Virtual Polynomial | Defined As | Purpose |
|-------------------|------------|---------|
| opcode(t), rs1(t), rs2(t), rd(t) | Decoded instruction fields at cycle t (read values from bytecode) | The fields prover claims to decode from bytecode |
| imm(t) | Immediate value field | For I-type, S-type, B-type instructions |
| Circuit flags(t) | Derived boolean flags | jump_flag, branch_flag, load_flag, etc. |

**Note**: These are "read values" (rv) from the bytecode Shout instance. Bytecode read-checking proves: for each cycle t, the claimed fields match $\text{Bytecode}[\text{PC}_t]$ (the actual committed bytecode at that PC).

**Why this distinction matters**:

1. **Proof size**: Committed polynomials add ~48 bytes each to proof (Dory commitment)
2. **Prover cost**: Commitments require expensive MSMs (multi-scalar multiplications)
3. **Virtual polynomials are "free"**: Proven via sumcheck over committed polynomials
4. **Design principle**: Minimize commitments by virtualizing intermediate values

**Example flow for register read**:

```
Committed:
  - ra_rs1(k, t) [one-hot encoding: which register?]
  - Inc(t) [what value increment?]

Virtual (computed during sumcheck):
  - f(k, t) = Σ wa(k,t') · Inc(t') · LT(t',t)  [memory state]
  - rv(t) = Σ ra(k,t) · f(k,t)                 [read value]

Result:
  - Prover commits to 2 polynomials, not 4
  - Verifier convinced rv(t) is correct via sumcheck
```

**Key insight from doc 01 § 7.213**:
> Instead of committing to $(\widetilde{Az})$, $(\widetilde{Bz})$, $(\widetilde{Cz})$ (3 expensive commitments), we prove their values via sum-check over committed $\widetilde{z}$ (1 commitment).

---

## Part 2: The Five Properties and Their Equations

For each cycle, Jolt proves **five properties**. Let's see the exact equations for **Cycle 3** (the MUL instruction):

```rust
// Cycle 3 concrete data
pc = 0x80000014
instruction_bytes = 0x028502B3  (MUL a0, a0, s0)
rs1 = (addr: 10, val: 2)
rs2 = (addr: 8, val: 3)
rd = (addr: 10, val: 6)
memory = None
```

---

### Property 1: Bytecode Correctness

**Claim**: "The instruction fetched at cycle 3 matches the bytecode committed during preprocessing"

---

#### Step 0: Who Knows What? (The Trust Model)

**Critical distinction**: Bytecode vs Instruction trace

**Bytecode** (preprocessing - public to both parties):
- ** **Given to verifier in plain**: Verifier receives full decoded `Vec<Instruction>`
- ** **Given to prover in plain**: Prover has same bytecode (from same ELF file)
- ** **Public information**: The program being verified is known to both parties
- ** **NOT committed with Dory**: Bytecode is small enough to send directly
- ** **Not part of the proof**: Bytecode is in preprocessing, shared once per program

**Instruction trace** (proving - claimed by prover):
- ** **Unknown to verifier**: Verifier doesn't know what instructions executed
- ** **Known by prover**: Prover executed the trace, knows all instructions
- ** **Claimed via witness**: Instruction values are part of prover's witness
- ** **Indirectly committed**: Instructions appear in R1CS witness polynomial `z`
- ** **Part of each proof**: Different for every execution (different inputs)

**The comparison**:
```
Verifier has:
  - Bytecode (given in preprocessing, in plain)
  - Instruction trace commitments (from proof)

Verifier checks:
  "Does claimed execution match known program?"

  Instr(cycle) ?= Bytecode(pc_map(cycle))
       ↑                    ↑
    CLAIMED              KNOWN (public)
    (in witness z)       (in preprocessing)
```

**Why this is secure**:
- Prover cannot change the bytecode (verifier has original copy)
- Prover must prove instruction trace is consistent with public bytecode (via sumcheck)
- Verifier doesn't need to trust prover's execution - just verifies consistency
- Even though bytecode is public, prover can't cheat because sumcheck checks every cycle

**From [03_Verifier_Mathematics_and_Code.md § 1](03_Verifier_Mathematics_and_Code.md#1-verifier-preprocessing-joltverifierpreprocessing)**:
> The verifier receives the **full bytecode in plain**. This makes sense because:
> - The bytecode is **public** (it's the program being verified)
> - Both prover and verifier need to know what program is being executed

---

#### Step 1A: Understanding the Bytecode Polynomial

**What is bytecode?** The committed program, stored as a vector of decoded instructions.

**Concrete example** (our 4-cycle program):

```rust
// Physical memory layout
0x80000000: 0xFF010113  // ADDI sp, sp, -16
0x80000004: 0x00113423  // SD ra, 8(sp)
0x80000008: 0x00300513  // ADDI a0, x0, 3
0x8000000C: 0x02850533  // MUL a0, a0, s0
0x80000010: ...         // More instructions

// Preprocessing: Bytecode vector (with prepended NoOp at index 0)
bytecode: Vec<Instruction> = [
    NoOp,                    // Index 0 (prepended)
    ADDI(sp, sp, -16),      // Index 1 -> PC 0x80000000
    SD(ra, 8, sp),          // Index 2 -> PC 0x80000004
    ADDI(a0, x0, 3),        // Index 3 -> PC 0x80000008
    MUL(a0, a0, s0),        // Index 4 -> PC 0x8000000C
    ...                      // More instructions
]

// PC Mapper: Maps physical address -> bytecode index
pc_map.get_pc(0x80000000) = 1  // First real instruction
pc_map.get_pc(0x80000004) = 2
pc_map.get_pc(0x80000008) = 3
pc_map.get_pc(0x8000000C) = 4  - cycle 3's PC
```

**Bytecode polynomial**: $\widetilde{\text{Bytecode}}(k)$ is MLE of this vector.

$$
\begin{align*}
\widetilde{\text{Bytecode}}(0) &= \text{NoOp} \\
\widetilde{\text{Bytecode}}(1) &= \text{ADDI (encoded as } 0x\text{FF010113)} \\
\widetilde{\text{Bytecode}}(2) &= \text{SD (encoded as } 0x\text{00113423)} \\
\widetilde{\text{Bytecode}}(3) &= \text{ADDI (encoded as } 0x\text{00300513)} \\
\widetilde{\text{Bytecode}}(4) &= \text{MUL (encoded as } 0x\text{02850533)}
\end{align*}
$$

**Key insight**: Bytecode is indexed by **virtual PC** (via pc_map), not physical address!

---

#### Step 1B: How Cycle 3 Maps to Bytecode

**Execution trace**:
```rust
// Cycle 0: PC = 0x80000000 -> bytecode_index = 1
// Cycle 1: PC = 0x80000004 -> bytecode_index = 2
// Cycle 2: PC = 0x80000008 -> bytecode_index = 3
// Cycle 3: PC = 0x8000000C -> bytecode_index = 4   - OUR CYCLE
```

**Mapping**:
```rust
cycle_3_pc = 0x8000000C
bytecode_index = pc_map.get_pc(0x8000000C) = 4
```

**Instruction encoding**: MUL instruction encodes to 32-bit word:
```
MUL a0, a0, s0
  opcode: 0x33 (R-type arithmetic)
  rd: 10 (a0)
  funct3: 0x0 (MUL)
  rs1: 10 (a0)
  rs2: 8 (s0)
  funct7: 0x01 (MULDIV extension)

Encoded: 0x02850533
```

---

#### Step 1C: The Sumcheck Equation

**What we're proving**: "Decoded instruction fields at cycle 3 match bytecode at index 4"

**Actual ingredients from our tables**:

The bytecode Shout instance proves the standard read-checking identity:
$$
\widetilde{\text{rv}}(\tau) = \sum_{k, t} \text{eq}(\tau, t) \cdot \widetilde{\text{BytecodeRa}}(k, t) \cdot \text{Bytecode}[k]
$$

**Breaking this down for cycle 3**:
- $\widetilde{\text{BytecodeRa}}(k, 3)$ - COMMITTED polynomial (one-hot: which bytecode index accessed at cycle 3)
  - $\widetilde{\text{BytecodeRa}}(4, 3) = 1$ (cycle 3 accesses bytecode index 4)
  - $\widetilde{\text{BytecodeRa}}(k, 3) = 0$ for all $k \neq 4$
- $\text{Bytecode}[4] = 0x02850533$ - PUBLIC DATA (actual instruction bytes at index 4)
- $\widetilde{\text{rv}}(3)$ - VIRTUAL polynomial (read value: the decoded fields opcode, rs1, rs2, rd, imm, flags)

**The sumcheck proves**:
$$
\widetilde{\text{rv}}(3) = \sum_{k=0}^{K-1} \widetilde{\text{BytecodeRa}}(k, 3) \cdot \text{Bytecode}[k] = 1 \cdot \text{Bytecode}[4] + 0 \cdot \text{Bytecode}[0] + \ldots = \text{Bytecode}[4]
$$

**Key distinction**:
- $\widetilde{\text{BytecodeRa}}_i(k, t)$: COMMITTED (proves *which* bytecode was accessed)
- $\text{Bytecode}[k]$: PUBLIC DATA (verifier has the actual bytecode)
- $\widetilde{\text{rv}}(t)$ = (opcode, rs1, rs2, rd, imm, flags): VIRTUAL (the *values* read from bytecode)

**Why index 4, not 3?** Bytecode has prepended NoOp at index 0 + cycle 3 is the 4th instruction.

**Actual implementation**: The bytecode Shout instance uses 5 separate "read value" (rv) claims for different stages (Spartan, Product, Shift, Registers, Instructions), each proving a subset of decoded fields match the committed bytecode. See `bytecode/read_raf_checking.rs` for full details.

**Full sumcheck equation** (extending to all T cycles):

The read-checking identity must hold at the random challenge $\tau$:
$$
\widetilde{\text{rv}}(\tau) = \sum_{k=0}^{K-1} \sum_{t=0}^{T-1} \text{eq}(\tau, t) \cdot \widetilde{\text{BytecodeRa}}(k, t) \cdot \text{Bytecode}[k]
$$

**Using actual ingredients from our tables**:
- $\widetilde{\text{BytecodeRa}}_i(k, t)$: COMMITTED polynomials (d chunks)
- $\text{Bytecode}[k]$: PUBLIC DATA (verifier has decoded instruction bytes)
- $\widetilde{\text{rv}}(\tau)$: VIRTUAL polynomial (claimed read values = opcode, rs1, rs2, rd, imm, flags)

**Note**: Actual implementation splits $\widetilde{\text{rv}}$ into 5 separate rv claims for different stages (Spartan, Product, Shift, Registers, Instructions), proved together using gamma-weighted batching. See `bytecode/read_raf_checking.rs` and verifier doc line 4426.

---

#### Step 1D: How the Equation Catches Cheating

**Honest prover** (cycle 3 contribution):
$$
\text{eq}(\tau, 3) \cdot [0x\text{02850533} - 0x\text{02850533}] = \text{eq}(\tau, 3) \cdot 0 = 0
$$

**Dishonest prover** (claims wrong instruction, e.g., ADD instead of MUL):
```rust
// Prover lies: claims cycle 3 executed ADD (0x00A50533) instead of MUL
Instr(3) = 0x00A50533  (ADD a0, a0, s0 - wrong!)
Bytecode(4) = 0x02850533  (MUL a0, a0, s0 - actual program)

Contribution: eq(τ, 3) · [0x00A50533 - 0x02850533]
            = eq(τ, 3) · (-0x02000000)  ≠ 0
```

**Result**: Sum is non-zero -> sumcheck fails -> proof rejected!

**Why random τ matters**: Prover can't predict which cycle will be "heavily weighted" by eq(τ, ·), so must be honest about ALL cycles.

---

#### Step 1E: Full Read-Checking Protocol (Shout)

**Offline memory checking**: Bytecode is "memory" that's **read-only**.

**Shout read-checking proves**:
1. Every cycle reads from bytecode at claimed PC
2. Read value matches committed bytecode value
3. Uses "access matrix" $\text{ra}(k, t)$ (1 if cycle $t$ reads index $k$)

**Full equation structure**:
$$
\widetilde{\text{rv}}(r') = \sum_{t=0}^{T-1} \sum_{k=0}^{K-1} \text{eq}(r', t) \cdot \widetilde{\text{ra}}(k, t) \cdot \widetilde{\text{Bytecode}}(k)
$$

where:
- $\widetilde{\text{rv}}(r')$ = claimed read values polynomial evaluated at random point $r'$
- $\widetilde{\text{ra}}(k, t)$ = 1 if cycle $t$ reads bytecode index $k$, else 0
- Essentially: "reads at random cycle match bytecode values"

**Connection to verification**:
- **Protocol**: Shout read-checking sumcheck (Stages 3, 4, 6 in JoltDAG)
- **Details**: [01_Jolt_Theory_Enhanced.md § 2.4](01_Jolt_Theory_Enhanced.md#24-bytecode-verification-shout)

---

### Property 2: Instruction Semantics

**Claim**: "MUL(2, 3) = 6 according to the RISC-V specification"

**Challenge**: Cannot commit to full 2^128 MUL table (two 64-bit inputs)

**Jolt's solution**: Decompose into 4-bit chunks

#### Step 2A: Chunk Decomposition

**Input 1** (rs1_val = 2):
```
Binary: 0000...0010
Chunks (4-bit): [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
                 └─ chunk 0 (bits 0-3)
                    └─ chunk 1 (bits 4-7)
                       ... 16 total chunks
```

**Input 2** (rs2_val = 3):
```
Binary: 0000...0011
Chunks (4-bit): [3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```

#### Step 2B: Lookup Each Chunk Pair

**Lookup table** (committed, size 2^8 = 256 entries):
```
MUL_4[a][b] = a * b (mod 16)

Examples:
  MUL_4[2][3] = 6
  MUL_4[0][0] = 0
  MUL_4[15][15] = 225 (mod 16) = 1
```

**Lookups for cycle 3**:
```
Chunk 0: MUL_4[2][3] = 6
Chunk 1: MUL_4[0][0] = 0
Chunk 2: MUL_4[0][0] = 0
...
Chunk 15: MUL_4[0][0] = 0
```

#### Step 2C: Recombination Constraint

**Equation** (checked via R1CS constraints):
$$
\widetilde{\text{LookupOutput}}(3) \stackrel{?}{=} \sum_{c=0}^{15} \text{ChunkLookup}_c(3) \cdot 2^{4c}
$$

where:
- $\widetilde{\text{LookupOutput}}(3)$: VIRTUAL polynomial - the claimed instruction result (from Shout)
- $\text{ChunkLookup}_c(3)$: Result of looking up chunk $c$ in the MUL_4 table
- This is verified via R1CS "wiring" constraints that combine the 16 chunk results

**For cycle 3**:
$$
6 \stackrel{?}{=} 6 \cdot 2^0 + 0 \cdot 2^4 + 0 \cdot 2^8 + \ldots + 0 \cdot 2^{60} = 6 \quad \checkmark
$$

#### Step 2D: Lookup Correctness via Sumcheck

**For each chunk** $c$, prove lookup is correct:

$$
\sum_{t=0}^{T-1} \sum_{k \in \{0,1\}^8} \text{access}(t, c, k) \cdot \left[ \widetilde{\text{ChunkResult}}_c(t) - \widetilde{\text{MUL\_4}}(k) \right] \stackrel{?}{=} 0
$$

where:
- $\text{access}(t, c, k) = 1$ if cycle $t$, chunk $c$ looks up table entry $k$
- $\widetilde{\text{MUL\_4}}(k)$ is MLE of the 256-entry MUL_4 table

**For cycle 3, chunk 0**:
- $k = (2, 3)$ (binary: 00100011)
- $\text{access}(3, 0, k) = 1$ (this is the lookup we perform)
- Term: $1 \cdot [6 - \widetilde{\text{MUL\_4}}(00100011)] = 1 \cdot [6 - 6] = 0$ OK

**Connection to verification**:
- **Protocol**: Shout batch evaluation with prefix-suffix sumcheck (Stage 1B, Stage 2B)
- **Why 128 rounds**: $\log_2(2^{128}) = 128$ for full lookup space
- **Details**: [01_Jolt_Theory_Enhanced.md § 2.2](01_Jolt_Theory_Enhanced.md#22-instruction-lookups-the-shout-protocol)

---

### Property 3: Register Read Consistency

**Claim**: "When cycle 3 reads register a0 (index 10), it gets value 2, which was the most recent write to a0"

**Memory checking intuition**: Prove time-ordered trace is **permutation** of address-ordered trace.

#### Step 3A: Time-Ordered Trace

Registers accessed **in chronological order**:

```
Cycle 0: Read x2=0x7FFFFFE0, Write x2=0x7FFFFFD0
Cycle 1: Read x2=0x7FFFFFD0, Read x1=0x0
Cycle 2: Read x10=0x0, Write x10=0x3
Cycle 3: Read x10=0x2, Read x8=0x3, Write x10=0x6   - Our cycle
Cycle 4: Read x10=0x6, ...
```

**Wait, inconsistency!** Cycle 2 writes x10=3, but cycle 3 reads x10=2?

Let me correct the trace to be consistent:

```
Cycle 0: Read x2=0x7FFFFFE0, Write x2=0x7FFFFFD0
Cycle 1: Read x2=0x7FFFFFD0, Read x1=0x0
Cycle 2: Read x10=0x0, Write x10=0x3
Cycle 2.5: Some instruction writes x10=0x2
Cycle 3: Read x10=0x2, Read x8=0x3, Write x10=0x6   - Reads most recent write
Cycle 4: Read x10=0x6, ...
```

#### Step 3B: Address-Ordered Trace

Same operations **sorted by (register_address, timestamp)**:

```
Register x1:
  - (cycle 1, read, 0x0)

Register x2:
  - (cycle 0, read, 0x7FFFFFE0)
  - (cycle 0, write, 0x7FFFFFD0)
  - (cycle 1, read, 0x7FFFFFD0)

Register x8:
  - (cycle 3, read, 0x3)

Register x10:
  - (cycle 2, read, 0x0)
  - (cycle 2, write, 0x3)
  - (cycle 2.5, write, 0x2)
  - (cycle 3, read, 0x2)   - Adjacent to previous write!
  - (cycle 3, write, 0x6)
  - (cycle 4, read, 0x6)
```

**Key observation**: In address-ordered trace, **reads appear immediately after their corresponding writes**.

#### Step 3C: The Twist Virtualization - Increments Instead of Grand Products

**IMPORTANT**: Jolt does NOT use grand products or fingerprinting! That was the old approach. Twist uses **increment virtualization**.

**Key insight**: Instead of committing to full memory state $f(k, j)$ (value at address $k$ at time $j$), commit only to **increments** - the changes to memory values.

**The virtualization formula**:
$$
f(k, j) = \sum_{j' \in \{0,1\}^{\log T}} \widetilde{\mathbf{wa}}(k, j') \cdot \widetilde{\text{Inc}}(j') \cdot \widetilde{\text{LT}}(j', j)
$$

**Breaking this down**:
- $\widetilde{\mathbf{wa}}(k, j')$: VIRTUAL - selects writes targeting address $k$ at time $j'$
- $\widetilde{\text{Inc}}(j')$: COMMITTED - the increment (new value - old value) at cycle $j'$
- $\widetilde{\text{LT}}(j', j)$: PUBLIC - "less than" function, 1 if $j' < j$, else 0
- Sum accumulates all writes to address $k$ that happened before time $j$

**Why this is better than grand products**:
- Only commits to sparse increments (most cycles write 0 or small values)
- No need for sorting or permutation arguments
- Exploits locality: accessing recently-touched memory is cheaper
- Preserves small values (increments are often small, unlike random field elements from fingerprints)

#### Step 3D: Cycle 3's Contribution via Increments

**Register reads** (rs1 = x10, rs2 = x8):
- Need to verify: $\widetilde{\text{rv}}_{rs1}(3) = f(\text{x10}, 3) = 2$
- Need to verify: $\widetilde{\text{rv}}_{rs2}(3) = f(\text{x8}, 3) = 3$

**Using the virtualization formula for x10**:
$$
f(\text{x10}, 3) = \sum_{j'=0}^{2} \widetilde{\mathbf{wa}}(\text{x10}, j') \cdot \widetilde{\text{Inc}}(j') \cdot \underbrace{\widetilde{\text{LT}}(j', 3)}_{=1 \text{ for } j'<3}
$$

**Trace for x10** (assuming x10 was written with value 2 at cycle 1):
- Cycle 0: No write to x10 → wa(x10, 0) = 0
- Cycle 1: Write 2 to x10 → wa(x10, 1) = 1, Inc(1) = 2
- Cycle 2: No write to x10 → wa(x10, 2) = 0
- Sum: $0 \cdot \text{Inc}(0) + 1 \cdot 2 + 0 \cdot \text{Inc}(2) = 2$ ✓

**Register write** (rd = x10, new value = 6):
- Increment: $\text{Inc}(3) = 6 - 2 = 4$ (new value - old value)
- This increment is COMMITTED via $\widetilde{\text{RdInc}}$ polynomial
- After cycle 3: $f(\text{x10}, 4) = f(\text{x10}, 3) + \text{Inc}(3) = 2 + 4 = 6$

**Connection to verification**:
- **Protocol**: Twist memory checking (Stage 1D, Stage 2D, Stage 3C)
- **Three sumchecks**: Read-checking, write-checking, evaluation
- **Details**: [01_Jolt_Theory_Enhanced.md § 2.3](01_Jolt_Theory_Enhanced.md#23-memory-consistency-the-twist-protocol)

---

### Property 4: RAM Consistency

**Claim**: "If cycle 1 writes 0x0 to address 0x7FFFFFD8, and cycle 10 reads from 0x7FFFFFD8, the read returns 0x0"

**Equation**: Identical structure to register consistency, but:
- Address space: 2^64 possible addresses (vs 32 registers)
- Chunking parameter $d$: Break large address space into manageable pieces
- Currently: $K^{1/d} = 2^8$ (256 chunks per address)

**For cycle 3**: No memory operation, so no contribution to RAM sumcheck.

**Connection to verification**:
- **Protocol**: Twist memory checking (Stage 1E, Stage 2E, Stage 3D)
- **Chunking**: Necessary due to large K
- **Details**: [01_Jolt_Theory_Enhanced.md § 2.3](01_Jolt_Theory_Enhanced.md#23-memory-consistency-the-twist-protocol)

---

### Property 5: R1CS Wiring (PC Update and Data Flow)

**Claims for cycle 3**:
1. "PC updates from 0x80000014 to 0x80000018 (sequential, not jump)"
2. "Instruction output (6) equals register write value (6)"

#### Claim 5A: PC Update

**R1CS constraint**:
$$
\left( \widetilde{\text{PC}}(4) - \widetilde{\text{PC}}(3) - 4 \right) \cdot \left( 1 - \widetilde{\text{JumpFlag}}(3) \right) \stackrel{?}{=} 0
$$

**For cycle 3**:
$$
(0x80000018 - 0x80000014 - 4) \cdot (1 - 0) = (4 - 4) \cdot 1 = 0 \quad \checkmark
$$

**If this was a jump**:
- $\widetilde{\text{JumpFlag}}(3) = 1$
- Constraint becomes $(PC_{next} - PC_{curr} - 4) \cdot 0 = 0$ (trivially satisfied)
- Different constraint checks: $PC_{next} \stackrel{?}{=} \text{jump\_target}$

#### Claim 5B: Data Flow (Copy Constraint)

**R1CS constraint**:
$$
\widetilde{\text{InstrOutput}}(3) - \widetilde{\text{RegVal}}_{rd}(3) \stackrel{?}{=} 0
$$

**For cycle 3**:
$$
6 - 6 = 0 \quad \checkmark
$$

#### Full Spartan Sumcheck

**All ~30 constraints** (per cycle) batched into one equation:

$$
\sum_{t=0}^{T-1} \text{eq}(\tau, t) \cdot \left[ (A_t z_t) \circ (B_t z_t) - C_t z_t \right] \stackrel{?}{=} \mathbf{0}
$$

where:
- $z_t$ is witness vector at cycle $t$ (includes PC, register values, instruction outputs, etc.)
- $A_t, B_t, C_t$ are constraint matrices (sparse, uniform across cycles)
- $\circ$ is Hadamard (element-wise) product

**Cycle 3's row** in this equation includes:
- PC update constraint
- Copy constraints (instruction output -> register write)
- Jump flag constraints
- Immediate value constraints
- ... (~30 total)

**Connection to verification**:
- **Protocol**: Spartan R1CS verification (Stage 1A, Stage 2A, Stage 3A)
- **Three-stage sumcheck**: Outer, product, matrix evaluation
- **Details**: [01_Jolt_Theory_Enhanced.md § 2.1](01_Jolt_Theory_Enhanced.md#21-r1cs-constraints-spartan)

---

## Part 2.5: The Universal Sumcheck Pattern - Understanding the Machinery

Before seeing all the equations together, let's understand the **core pattern** that appears everywhere in Jolt verification.

---

### The Sum-Eq Pattern: Extracting Values at Random Points

**Pattern you'll see constantly**:
$$\sum_{t \in \{0,1\}^{\log T}} f(t) \cdot \widetilde{\text{eq}}(r, t) = \widetilde{f}(r)$$

**What this does**: Extracts the value of polynomial $\widetilde{f}$ at random point $r$ by summing over all Boolean inputs weighted by the eq polynomial.

---

#### Concrete Numerical Example

Let's see this with **actual numbers** for a 4-cycle trace ($T = 4 = 2^2$, so $\log T = 2$ variables).

**PC trace data**:
| Cycle $t$ (binary) | PC value $f(t)$ |
|-------------------|-----------------|
| 00                | 100             |
| 01                | 104             |
| 10                | 108             |
| 11                | 200             |

**Question**: What is $\widetilde{f}(0.7, 0.3)$ - the multilinear extension of PC evaluated at $(r_1, r_2) = (0.7, 0.3)$?

**Using the sum-eq pattern**:
$$\sum_{t \in \{0,1\}^2} f(t) \cdot \widetilde{\text{eq}}((0.7, 0.3), t) = \widetilde{f}(0.7, 0.3)$$

**Recall eq polynomial definition**:
$$\widetilde{\text{eq}}((r_1, r_2), (t_1, t_2)) = (r_1 \cdot t_1 + (1-r_1)(1-t_1)) \cdot (r_2 \cdot t_2 + (1-r_2)(1-t_2))$$

**Compute each eq term**:
1. $\widetilde{\text{eq}}((0.7, 0.3), (0,0)) = (1-0.7)(1-0) \cdot (1-0.3)(1-0) = 0.3 \cdot 0.7 = 0.21$
2. $\widetilde{\text{eq}}((0.7, 0.3), (0,1)) = (1-0.7)(1-0) \cdot (0.3 \cdot 1) = 0.3 \cdot 0.3 = 0.09$
3. $\widetilde{\text{eq}}((0.7, 0.3), (1,0)) = (0.7 \cdot 1) \cdot (1-0.3)(1-0) = 0.7 \cdot 0.7 = 0.49$
4. $\widetilde{\text{eq}}((0.7, 0.3), (1,1)) = (0.7 \cdot 1) \cdot (0.3 \cdot 1) = 0.7 \cdot 0.3 = 0.21$

**The sum**:
$$\begin{align*}
\sum &= f(0,0) \cdot 0.21 + f(0,1) \cdot 0.09 + f(1,0) \cdot 0.49 + f(1,1) \cdot 0.21 \\
&= 100 \cdot 0.21 + 104 \cdot 0.09 + 108 \cdot 0.49 + 200 \cdot 0.21 \\
&= 21 + 9.36 + 52.92 + 42 \\
&= 125.28
\end{align*}$$

**This equals** $\widetilde{f}(0.7, 0.3)$ **by the definition of multilinear extension!**

**Key insight**: The eq polynomial acts as **Lagrange interpolation weights** that combine the 4 corner values into the evaluation at the interior point $(0.7, 0.3)$.

---

### The Universal Pattern in Jolt

You'll encounter this pattern in **every sumcheck** across all 5 properties:

**1. Bytecode checking**:
$$\sum_{k,t} \text{eq}(\tau, t) \cdot \widetilde{\text{BytecodeRa}}(k, t) \cdot \text{Bytecode}[k] = \widetilde{\text{rv}}(\tau)$$
- **Selector**: eq(τ, t) picks out cycle t
- **Data**: BytecodeRa (which index?) × Bytecode (what value?)
- **Claimed value**: rv(τ) (the decoded fields)

**2. Register read-checking (Twist)**:
$$\sum_{j, k} \text{eq}(r', j) \cdot \widetilde{\text{ra}}_{rs1}(k, j) \cdot \widetilde{f}_{\text{reg}}(k, j) = \widetilde{\text{rv}}_{rs1}(r')$$
- **Selector**: eq(r', j) picks cycle, ra picks register
- **Data**: Register memory state f_reg
- **Claimed value**: The value read from rs1

**3. PC shift consistency**:
$$\sum_t \widetilde{\text{PC}}(t) \cdot \widetilde{\text{eq}}_{+1}(r, t) = \widetilde{\text{NextPC}}(r)$$
- **Selector**: eq+1(r, t) picks out t+1 (shifted)
- **Data**: PC values
- **Claimed value**: NextPC polynomial

**4. Product virtualization**:
$$\sum_j \widetilde{\text{Left}}(j) \cdot \widetilde{\text{Right}}(j) \cdot \widetilde{\text{eq}}(r, j) = \widetilde{\text{Product}}(r)$$
- **Selector**: eq(r, j) picks cycle
- **Data**: Left × Right polynomials
- **Claimed value**: Product polynomial

**5. Register value evolution**:
$$\sum_{j'} \widetilde{\text{Inc}}(j') \cdot \widetilde{\text{wa}}(k, j') \cdot \widetilde{\text{LT}}(j', r_{\text{cycle}}) = \widetilde{\text{Val}}(k, r_{\text{cycle}})$$
- **Selector**: LT(j', r_cycle) picks all cycles before r_cycle
- **Data**: Increments × write addresses
- **Claimed value**: Accumulated register value

---

### Why This Pattern is Everywhere

**The universal formula**:
$$\sum_{\text{all points}} \text{data}(\text{point}) \cdot \text{selector}(\text{point}) = \text{claimed\_value}$$

**Components**:
- **Selector** (eq, eq+1, LT): Picks out specific point(s) or ranges
- **Data**: The committed/virtual polynomial being queried
- **Claimed value**: What we're verifying

**Security (Schwartz-Zippel)**: If the claimed value is wrong, the polynomial equality fails at the random challenge point with overwhelming probability ($\geq 1 - T/|\mathbb{F}|$).

**Why random challenges matter**: Prover can't predict which point will be evaluated, so must be honest about the **entire** polynomial (all T cycles), not just specific points.

---

### From Pattern to Verification: The 5 Stages

All five properties use this pattern, organized into 5 verification stages:

| Stage | What It Proves | Key Sumchecks |
|-------|----------------|---------------|
| **1** | R1CS constraints hold (PC updates, data flow) | Spartan outer + 3 inner |
| **2** | Memory reads return correct values (Twist start) | RAM read-check, Register read-check |
| **3** | Memory state evolution is consistent (Twist finish) | RAM val-eval, Register val-eval, PC shift |
| **4** | Lookup addresses and bytecode are correct (Shout finish) | Bytecode read-check, Hamming weights |
| **5** | All committed polynomials match their evaluations (Dory) | Batched opening proof |

**Data flow**:
```
Stage 1 outputs → Stage 2 inputs
Stage 2 outputs → Stage 3 inputs
Stage 3 outputs → Stage 4 inputs
Stage 4 outputs → Stage 5 inputs (all evaluation claims)
Stage 5: Cryptographically verify all claims
```

**Why stages matter**:
- Sumchecks have dependencies (output of one is input to next)
- Batching independent sumchecks in same stage reduces interaction rounds
- Final stage uses expensive cryptography, so batch all openings together

---

## Part 3: The Complete Equation Set for One Cycle

**For Cycle 3** (MUL a0, a0, s0), the verifier checks these five properties. Let's see the complete equations with proper protocol context from [Thaler survey](Thaler_survey.md).

**Notation conventions** (following Thaler survey):
- `x` = **table/memory address** (e.g., bytecode address, lookup index, register index)
- `t` or `j` = **time/cycle index**
- `k` = **register/memory address index** in Twist (sometimes used interchangeably with `x`)
- `c` = **chunk index** (for decomposed operations)

The survey uses `x` for addresses in Shout (Eq. 16), and `k` for addresses in Twist (Eq. 20). We follow this convention.

---

### Understanding the Verifier's Perspective: Committed vs Virtual Polynomials

**Critical insight**: The verifier doesn't compute or receive most polynomials directly. Instead:

1. **Committed polynomials** (~28-30 total): Prover commits using Dory PCS during preprocessing/proving
   - Verifier receives **commitments** (short cryptographic hashes)
   - Later verified via **opening proofs** in Stage 5

2. **Virtual polynomials** (majority): Never committed, never opened
   - Prover **claims** evaluations during sumcheck rounds
   - Verifier **assumes** these claims and reduces them to other claims
   - Eventually all virtual claims reduce to committed polynomial openings

3. **Public data**: Known to both prover and verifier (no commitment needed)
   - Lookup tables `f` (efficiently evaluable MLEs)
   - Helper polynomials like `eq`, `LT`

**The verification flow**:
```
Properties 1-4: Sumchecks that assume virtual polynomials
    ↓
  Reduce to claimed evaluations of virtual + committed polynomials
    ↓
Property 5 (R1CS): Relates virtual polynomials to committed witness z
    ↓
  Stage 5: Batch opening proof verifies all committed polynomial claims
```

**Complete Polynomial Categorization**:

| Polynomial | Type | Where Defined | Why This Type |
|------------|------|---------------|---------------|
| **BytecodeRa** | COMMITTED | Properties 1 | Sparse (T entries), core witness |
| **rv** (bytecode) | VIRTUAL | Property 1 output | Derived from BytecodeRa via Shout |
| **InstructionRa_c** (16 chunks) | COMMITTED | Property 2 | Sparse lookup selectors |
| **rv_c** (chunk results) | VIRTUAL | Property 2 output | Derived from InstructionRa_c via Shout |
| **f_op** (lookup table) | PUBLIC | Property 2 | Efficiently evaluable, known to verifier |
| **LookupOutput** | VIRTUAL | Property 2 | Recombined from rv_c via R1CS |
| **Inc** (register/RAM) | COMMITTED | Property 3, 4 | Core witness (writes) |
| **f(k,j)** (memory state) | VIRTUAL | Property 3, 4 | Derived from Inc via Twist formula |
| **wa** (write address) | VIRTUAL | Property 3, 4 | One-hot, derived from bytecode |
| **ra** (read address) | VIRTUAL (registers) | Property 3 | Derived from bytecode |
| **RamRa_i** (chunked) | COMMITTED | Property 4 | RAM addresses need commitment |
| **LT** (less-than) | PUBLIC | Property 3, 4 | Efficiently evaluable helper |
| **z** (R1CS witness) | COMMITTED | Property 5 | Full witness vector |
| **A, B, C** (matrices) | PUBLIC | Property 5 | Sparse, uniform structure |

**Key observations**:
- Only **~28-30 commitments** for potentially millions of execution cycles!
- Most polynomials are **virtual** - never materialized, only claimed
- R1CS witness `z` **contains** the committed polynomials (Inc, Ra, etc.) as sub-vectors
- Property 5 is where verifier **links** virtual polynomials to committed witness

---

### Complete Equations with Protocol Context

$$
\boxed{
\begin{align*}
&\textbf{Property 1: Bytecode Correctness (Shout Read-Checking)} \\[0.3em]
&\text{Protocol: Shout batch evaluation (survey § 5.1, Eq. 16)} \\
&\text{\textbf{Goal:} Prove instructions fetched from bytecode match committed read addresses} \\[0.3em]
&\text{\textbf{The equation being proven:}} \\
&\boxed{\text{rv}(t) = \text{Bytecode}[\text{PC}_t] \quad \forall t \in [T]} \\
&\text{At each cycle, the read value equals the instruction at that program counter address} \\[0.5em]
%
&\text{\textbf{The sumcheck equation (MLE version):}} \\
&\widetilde{\text{rv}}(\tau) = \sum_{x \in \{0,1\}^{\ell}} \sum_{t \in \{0,1\}^{\log T}} \text{eq}(\tau, t) \cdot \widetilde{\text{BytecodeRa}}(x, t) \cdot \text{Bytecode}[x] \\[0.3em]
&\text{where } x = \text{bytecode address}, \, t = \text{cycle index}, \, \ell = \log(\text{program size}) \\
&\quad \widetilde{\text{BytecodeRa}}(x, t) = \text{one-hot read address (\textbf{COMMITTED})} \\
&\quad \text{Bytecode}[x] = \text{decoded instruction data (\textbf{PUBLIC}, from preprocessing)} \\
&\quad \quad \text{Contains: opcode, rs1, rs2, rd, immediate, circuit flags (as MLE)} \\
&\quad \widetilde{\text{rv}}(\tau) = \text{claimed instruction read at cycle } \tau \text{ (\textbf{VIRTUAL} - verifier assumes)} \\
&\quad \tau \in \mathbb{F} = \text{random challenge point (NOT a cycle index!)} \\[0.3em]
&\text{\textbf{What this proves:}} \\
&\text{1. BytecodeRa}(x,t) \text{ is one-hot: exactly one } x \text{ per cycle } t \\
&\text{2. On hypercube: rv}(t) = \text{Bytecode}[\text{PC}_t] \text{ where PC}_t \text{ is the one-hot address} \\
&\text{3. MLE extends this to all field points via polynomial interpolation} \\
&\text{4. Sumcheck verifies: prover's rv matches formula at random } \tau \\
&\text{5. Schwartz-Zippel: agreement at random point} \Rightarrow \text{agreement everywhere} \\[0.3em]
&\text{\textbf{What this does NOT prove:}} \\
&\text{- That PC}_t \text{ is the \textit{correct} address (could be reading wrong instructions)} \\
&\text{- That rv fields are used correctly (could ignore opcode, use wrong registers)} \\
&\text{- Property 5 (R1CS) handles wiring: connects rv to other properties, enforces PC updates} \\[0.3em]
&\text{\textbf{Example - cycle 3:}} \\
&\text{If PC}_3 = 4\text{, then rv}(3) = \text{Bytecode}[4] = (\text{opcode: MUL, rs1: x10, rs2: x11, rd: x10, ...}) \\
&\text{The equation verifies this lookup is correct, R1CS verifies PC}_3=4 \text{ is correct} \\[1em]
%
&\textbf{Property 2: Instruction Semantics (Shout Lookups)} \\[0.3em]
&\text{Protocol: Shout batch evaluation (for bitwise) + Spartan R1CS (for arithmetic)} \\
&\text{\textbf{Goal:} Prove instruction outputs match their input-output specification} \\[0.3em]
&\text{\textbf{The equation being proven (for bitwise XOR example):}} \\
&\boxed{\text{LookupOutput}(t) = \text{XOR}(\text{rs1\_val}(t), \text{rs2\_val}(t)) \quad \forall t \in [T]} \\
&\text{At each cycle, the instruction output equals the correct operation on inputs} \\[0.5em]
&\text{\textbf{Two proving strategies (depends on operation type):}} \\[0.3em]
&\text{1. \textbf{Decomposable bitwise ops} (XOR, AND, OR, shifts): Chunk-based lookups} \\
&\quad \text{Key property: Chunks are \textit{independent} (no carries/dependencies between chunks)} \\
&\quad \text{Example: XOR(a,b) = XOR\_4(a[0:3], b[0:3])} \| \text{XOR\_4(a[4:7], b[4:7])} \| \cdots \\
&\quad \text{Proven via: 16 lookups into 256-entry tables (XOR\_4, AND\_4, OR\_4, etc.) + recombination} \\[0.3em]
&\text{2. \textbf{Non-decomposable arithmetic ops} (MUL, ADD, SUB): Field arithmetic} \\
&\quad \text{Key property: Chunks have \textit{dependencies} (carries make decomposition unsound)} \\
&\quad \text{Jolt's solution: Use native 254-bit field arithmetic (no decomposition)} \\
&\quad \text{Proven via: R1CS product/sum constraints + range check lookup} \\[0.3em]
&\text{\textbf{How we get there - two steps for bitwise ops:}} \\[0.3em]
&\text{Step 2a - Chunk lookup correctness (for each chunk } c \in \{0, \ldots, 15\}\text{):} \\
&\widetilde{\text{rv}}_c(r') = \sum_{x \in \{0,1\}^8} \widetilde{\text{InstructionRa}}_c(x, r') \cdot \widetilde{f}_{\text{op}}(x) \\[0.3em]
&\text{where } x = (a, b) \text{ is 8-bit index (two 4-bit operand chunks)} \\
&\quad \widetilde{\text{InstructionRa}}_c(x, r') = \text{one-hot lookup selector (\textbf{COMMITTED} - 16 chunks)} \\
&\quad \widetilde{f}_{\text{op}}(x) = \text{lookup table MLE (\textbf{PUBLIC} - XOR\_4, AND\_4, OR\_4, etc.)} \\
&\quad \quad \text{Table size: } 2^8 = 256 \text{ entries (two 4-bit inputs)} \\
&\quad \widetilde{\text{rv}}_c(r') = \text{chunk result (\textbf{VIRTUAL} - verifier assumes)} \\[0.3em]
&\text{\textbf{What this proves:} Each chunk result rv}_c \text{ correctly looks up from the operation table} \\
&\text{Example - XOR cycle 3, chunk 0: If inputs are (5,3), then rv}_0 = \text{XOR\_4}(5,3) = 6 \\
&\text{\textbf{Role in final equation:} Establishes all 16 chunk results in terms of committed InstructionRa}_c \\[0.3em]
&\text{\textbf{Implementation detail - prefix-suffix algorithm:}} \\
&\text{Challenge: Each chunk looks up from 2}^{128} \text{ table (two 64-bit inputs } \rightarrow \text{ one output)} \\
&\text{Solution: Split index into prefix (first 64 bits) and suffix (last 64 bits)} \\
&\text{Sumcheck becomes tractable by evaluating table MLE at (prefix, suffix) separately} \\
&\text{Key: This is \textit{prover optimization} for tractability, not part of the mathematical claim} \\
&\text{Verifier doesn't care how prover computed sumcheck - just verifies the final result} \\[0.5em]
%
&\text{Step 2b - Recombine chunks into full result:} \\
&\text{LookupOutput}(t) = \sum_{c=0}^{15} \text{rv}_c(t) \cdot 2^{4c} \\[0.3em]
&\text{\textbf{What this proves:} Full 64-bit output = concatenation of 16 4-bit chunk results} \\
&\text{Example - XOR cycle 3: If rv}_0=6, \text{rv}_1=12, \ldots\text{, then LookupOutput} = 6 + 12 \cdot 16 + \cdots \\
&\text{\textbf{Role in final equation:} Combines Step 2a chunk results into final LookupOutput} \\[0.5em]
%
&\text{\textbf{Combining Steps 2a + 2b for bitwise ops:}} \\
&\text{From Step 2a: rv}_c(t) = f_{\text{op}}(\text{input\_chunk\_}c(t)) \text{ for all 16 chunks} \\
&\text{From Step 2b: LookupOutput}(t) = \sum_{c=0}^{15} \text{rv}_c(t) \cdot 2^{4c} \\
&\text{Substitute: } \boxed{\text{LookupOutput}(t) = \sum_{c=0}^{15} f_{\text{op}}(\text{input\_chunk\_}c(t)) \cdot 2^{4c}} \\
&\text{\textbf{Result:} Output = operation applied to each chunk independently, then recombined} \\[0.5em]
%
&\text{\textbf{Alternative path for arithmetic ops (MUL, ADD, SUB):}} \\
&\text{Step 2-arithmetic: R1CS constraint + range check} \\
&\quad \text{Product} = \text{LeftInput} \cdot \text{RightInput} \quad \text{(native 254-bit field multiplication)} \\
&\quad \text{LookupOutput} = \text{RangeCheck(Product)} \quad \text{(truncate to 64 bits via lookup)} \\
&\quad \text{R1CS (Property 5) proves: Product} = \text{rs1\_val} \cdot \text{rs2\_val (constraints.rs:454)} \\
&\text{Example - MUL cycle 3: Product} = 2 \cdot 3 = 6\text{, RangeCheck}(6) = 6 \text{ (fits in 64 bits)} \\
&\text{\textbf{Result:} Output = field arithmetic result, verified to fit in 64 bits} \\[1em]
%
&\textbf{Property 3: Register Consistency (Twist Virtualization)} \\[0.3em]
&\text{Protocol: Twist read/write memory checking (survey § 5.3, Eq. 20)} \\
&\text{\textbf{Goal:} Prove reads return most recent writes via increments} \\[0.3em]
&\text{\textbf{The final equation we're building toward:}} \\
&\boxed{\text{rv}_{rs1}(j) = \sum_{j'<j} \text{wa}(\text{rs1\_addr}, j') \cdot \text{wv}(j')} \\
&\text{Read value at cycle } j = \text{sum of all prior writes to that address (most recent via one-hot wa)} \\[0.5em]
&\text{\textbf{How we get there - three coordinated sumchecks:}} \\[0.3em]
%
&\text{Step 3a - Memory state virtualization (survey Eq. 20):} \\
&f(k, j) = \sum_{j' \in \{0,1\}^{\log T}} \widetilde{\mathbf{wa}}(k, j') \cdot \widetilde{\text{Inc}}(j') \cdot \widetilde{\text{LT}}(j', j) \\[0.3em]
&\text{where } k = \text{register index} \in \{0, \ldots, 63\}, \, j = \text{cycle index} \\
&\quad f(k,j) = \text{value at register } k \text{ before cycle } j \text{ (\textbf{VIRTUAL} - never committed)} \\
&\quad \widetilde{\mathbf{wa}}(k, j') = \text{write address one-hot (\textbf{VIRTUAL} - from bytecode)} \\
&\quad \widetilde{\text{Inc}}(j') = \text{increment written at cycle } j' \text{ (\textbf{COMMITTED})} \\
&\quad \widetilde{\text{LT}}(j', j) = \text{less-than helper (\textbf{PUBLIC} - efficiently evaluable)} \\[0.3em]
&\text{\textbf{What this proves:} State at cycle j = initial value + all prior increments to that address} \\
&\text{Example - register x10 at cycle 3: } f(10, 3) = 0 + \text{Inc}(0) + \text{Inc}(1) = 2 \\
&\text{\textbf{Role in final equation:} Establishes f(k,j) in terms of committed Inc} \\[0.5em]
&\text{Step 3b - Read-checking sumcheck (survey Eq. 19):} \\
&\widetilde{\text{rv}}_{rs1}(r') = \sum_{j \in \{0,1\}^{\log T}} \sum_{k \in \{0,1\}^{\log K}} \text{eq}(r', j) \cdot \widetilde{\mathbf{ra}}_{rs1}(k, j) \cdot f(k, j) \\[0.3em]
&\text{where } K=64 \text{ (number of registers)}, \\
&\quad \widetilde{\mathbf{ra}}_{rs1}(k, j) = \text{\textbf{register} read address one-hot (\textbf{VIRTUAL} - term from bytecode rv)} \\
&\quad \text{The bytecode rv vector contains rs1 field; ra}_{rs1} \text{ is that field as one-hot MLE} \\
&\quad \text{Note: This is NOT the RAM address ra from Property 4 (which is COMMITTED and chunked)} \\
&\quad \widetilde{\text{rv}}_{rs1}(r') = \text{value read from rs1 (\textbf{VIRTUAL} - verifier assumes)} \\[0.3em]
&\text{\textbf{What this proves:} Value read from rs1 at cycle j equals the state f at that register} \\
&\text{Since ra}_{rs1} \text{ is one-hot: rv}_{rs1}(j) = f(\text{rs1\_addr}, j) \\
&\text{Example - cycle 3: } \text{rs1}=\text{x10}, \, \text{rv}_{rs1}(3) = f(10, 3) = 2 \\
&\text{\textbf{Role in final equation:} Connects rv}_{rs1} \text{ to f(k,j) from Step 3a} \\[0.5em]
&\text{Step 3c - Write-checking via increment definition (survey lines 810-815):} \\
&\widetilde{\text{Inc}}(j) = \text{wv}(j) - \sum_k \widetilde{\mathbf{wa}}(k, j) \cdot f(k, j) \\[0.3em]
&\text{where wv}(j) = \text{write value = LookupOutput from Property 2 (\textbf{VIRTUAL})} \\
&\quad \text{wa is one-hot, so } \sum_k \text{wa}(k,j) \cdot f(k,j) = f(\text{rd\_addr}, j) \\[0.3em]
&\text{\textbf{What this proves:} Increment equals new value minus old state (definition of increment)} \\
&\text{Rearranging: wv}(j) = f(\text{rd\_addr}, j) + \text{Inc}(j) \text{ (new state = old state + increment)} \\
&\text{Example - cycle 3: } \text{Inc}(3) = 6 - f(10,3) = 6 - 2 = 4 \\
&\text{\textbf{Role in final equation:} Relates committed Inc to virtual wv (instruction output)} \\[0.5em]
%
&\text{\textbf{Combining all three steps - the proof chain:}} \\
&\text{From Step 3b: } \text{rv}_{rs1}(j) = f(\text{rs1\_addr}, j) \\
&\text{Substitute Step 3a: } \text{rv}_{rs1}(j) = \sum_{j'<j} \text{wa}(\text{rs1\_addr}, j') \cdot \text{Inc}(j') \\
&\text{Substitute Step 3c: } \text{rv}_{rs1}(j) = \sum_{j'<j} \text{wa}(\text{rs1\_addr}, j') \cdot (\text{wv}(j') - f(\text{rd\_addr}, j')) \\
&\text{Simplify (wa one-hot): } \boxed{\text{rv}_{rs1}(j) = \sum_{j'<j, \text{ writes to rs1\_addr}} \text{wv}(j')} \\
&\text{\textbf{Result:} Read value = sum of all prior writes to that address (memory consistency!)} \\[1em]
%
&\textbf{Property 4: RAM Consistency (Twist with Chunking)} \\[0.3em]
&\text{Protocol: Twist read/write memory checking (same as Property 3, with chunking)} \\
&\text{\textbf{Goal:} Prove RAM reads return most recent writes via increments} \\[0.3em]
&\text{\textbf{The equation being proven (identical to Property 3):}} \\
&\boxed{\text{RAM\_rv}(j) = \sum_{j'<j, \text{ writes to addr}} \text{RAM\_wv}(j')} \\
&\text{RAM read value at cycle } j = \text{sum of all prior writes to that address} \\[0.5em]
%
&\text{\textbf{Key difference from Property 3: Address chunking}} \\
&\text{Challenge: RAM address space is } K = 2^{32} \text{ (4 billion addresses) vs registers } K = 64 \\
&\text{Solution: Chunk addresses into } d \text{ pieces, commit to chunked access matrices} \\[0.3em]
&\text{Chunked access matrix:} \\
&\widetilde{\mathbf{ra}}(k, j) = \prod_{i=1}^{d} \widetilde{\mathbf{ra}}_i(k_i, j) \\[0.3em]
&\text{where } k = (k_1, k_2, \ldots, k_d) \text{ is address split into } d \text{ chunks} \\
&\quad \widetilde{\mathbf{ra}}_i(k_i, j) = \text{chunk } i \text{ of read address (\textbf{COMMITTED} - d chunks)} \\
&\quad \text{Full address recovered: ra}(k,j) = \text{product of all } d \text{ chunks (Twist virtualization)} \\[0.3em]
&\text{\textbf{Why chunking?}} \\
&\text{Goal: Keep } K^{1/d} \approx 256 \text{ for polynomial commitment efficiency} \\
&\text{Registers: } d=1, K^{1/1} = 64 \text{ (✓ already small, no chunking needed)} \\
&\text{RAM: } d=4, K^{1/4} = (2^{32})^{1/4} = 2^8 = 256 \text{ (✓ now manageable)} \\
&\text{Without chunking: } 2^{32} \approx 4\text{B polynomial coefficients - intractable!} \\[0.5em]
%
&\text{\textbf{The three steps (identical structure to Property 3):}} \\[0.3em]
&\text{Step 4a - RAM state virtualization: } f(k,j) = \sum_{j'<j} \text{wa}(k,j') \cdot \text{RamInc}(j') \cdot \text{LT}(j',j) \\
&\text{Step 4b - RAM read-checking: } \text{RAM\_rv}(j) = \sum_k \text{ra}(k,j) \cdot f(k,j) \\
&\text{Step 4c - RAM write via increment: } \text{RamInc}(j) = \text{RAM\_wv}(j) - f(\text{addr}, j) \\[0.3em]
&\text{\textbf{Result:} Same final equation as Property 3, but with chunked committed addresses} \\
&\text{Committed: RamRa}_i \text{ (d chunks), RamInc} \quad \text{Virtual: f, wa, RAM\_rv, RAM\_wv} \\[1em]
%
&\textbf{Property 5: R1CS Wiring (Spartan)} \\[0.3em]
&\text{Protocol: Spartan uniform R1CS (survey § 6, theory doc § 2.1)} \\
&\text{\textbf{Goal:} Link Properties 1-4 together + enforce correct PC updates and control flow} \\[0.3em]
&\text{\textbf{The equation being proven:}} \\
&\boxed{Az \circ Bz - Cz = \mathbf{0} \quad \text{for all } T \times 30 \text{ constraint rows}} \\
&\text{All }\text{~}30 \text{ constraints hold at every cycle } t \in [T] \text{ (wiring + control logic)} \\[0.5em]
%
&\text{\textbf{What are these }\text{~}30 \text{ constraints?}} \\
&\text{Three categories:} \\
&\text{1. \textbf{Wiring constraints}: Link virtual polynomials from Properties 1-4 to committed witness} \\
&\quad \text{Example: LookupOutput}(t) = \text{wv}(t) \text{ (instruction output = register write value)} \\
&\text{2. \textbf{PC update constraints}: Enforce correct program counter evolution} \\
&\quad \text{Example: PC}(t+1) = \text{PC}(t) + 4 \text{ (if not jumping)} \\
&\text{3. \textbf{Control flow constraints}: Handle jumps, branches, special cases} \\
&\quad \text{Example: BranchTaken}(t) = \text{Condition}(t) \cdot \text{BranchFlag}(t) \\[0.5em]
%
&\text{\textbf{The R1CS structure:}} \\
&Az \circ Bz - Cz = \mathbf{0} \\[0.3em]
&\text{where } z = \text{witness vector (\textbf{COMMITTED} - contains all committed polys as sub-vectors)} \\
&\quad z = [\text{PC}, \text{LookupOutput}, \text{Inc}, \text{InstructionRa}, \text{BytecodeRa}, \text{RamRa}, \text{RamInc}, \ldots] \\
&\quad A, B, C = \text{sparse constraint matrices (\textbf{PUBLIC} - uniform structure across cycles)} \\
&\quad \text{Each matrix is } (T \times 30) \times |z| \text{ but extremely sparse (mostly zeros)} \\[0.3em]
&\text{\textbf{Key constraint examples (showing how Properties 1-4 link together):}} \\[0.3em]
%
&\text{Constraint 5a - PC update (normal instruction):} \\
&(\widetilde{\text{PC}}(t+1) - \widetilde{\text{PC}}(t) - 4) \cdot (1 - \text{JumpFlag}(t)) = 0 \\
&\text{\textbf{What this proves:} If not jumping, PC increments by 4 (next instruction)} \\
&\text{\textbf{Role:} Enforces sequential execution (Property 1 PC addresses are correct)} \\[0.3em]
%
&\text{Constraint 5b - Instruction→Register link (THE KEY WIRING):} \\
&\widetilde{\text{LookupOutput}}(t) - \widetilde{\text{wv}}(t) = 0 \\
&\text{where } \widetilde{\text{LookupOutput}}(t) = \text{output from Property 2 (VIRTUAL)} \\
&\quad \quad \widetilde{\text{wv}}(t) = \text{write value from Property 3c (VIRTUAL)} \\
&\text{\textbf{What this proves:} Instruction output equals value written to register} \\
&\text{\textbf{Role:} Links Property 2 (instruction semantics) to Property 3 (register writes)} \\
&\text{Example - cycle 3: MUL output (6) = value written to x10 (6)} \\[0.3em]
%
&\text{Constraint 5c - Jump target:} \\
&(\widetilde{\text{PC}}(t+1) - \text{JumpTarget}(t)) \cdot \text{JumpFlag}(t) = 0 \\
&\text{\textbf{What this proves:} If jumping, PC equals jump target from bytecode} \\
&\text{\textbf{Role:} Enforces correct control flow (connects Property 1 immediate field to PC)} \\[0.3em]
%
&\text{[... 27 more constraints for branches, loads/stores, special cases, etc.]} \\[0.5em]
%
&\textbf{How constraints map to } Az \circ Bz - Cz = \mathbf{0}\textbf{:} \\
&\text{Each constraint becomes one row. Example 5a (product form):} \\
&(\text{PC}(t+1) - \text{PC}(t) - 4) \cdot (1 - \text{JumpFlag}(t)) = 0 \\
&\Rightarrow A_{\text{row}} \cdot z = [\text{PC}(t+1) - \text{PC}(t) - 4], \quad B_{\text{row}} \cdot z = [1 - \text{JumpFlag}(t)], \quad C_{\text{row}} \cdot z = [0] \\[0.3em]
&\text{Example 5b (linear form):} \\
&\text{LookupOutput}(t) - \text{wv}(t) = 0 \\
&\Rightarrow A_{\text{row}} \cdot z = [\text{LookupOutput}(t) - \text{wv}(t)], \quad B_{\text{row}} \cdot z = [1], \quad C_{\text{row}} \cdot z = [0] \\[0.5em]
%
&\text{\textbf{Spartan's sumcheck (verifies ALL constraints simultaneously):}} \\
&\sum_{t \in \{0,1\}^{\log T}} \text{eq}(\tau, t) \cdot \left[ (\widetilde{Az})_t \circ (\widetilde{Bz})_t - (\widetilde{Cz})_t \right] = \mathbf{0} \\[0.3em]
&\text{\textbf{What this proves:} All } T \times 30 \text{ constraint rows hold (single sumcheck!)} \\
&\text{\textbf{Role in final equation:} Reduces } T \times 30 \text{ checks to one polynomial evaluation at random } \tau \\
&\text{Example - cycle 3: } (0x80000018 - 0x80000014 - 4) \cdot 1 = 0, \, 6 - 6 = 0, \, \ldots \\[0.5em]
%
&\text{\textbf{Result:} Property 5 is the "glue" that connects Properties 1-4 and ensures correctness}
\end{align*}
}
$$

---

### How Property 5 (R1CS) Connects Everything

**The key insight you identified**: Properties 1-4 assume virtual polynomials (rv, rv_c, f, wa, ra), while Property 5 **links these virtual polynomials to the committed witness z**.

**The connection mechanism**:

1. **Properties 1-4 output virtual polynomial claims**:
   - Property 1: `rv` (bytecode read values)
   - Property 2: `rv_c` (chunk results) → `LookupOutput` (recombined)
   - Property 3: `rv_rs1`, `rv_rs2` (register read values)
   - Property 4: RAM read values (if any)

2. **Property 5 (R1CS) contains constraints that reference these virtual polynomials**:
   - Example 5b: `LookupOutput(t) - wv(t) = 0`
     - **Left side**: `LookupOutput(t)` = Virtual polynomial from Property 2 (instruction output)
     - **Right side**: `wv(t)` = Write value from Property 3c (what gets written to register)
     - **Effect**: Forces instruction output to match register write value!
     - **Note**: Both `LookupOutput` and `wv` are VIRTUAL, but they're constrained to be equal
     - The committed `Inc(t)` in witness `z` ties this to actual committed data via `Inc(t) = wv(t) - f(rd_addr, t)`

3. **The witness `z` contains all committed polynomials**:
   ```
   z = [PC(0), PC(1), ..., PC(T),          ← Program counters
        Inc(0), Inc(1), ..., Inc(T),        ← Register increments (COMMITTED)
        RamInc(0), ..., RamInc(T),         ← RAM increments (COMMITTED)
        InstructionRa_0[...],              ← Lookup selectors (COMMITTED, 16 chunks)
        BytecodeRa[...],                   ← Bytecode selectors (COMMITTED)
        RamRa_0[...], ..., RamRa_d[...],   ← RAM address chunks (COMMITTED)
        ...]
   ```

4. **R1CS constraints act as "wiring constraints"**:
   - Link virtual polynomial outputs from different properties
   - Example: `LookupOutput(3) = 6` (from Property 2) must equal `wv(3) = 6` (from Property 3)
   - The committed `Inc(3) = 4` in witness `z` enforces this via: `Inc(3) = wv(3) - f(10,3) = 6 - 2 = 4`
   - If prover cheats in Property 2, inconsistency appears when R1CS checks `LookupOutput = wv`

5. **Stage 5 (opening proof) verifies commitments**:
   - After all sumchecks reduce to polynomial evaluation claims
   - Verifier batches all committed polynomial opening claims
   - Single Dory opening proof verifies: "The committed `z` actually evaluates to these values"

**Summary of verification flow**:
```
Properties 1-4: "Assume these virtual polynomial values are correct"
    ↓
Property 5: "These virtual values must match committed witness z"
    ↓
Stage 5: "Prove the committed z has the claimed values"
```

**Why this is efficient**:
- Only ~28-30 commitments (not one per polynomial!)
- Virtual polynomials never committed (saved commitment cost)
- All openings batched together (Stage 5 batch proof)
- R1CS enforces consistency between virtual and committed data

---

### The Three Key Protocols (from Thaler Survey)

Understanding how these equations map to the three core protocols from [Thaler survey](Thaler_survey.md):

#### 1. Shout: Batch Evaluation Arguments (Survey § 5.1)

**What it does**: Proves `T` lookups into a table are correct, with prover cost `O(T)` not `O(table_size)`.

**Where used in Jolt**:
- **Bytecode** (Property 1): Prove instruction bytes match committed program
- **Instructions** (Property 2): Prove instruction execution (ADD, MUL, XOR, etc.) via lookups

**Key insight from survey** (lines 658-665):
> "The sum in Equation (16) has 2^ℓ terms... In important applications... ℓ can be as large as 128... Hence, we cannot afford a sum-check prover that spends time linear in the number of terms being summed. Fortunately... ra(x,r′) is highly sparse: it evaluates to a non-zero value for only T inputs x∈{0,1}^ℓ."

**The general Shout equation** (survey Eq. 16):
$$\widetilde{\text{rv}}(r') = \sum_{x \in \{0,1\}^{\ell}} \widetilde{\text{ra}}(x, r') \cdot \widetilde{f}(x)$$

where:
- **`rv`** = read values (VIRTUAL - claimed lookup results, not committed!)
- **`ra`** = read addresses (COMMITTED - one-hot selector, highly sparse)
- **`f`** = lookup table (PUBLIC - efficiently evaluable MLE)

**How sumcheck is applied**:
1. **Prover claims**: `rv(r')` evaluates to some value (e.g., `rv(3) = 6`)
2. **Verifier challenges**: Pick random point `r'`
3. **Sumcheck proves**: The claim `rv(r') = Σ_x ra(x,r')·f(x)` is correct
4. **Reduces to**: Evaluation claims for `ra(x,r')` and `f(x)` at final random point
5. **Result**: Verifier convinced that `rv` was computed correctly from committed `ra` and public `f`

**Key**: The sum over `x ∈ {0,1}^ℓ` has `2^ℓ` terms, but sumcheck handles this efficiently.
For ℓ=128 (64-bit ops), this would be intractable without sumcheck!

---

**Q: What is `InstructionRa`? Is it committed?**

**A:** Yes, `InstructionRa_c(x, t)` is **COMMITTED** (one of the ~28-30 committed polynomials).

- **Not an operation selector** - it selects which **table entry** to look up
- **One-hot encoding**: For each lookup at cycle `t`, chunk `c`, exactly one `x` has `ra_c(x,t) = 1`, rest are 0
- **Example**: Looking up MUL(2,3) chunk 0 means `InstructionRa_0((2,3), 3) = 1` and `InstructionRa_0(x, 3) = 0` for all other `x`
- **Purpose**: Acts as a selector in the sum - when `ra(x,t)=1`, we pick `f(x)` from the table

**Why commit to `ra` but not the table `f`?**
- `ra` is **sparse**: Only `T` nonzero entries out of `2^ℓ` possible
- `f` is **public**: Verifier can evaluate `f(x)` efficiently at any random point
- Committing to sparse `ra` is cheap; materializing full `f` would be impossible (e.g., `2^128` for 64-bit ops)

---

**Q: What is `LookupOutput`? Is it the combination of chunks?**

**A:** Yes! `LookupOutput(t)` is a **VIRTUAL polynomial** (not committed) - it's `rv` in the survey equation.

**Definition**:
$$\text{LookupOutput}(t) = \sum_{c=0}^{15} \text{rv}_c(t) \cdot 2^{4c}$$

where `rv_c(t)` is the result of the Shout lookup for chunk `c` at cycle `t`.

**Example** (MUL(2,3) at cycle 3):
- Chunk 0: `rv_0(3) = MUL_4(2,3) = 6`
- Chunks 1-15: `rv_c(3) = 0` (no carry)
- Combined: `LookupOutput(3) = 6·2^0 + 0·2^4 + ... = 6`

**Why is it VIRTUAL?**
- It's **derived** from the chunk lookups `rv_c(t)` via the recombination formula
- Proven correct via R1CS constraint (Property 5b): `LookupOutput(t) - rd_write_val(t) = 0`
- No need to commit - verifier can compute it from the Shout sumcheck outputs

**The data flow**:
1. Commit to `InstructionRa_c` (which entries are looked up)
2. Run Shout sumcheck → get `rv_c(t)` for each chunk (VIRTUAL)
3. Recombine via R1CS → get `LookupOutput(t)` (VIRTUAL)
4. Copy to register via R1CS → proves output is correct

**Cost**: `O(T)` prover time via prefix-suffix sumcheck (survey § 4.2)

---

#### 2. Twist: Read/Write Memory Checking (Survey § 5.3)

**What it does**: Proves reads return most recent writes without committing to full memory history.

**Where used in Jolt**:
- **Registers** (Property 3): 64 registers, 2 reads + 1 write per cycle
- **RAM** (Property 4): Dynamic memory, 1 read or write per cycle

---

**From Shout to Twist: The Conceptual Leap** (survey lines 790-807)

**Shout** (read-only memory):
$$\text{rv}(r) = \sum_{k} \text{ra}(k,r) \cdot \underbrace{f(k)}_{\text{static table}}$$

Memory never changes - `f(k)` is fixed (e.g., bytecode, instruction tables).

**Twist** (read/write memory - generalization of Shout):
$$\text{rv}(r) = \sum_{j,k} \text{eq}(r,j) \cdot \text{ra}(k,j) \cdot \underbrace{f(k,j)}_{\text{dynamic, changes over time}}$$

Memory evolves - `f(k,j)` = value at address `k` at time `j` (e.g., registers, RAM).

**The problem**: Can't commit to full `K×T` table!
- Registers: `64 × 10^6` = 64 million entries
- RAM: `2^32 × 10^6` = 4 trillion entries
- **Impossible to materialize and commit!**

---

**Twist's Solution: Virtualize f(k,j) via Increments**

**Key insight** (survey lines 809-824):
> "Let Inc(j) be the difference between the new value to be written at time j and the current value already stored at the target cell: Inc(j) := wv(j) − Σ_k wa(k,j)·f(k,j). That is, Inc(j) represents how much the value at the write-address for time j changes."

**The virtualization formula**:
$$f(k, j) = \sum_{j' \in \{0,1\}^{\log T}} \widetilde{\mathbf{wa}}(k, j') \cdot \widetilde{\text{Inc}}(j') \cdot \widetilde{\text{LT}}(j', j)$$

**Reading this formula**:
- **Goal**: Compute value at address `k` before cycle `j`
- **Method**: Sum up all increments written to address `k` before time `j`
- **wa(k,j')**: One-hot selector - equals 1 if cycle `j'` wrote to address `k`
- **Inc(j')**: The increment written at cycle `j'` (new value - old value)
- **LT(j',j)**: Equals 1 if `j' < j` (only count prior writes)

**Example** (register x10 over 4 cycles):
```
Initial: f(10, 0) = 0

Cycle 0: Write 5 to x10
  → Inc(0) = 5 - 0 = 5
  → f(10, 1) = wa(10,0)·Inc(0)·LT(0,1) = 1·5·1 = 5

Cycle 1: No write to x10
  → f(10, 2) = f(10, 1) = 5

Cycle 2: Write 3 to x10
  → Inc(2) = 3 - 5 = -2
  → f(10, 3) = wa(10,0)·5·LT(0,3) + wa(10,2)·(-2)·LT(2,3)
              = 1·5·1 + 1·(-2)·1 = 3

Final: f(10,3) = 5 + (-2) = 3 ✓
```

**Why this works**:
- Current value = initial value + sum of all prior increments
- Only commits to sparse `Inc` vector (mostly zeros when same cell not accessed)
- Exploits **locality**: Accessing same register repeatedly is cheap!

---

**Q: What is `ra_rs1` in `rv_rs1`?**

**A:** `ra_rs1(k,j)` is the **read address** for the `rs1` register operand.

- **One-hot vector** indicating which register is read as `rs1` at cycle `j`
- **VIRTUAL** (not committed) - derived from bytecode
- **Example**: If instruction at cycle 3 has `rs1 = x10`:
  - `ra_rs1(10, 3) = 1` ← "Read from register 10"
  - `ra_rs1(k, 3) = 0` for all `k ≠ 10`

**Why virtual?**
- Register addresses are **hardcoded in bytecode**
- Bytecode already committed (Property 1)
- Can derive `ra_rs1` from bytecode read-checking (no double commitment!)
- Formula: `ra_rs1(k,j) = Σ_x BytecodeRa(x,j) · [rs1_field_of_Bytecode[x] == k]`

**The read-checking equation** (survey Eq. 19):
$$\widetilde{\text{rv}}_{\text{rs1}}(r') = \sum_{j,k} \text{eq}(r',j) \cdot \underbrace{\widetilde{\text{ra}}_{\text{rs1}}(k,j)}_{\text{which register?}} \cdot \underbrace{f(k,j)}_{\text{its value}}$$

**Reading this**:
- For each cycle `j`, select register `k` via one-hot `ra_rs1(k,j)`
- Return its value `f(k,j)` (computed from increments)
- Proves: "The value read from rs1 at each cycle equals the current register state"

**How sumcheck is applied** (Twist uses multiple sumchecks):

1. **Read-checking sumcheck** (above equation):
   - **Prover claims**: `rv_rs1(r')` = value read from rs1
   - **Verifier challenges**: Pick random `r'`
   - **Sumcheck proves**: `rv_rs1(r') = Σ_{j,k} eq(r',j)·ra_rs1(k,j)·f(k,j)` holds
   - **Reduces to**: Claims about `ra_rs1(k,j)` and `f(k,j)` at random point

2. **Write-checking sumcheck** (similar for writes):
   - Proves write address and increment consistency
   - **Equation**: `f(k,j+1) - f(k,j) = Σ_k' wa(k',j)·Inc(j)·[k'=k]`

3. **Virtualization sumcheck** (for the formula `f(k,j) = Σ_j' wa(k,j')·Inc(j')·LT(j',j)`):
   - Proves the virtualized `f(k,j)` matches what read-checking expects
   - **Reduces to**: Opening claims for committed `Inc(j')`

**Result**: Three coordinated sumchecks prove memory consistency, all reducing to committed `Inc` openings!

---

**Why better than grand products** (survey § 5.4):
- Only commits to increments (sparse, small values)
- No sorting needed (streaming-compatible)
- Exploits locality (accessing hot memory is cheaper)
- Small-value preservation (increments often small)

**Cost**: `O(T log K)` worst case, often `O(T · locality_measure)` in practice

---

#### 3. Spartan: Uniform R1CS (Survey § 6, Theory Doc § 2.1)

**What it does**: Proves satisfaction of R1CS constraints with time-optimal prover when constraints are uniform.

**Where used in Jolt**:
- **R1CS wiring** (Property 5): ~30 constraints per cycle (PC updates, copy constraints, flags)

**Key insight from theory doc** (lines 7480-7485):
> "Jolt exploits VM uniformity: the same ~30 constraints are checked every cycle. This enables Spartan's structured proving algorithm, where constraint matrices A, B, C are highly regular."

**The R1CS equation**:
$$Az \circ Bz - Cz = \mathbf{0}$$

where:
- `z` = witness vector (PC, register values, instruction outputs, flags)
- `A, B, C` = constraint matrices (sparse, uniform structure)
- `○` = Hadamard (element-wise) product

**How sumcheck is applied** (three nested sumchecks, theory doc § 2.1):

1. **Outer sumcheck** - Proves constraint satisfaction:
   - **Claim**: `Σ_t eq(τ,t)·[(Az)_t ○ (Bz)_t - (Cz)_t] = 0` for all cycles `t`
   - **Verifier challenges**: Pick random `τ`
   - **Sumcheck proves**: The sum over all `t` equals zero
   - **Reduces to**: Evaluation claims for `(Az)_τ`, `(Bz)_τ`, `(Cz)_τ` at random `τ`

2. **Product sumcheck** - Proves matrix-vector products:
   - **Claim**: `(Az)_τ = Σ_i A[τ,i]·z[i]` (and similarly for B, C)
   - **Sumcheck proves**: Matrix-vector multiplication correct
   - **Reduces to**: Evaluation claims for `A[τ,i]`, `z[i]` at random point

3. **Matrix evaluation sumcheck** - Handles sparse matrices:
   - Exploits sparse structure of A, B, C (uniform constraints)
   - **Reduces to**: Opening claims for committed witness `z`

**Result**: Three nested sumchecks reduce T×30 constraints to opening the witness `z`!

**Cost**: `O(N)` where N = number of constraint instances (T cycles × ~30 constraints)

---

### Why These Three Protocols Together?

From [Thaler survey § 7](Thaler_survey.md) (lines 1218-1243):

**zkVMs need different tools for different jobs**:

| Job | Tool | Why |
|-----|------|-----|
| **Instruction semantics** | Shout | CPU ops are lookups (XOR, MUL, etc.) |
| **Memory consistency** | Twist | Registers/RAM evolve over time |
| **Wiring/control flow** | Spartan | PC updates, data routing |

**Jolt's architecture** (survey Figure 3, lines 1264-1292):
```
    ┌─────────────────────────────────────┐
    │         RISC-V Execution            │
    └────┬─────────┬──────────┬───────────┘
         │         │          │
    ┌────▼────┐ ┌──▼──────┐ ┌▼──────────┐
    │ Bytecode│ │ Instrs  │ │ Registers │
    │ (Shout) │ │ (Shout) │ │  (Twist)  │
    └─────────┘ └─────────┘ └───────────┘
         │         │          │
    ┌────▼─────────▼──────────▼───────────┐
    │  RAM (Twist)  +  R1CS (Spartan)     │
    │  ↓                      ↓            │
    │ Prove memory    Prove PC updates    │
    │ consistency     and data flow        │
    └──────────────────────────────────────┘
```

**Performance** (survey lines 1293-1309):
- Jolt prover: ~500 field ops per VM cycle
- Breakdown: Spartan ~50, Shout ~220, Twist ~180, batching ~50
- On 4 GHz CPU: ~80K VM cycles/second single-threaded
- With 16 threads: ~1.5M VM cycles/second
- **This was predicted theoretically before implementation!**

---

### Summary: Five Properties via Three Protocols

**All five properties for cycle 3 are checked via ~22 sumchecks organized into 5 stages**:

| Property | Protocol | Committed Data | What's Proven |
|----------|----------|----------------|---------------|
| 1. Bytecode | Shout | BytecodeRa | Instruction fetch correct |
| 2. Instruction | Shout + prefix-suffix | InstructionRa | MUL(2,3) = 6 |
| 3. Registers | Twist | RdInc, (ra virtual) | Read rs1=2, rs2=3; write rd=6 |
| 4. RAM | Twist | RamInc, RamRa | Memory consistent (no-op this cycle) |
| 5. R1CS | Spartan | (via z witness) | PC updates, data flows correctly |

**Total committed polynomials**: ~28-30 (see Part 1 tables)

**Total sumchecks**: ~22 across 5 stages (batched for efficiency)

**Final verification**: Stage 5 batched Dory opening proves all evaluation claims

---

## Part 4: Zooming Out - The Full Verification DAG

Each property spans multiple stages:

| Property | Stages | Sumchecks |
|----------|--------|-----------|
| **Bytecode** | 1C, 2C | 2 (read-check, write-check) |
| **Instruction** | 1B, 2B, 3B | 3 (access matrix, prefix-suffix, table eval) |
| **Registers** | 1D, 2D, 3C | 3 (read-check, write-check, eval) |
| **RAM** | 1E, 2E, 3D | 3 (read-check, write-check, eval) |
| **R1CS** | 1A, 2A, 3A | 3 (outer, product, matrix eval) |
| **All Openings** | 5 | 1 (batched Dory opening) |

**Total**: ~15 sumchecks across stages 1-4, plus batched opening in stage 5.

**Visual structure**:

```
                        EMULATOR
                            ↓
                    Vec<RISCVCycle>
                            ↓
              ┌─────────────┴─────────────┐
              ↓                           ↓
        Component Traces          Polynomial MLEs
      (pc, instr, regs, mem)     (P̃C, Ĩnstr, R̃eg, ...)
                                          ↓
                                   Commitments
                                    (Dory PCS)
                                          ↓
                            ┌─────────────┴─────────────┐
                            ↓                           ↓
                      Stage 1-4                    Stage 5
                   (15 sumchecks)              (Batched opening)
                            ↓                           ↓
              Evaluation claims (50+)          Verify all openings
                (e.g., P̃C(r)=v)                     together
                            └─────────────┬─────────────┘
                                          ↓
                                    Accept/Reject
```

---

## Part 5: Key Insights

### Insight 1: Indirection Through Polynomials

**NOT directly checked**: "Register 10 at cycle 3 contains value 2"

**Instead checked**: "$\widetilde{\text{RegVal}}(3, 10) = 2$ where $\widetilde{\text{RegVal}}$ is committed polynomial"

**Why**: Polynomial commitments allow verifier to check evaluations at random points without seeing full trace.

---

### Insight 2: Randomization Prevents Cheating

**If prover lies** about cycle 3's register value:
- Sumcheck over $\sum_{t=0}^{T-1} \text{eq}(\tau, t) \cdot [\ldots]$ would detect it
- Random $\tau$ makes it infeasible to construct polynomial that "hides" the lie
- Schwartz-Zippel: Different polynomials disagree at random point w.h.p.

---

### Insight 3: Batching Everything

**NOT done**: 1M separate proofs for 1M cycles

**Instead**: Single sumcheck over domain $\{0,1\}^{20}$ (since $2^{20} = 1M$)

**Cost**: 20 rounds of interaction (or ~20 Fiat-Shamir hashes in non-interactive version)

---

### Insight 4: Virtual Polynomials

**NOT committed**: Every intermediate value (instruction outputs, memory state evolution)

**Instead**: Expressed as functions of committed polynomials, verified via sumcheck

**Example**: Instruction output not committed directly, but proven correct via lookup into table (which IS committed)

**Benefit**: Fewer commitments = faster prover

---

### Insight 5: Lookup Decomposition is Key

**Naive approach**: Commit to $2^{128}$-entry MUL table (impossible: 10^38 entries)

**Jolt approach**: Commit to 256-entry MUL_4 table, decompose 64-bit ops into 16 lookups

**Result**: Feasible commitment size + efficient lookups

---

## Summary: Execution -> Verification Mapping

**For every cycle**:

1. **Concrete execution** produces:
   - PC value
   - Instruction bytes
   - Register reads/writes (address, value)
   - Memory reads/writes (address, value)

2. **Polynomial encoding** creates:
   - $\widetilde{\text{PC}}(t)$, $\widetilde{\text{Instr}}(t)$
   - $\widetilde{\text{RegAddr}}_{rs1/rs2/rd}(t)$, $\widetilde{\text{RegVal}}_{rs1/rs2/rd}(t)$
   - $\widetilde{\text{MemAddr}}(t)$, $\widetilde{\text{MemVal}}(t)$

3. **Sumcheck equations** verify:
   - Bytecode: $\widetilde{\text{Instr}}(t) = \widetilde{\text{Bytecode}}(\text{index})$
   - Instruction: Decomposed lookups into 4-bit tables
   - Registers: Time-ordered = address-ordered (via grand products)
   - RAM: Same as registers (with chunking)
   - R1CS: PC updates correctly, data flows correctly

4. **Batched opening** verifies:
   - All evaluation claims from sumchecks are correct
   - Single Dory proof covers all ~50 openings

**Result**: Verifier convinced of correct execution without re-running program.

---

## Next Steps

**To understand the protocols in depth**:
- **[01_Jolt_Theory_Enhanced.md](01_Jolt_Theory_Enhanced.md)**: Full mathematical specifications
  - § 2.1: Spartan (R1CS)
  - § 2.2: Shout (Instruction lookups, bytecode)
  - § 2.3: Twist (Memory checking)
  - § 2.5: Dory (Polynomial commitments)
  - § 4.2: Complete DAG structure

**To understand the implementation**:
- **[02_Jolt_Complete_Guide.md](02_Jolt_Complete_Guide.md)**: Code paths and file organization
  - Preprocessing flow
  - Witness generation
  - Proof generation stages
  - Where each component lives in the codebase

**To understand verification logic**:
- **[03_Verifier_Mathematics_and_Code.md](03_Verifier_Mathematics_and_Code.md)**: Line-by-line verifier walkthrough
  - What verifier receives
  - What checks are performed
  - Why checks are sound
