---
title: "Jolt Verifier: Step-by-Step Mathematical Analysis"
abstract: "Detailed walkthrough of the Jolt verifier's mathematical operations and code implementation. Complete reference for implementers, auditors, and those debugging the verifier, covering all verification logic, checks, and mathematical soundness proofs."
author: "Parti"
date: "2025-10-23"
lang: en-US
titlepage: true
titlepage-rule-color: "360049"
toc: true
toc-own-page: true
toc-depth: 2
---

# Jolt Verifier: Step-by-Step Mathematical Analysis

> **For High-Level Overview**: This document provides complete implementation details with line-by-line code mapping. For a high-level introduction to verification, see [Jolt.md Part 4: Verification Deep Dive](Jolt.md#part-4-verification-deep-dive).

This document provides a detailed walkthrough of the Jolt verifier's mathematical operations and code implementation. We focus exclusively on verification logic—what the verifier receives, what checks it performs, and why those checks are mathematically sound.

**Purpose**: Complete reference for implementers, auditors, and those debugging the verifier.

## Table of Contents

1. [Verifier Overview](#verifier-overview)
2. [Input to the Verifier](#input-to-the-verifier)
3. [Verification Pipeline](#verification-pipeline)
4. [Stage-by-Stage Mathematical Analysis](#stage-by-stage-mathematical-analysis)
5. [Final Opening Verification](#final-opening-verification)
6. [Security Guarantees](#security-guarantees)

***

## Verifier Overview

### High-Level Flow

The Jolt verifier's job is to efficiently check that a RISC-V program executed correctly, without re-executing the program. The verifier receives:

1. **Preprocessing data** (generated once per program)
2. **Program inputs/outputs** (public data)
3. **A proof** (generated by the prover for this specific execution)

The verifier uses these to perform a series of **sumcheck verifications** that together prove:

- Bytecode was decoded correctly
- Instructions executed correctly
- Registers were read/written correctly
- RAM was accessed correctly
- PC (program counter) updated correctly

### Key Mathematical Primitive: Sumcheck Verification

**Recall the sumcheck protocol** (from [Theory/The Sum-check Protocol.md](../Theory/The%20Sum-check%20Protocol.md)):

**Prover's claim**: $C_0 = \sum_{x \in \{0,1\}^n} g(x)$

**Verification process**:
- For $j = 1$ to $n$:
  - Prover sends univariate polynomial $g_j(X_j)$
  - Verifier checks: $C_{j-1} \stackrel{?}{=} g_j(0) + g_j(1)$
  - Verifier samples random $r_j \leftarrow \mathbb{F}$
  - Verifier computes new claim: $C_j = g_j(r_j)$
- After $n$ rounds, verifier checks: $C_n \stackrel{?}{=} g(r_1, \ldots, r_n)$

**Why this works**:
- If prover is honest, all checks pass by definition (completeness)
- If prover lies about $C_0$, they must send wrong polynomial $g_1$
- Wrong polynomial disagrees with true polynomial at random point (Schwartz-Zippel)
- With probability $\geq 1 - \frac{d}{|\mathbb{F}|}$, the lie propagates to final check
- Final check directly compares prover's claim vs. verifier's computation

***

## Input to the Verifier

### 1. Verifier Preprocessing (`JoltVerifierPreprocessing`)

**Location**: [`jolt-core/src/zkvm/mod.rs:116-123`](../jolt-core/src/zkvm/mod.rs#L116-L123)

```rust
pub struct JoltVerifierPreprocessing<F, PCS> {
    pub generators: PCS::VerifierSetup,
    pub shared: JoltSharedPreprocessing,
}

pub struct JoltSharedPreprocessing {
    pub bytecode: BytecodePreprocessing,
    pub ram: RAMPreprocessing,
    pub memory_layout: MemoryLayout,
}
```

**What's inside**:
- **`generators`**: Commitment scheme verifier parameters (e.g., Dory SRS for verifier)
- **`bytecode`**: **Full decoded bytecode** (yes, the verifier has the complete bytecode!)
  - Contains the actual `Vec<Instruction>` (all program instructions)
  - Also includes PC mapping and chunking parameter `d`
  - This is **NOT** a commitment - it's the actual bytecode
- **`ram`**: Initial RAM state (program + static data as bytecode words)
- **`memory_layout`**: **Blueprint over RAM** - divides RAM into regions (inputs at 0x50000000, outputs at 0x51000000, code at 0x80000000, etc.). Not a physical component, just configuration metadata used by all components.

**Important clarification**: The verifier receives the **full bytecode in plain**. This makes sense because:
- The bytecode is **public** (it's the program being verified)
- Both prover and verifier need to know what program is being executed
- The verifier needs the bytecode to check that execution trace reads match the actual instructions

**Mathematical meaning**: This is the **structured reference string** for the specific program. It includes everything deterministic about the program (bytecode, memory layout, initial RAM state).

#### Concrete Toy Example

Let's walk through preprocessing for a tiny program that adds two numbers:

**Guest program** (Rust):
```rust
#[jolt::provable]
fn add_two_numbers(a: u32, b: u32) -> u32 {
    a + b
}
```

**Compiled RISC-V bytecode** (simplified):
```
Address    Instruction           Encoding
0x80000000 addi x10, x10, x11   0x00b50533   // x10 = x10 + x11
0x80000004 ret                   0x00008067   // return
```

**1. Bytecode Preprocessing**:

After compilation, the bytecode is decoded and **given to both prover and verifier**:
```rust
// Both prover AND verifier receive:
bytecode_preprocessing = BytecodePreprocessing {
    code_size: 4,  // Padded to power of 2
    bytecode: vec![
        Instruction::NoOp,  // Prepended no-op (index 0)
        Instruction::Add { rs1: 10, rs2: 11, rd: 10, ... },  // Index 1
        Instruction::Ret { ... },  // Index 2
        Instruction::NoOp,  // Padding (index 3)
    ],
    pc_map: /* maps real PC to virtual PC */,
    d: 2,  // Chunking parameter
}
```

**Key insight**: The bytecode is **public information**, not hidden from the verifier!

**Why this makes sense**:
- The program being verified is known to everyone
- The verifier needs to check that the execution trace correctly reads from this bytecode
- During verification, the bytecode sumcheck proves: "every cycle read the correct instruction from this public bytecode"

**What gets committed during proving** (not preprocessing):

The prover commits to **~28-30 witness polynomials** generated from the execution trace. These include:

| Category | Polynomials | Count | What They Encode |
|----------|-------------|-------|------------------|
| **Increment witnesses** | `RdInc`, `RamInc` | 2 | Register and RAM value increments each cycle |
| **Instruction lookups** | `InstructionRa(0)` through `InstructionRa(15)` | 16 | One-hot encoding of 128-bit instruction lookup indices (16 × 8-bit chunks) |
| **Bytecode lookups** | `BytecodeRa(0)` through `BytecodeRa(d-1)` | ~3-4 | One-hot encoding of which bytecode address (PC) was read each cycle |
| **RAM addresses** | `RamRa(0)` through `RamRa(d-1)` | ~8 | One-hot encoding of RAM addresses accessed each cycle (chunked for 64-bit addresses) |

**Total**: ~28-30 committed polynomials (exact count depends on dynamic `d` parameters for RAM and bytecode chunking)

**IMPORTANT - What is NOT committed** (these are VIRTUAL polynomials):
- **Register addresses** (`Rs1Ra`, `Rs2Ra`, `RdWa`): Virtualized via bytecode read-checking
- **R1CS witnesses** (`LeftInstructionInput`, `RightInstructionInput`, `WriteLookupOutputToRD`, `WritePCtoRD`, `ShouldBranch`, `ShouldJump`): Part of R1CS witness, accessed via Spartan sumchecks
- **Register/RAM values** (`Rs1Value`, `Rs2Value`, `RdWriteValue`, `RamReadValue`, `RamWriteValue`): Computed from increments via Twist

See [witness.rs:48-63](../jolt-core/src/zkvm/witness.rs#L48-L63) for the complete `CommittedPolynomial` enum and [witness.rs:571-616](../jolt-core/src/zkvm/witness.rs#L571-L616) for `VirtualPolynomial` enum.

The bytecode itself is **not** committed (it's public data) - only the **read addresses** (`BytecodeRa`) are committed.

**2. RAM Preprocessing**:

For our tiny program:
```rust
ram_preprocessing = RAMPreprocessing {
    min_bytecode_address: 0x80000000,  // RAM_START_ADDRESS
    bytecode_words: [
        0x00b50533,  // First instruction (ADD) at 0x80000000
        0x00008067,  // Second instruction (RET) at 0x80000004
    ],
}
```

***

#### Why Does RAM Have Initial Non-Zero State?

**Key insight**: In RISC-V, **program code lives in RAM**! (Von Neumann architecture)

##### Clarification: CPU, Memory, and Execution

When we say "memory" or "RAM", we're talking about the **main memory** that the **CPU** accesses:

```

+--------------------------------------------------------------+
|                          CPU CORE                             |
|  +-----------------+                                         |
|  | Program Counter | \leftarrow  Points to RAM address (e.g. 0x80000000)|
|  |      (PC)       |                                          |
|  +--------┬--------+                                         |
|           |                                                   |
|           ↓ "Read instruction at this address"               |

+-----------┼--------------------------------------------------+
            |
            ↓ CPU reads from RAM to know what to do next

+-----------┴--------------------------------------------------+
|                        RAM (Main Memory)                      |
|                                                                |
|  0x80000000: [instruction 1]  \leftarrow  CPU reads this when PC=0x80000000|
|  0x80000004: [instruction 2]  \leftarrow  CPU reads this when PC=0x80000004|
|  0x80000008: [instruction 3]                                  |
|  0x80010000: [variable data]  \leftarrow  CPU reads/writes data here   |

+----------------------------------------------------------------+
```

**The execution cycle:**
1. **Fetch**: CPU reads instruction from RAM[PC address]
2. **Decode**: CPU figures out what the instruction means
3. **Execute**: CPU performs the operation (may read/write MORE RAM for data)
4. **Update**: PC moves to next instruction address
   - **Normal instructions**: PC = PC + 4 (each RISC-V instruction is 4 bytes)
   - **Jumps**: PC = target_address (non-sequential)
   - **Branches**: PC = PC + offset (if condition true) or PC + 4 (if false)

So yes, **the CPU accesses RAM to load the program**, and **the CPU reads from RAM addresses to know what to do next**. The program counter (PC) always points to a RAM address containing the next instruction.

##### Memory Layout with Program Code

**Memory layout**:
```
Address Range           Purpose                  Initial Value

-----------------------------------------------------------------
0x50000000-0x50001000   Input region            0 (filled with inputs)
0x51000000-0x51001000   Output region           0 (written during execution)
0x52000000-0x52001000   Trusted advice          0 (or provided data)
0x53000000-0x53001000   Untrusted advice        0 (or provided data)
0x7fff0000-0x80000000   Stack                   0 (grows downward)
0x80000000-0x80000008   Program code (DRAM)     0x00b50533 \leftarrow  NON-ZERO!
0x80000008-0x80000010   Program code            0x00008067 \leftarrow  NON-ZERO!
0x80000010-0x80100000   Heap/Rest of DRAM       0 (dynamic allocation)
```

**Why program code is in RAM**:
- Program counter (PC) points to RAM addresses
- CPU fetches instructions by reading RAM at address PC
- For our program, PC starts at `0x80000000`
- Instructions are stored as data in RAM
- Without code in RAM, CPU wouldn't know what to execute!

**Most of RAM is still zero**:
- Program occupies tiny portion (2 instructions = 8 bytes)
- Stack/heap start at zero
- Only non-zero values are stored (sparse representation)

***

#### Twist with Non-Zero Initial State

**Memory checking equation**:
$$\text{final}[a] = \text{init}[a] + \sum_{t=0}^{T-1} \text{inc}[t, a]$$

**For each address** $a$:

| Address Type | Example | $\text{init}[a]$ | $\text{inc}[t, a]$ | $\text{final}[a]$ |
|--------------|---------|------------------|--------------------|--------------------|
| Program code | 0x80000000 | 0x00b50533 | 0 (never written) | 0x00b50533 |
| Stack/heap | 0x80001000 | 0 | Sum of writes | Accumulated value |
| Never accessed | 0x90000000 | 0 | 0 | 0 |

**Example execution**:
```
Cycle 0: Fetch instruction at PC=0x80000000

  - Read RAM[0x80000000] \rightarrow  gets 0x00b50533 (ADD instruction)
  - inc[0, 0x80000000] = 0 (read, not write)

Cycle 1: Execute ADD (operates on registers, not RAM)

  - No RAM access
  - inc[1, a] = 0 for all a

Cycle 2: Fetch instruction at PC=0x80000004

  - Read RAM[0x80000004] \rightarrow  gets 0x00008067 (RET instruction)
  - inc[2, 0x80000004] = 0 (read, not write)

Final: RAM unchanged (instructions never modified)
```

***

#### What Gets Committed vs. What's Public

**PUBLIC (in preprocessing, not committed)**:
- `RAMPreprocessing`: Initial RAM state
  - Sparse representation of non-zero initial values
  - Both prover and verifier have this

**COMMITTED (during proving)**:
```rust
CommittedPolynomial::RamInc      // inc[t, a]: memory increments each cycle
CommittedPolynomial::RamRa(0..d) // One-hot encoding of accessed addresses
```

**Why initial state is public**:
- It's determined by the program binary (ELF file)
- Both parties process the same ELF during preprocessing
- No security benefit to hiding program code
- Verifier needs it to check Twist equation

**3. Memory Layout**:

```rust
memory_layout = MemoryLayout {
    // Program
    program_size:               8,              // 2 instructions \times  4 bytes

    // I/O regions
    input_start:                0x50000000,
    input_end:                  0x50001000,     // 4KB for inputs
    max_input_size:             4096,

    output_start:               0x51000000,
    output_end:                 0x51001000,     // 4KB for outputs
    max_output_size:            4096,

    // Advice regions (for passing large data to guest)
    trusted_advice_start:       0x52000000,
    trusted_advice_end:         0x52001000,
    max_trusted_advice_size:    4096,

    untrusted_advice_start:     0x53000000,
    untrusted_advice_end:       0x53001000,
    max_untrusted_advice_size:  4096,

    // Stack and heap
    stack_end:                  0x80000000,     // 64KB stack
    stack_size:                 65536,
    memory_end:                 0x80100000,     // 1MB DRAM
    memory_size:                1048576,

    // Special addresses
    panic:                      0x51001000,     // Panic flag
    termination:                0x51001008,     // Termination flag
    io_end:                     0x51001010,
}
```

**Important: Memory layout is PUBLIC, NOT committed**

**How the verifier receives it**:

Location: [`jolt-core/src/guest/verifier.rs:47-55`](../jolt-core/src/guest/verifier.rs#L47)

```rust
pub fn verify<F, PCS, FS>(
    // ... other params
    preprocessing: &JoltVerifierPreprocessing<F, PCS>,
) {
    // Memory layout comes from preprocessing.shared.memory_layout
    let memory_config = MemoryConfig {
        max_untrusted_advice_size: preprocessing.shared.memory_layout.max_untrusted_advice_size,
        max_trusted_advice_size: preprocessing.shared.memory_layout.max_trusted_advice_size,
        max_input_size: preprocessing.shared.memory_layout.max_input_size,
        max_output_size: preprocessing.shared.memory_layout.max_output_size,
        stack_size: preprocessing.shared.memory_layout.stack_size,
        memory_size: preprocessing.shared.memory_layout.memory_size,
        program_size: Some(preprocessing.shared.memory_layout.program_size),
    };
    // Use this to reconstruct I/O device
    let mut io_device = JoltDevice::new(&memory_config);
}
```

**Serialization**:

The `MemoryLayout` struct derives `CanonicalSerialize` and `CanonicalDeserialize`:

```rust
#[derive(CanonicalSerialize, CanonicalDeserialize)]
pub struct MemoryLayout { /* ... */ }

// It's part of JoltSharedPreprocessing:
pub struct JoltSharedPreprocessing {
    pub bytecode: BytecodePreprocessing,
    pub ram: RAMPreprocessing,
    pub memory_layout: MemoryLayout,  // \leftarrow  Serialized in plain
}
```

**Verifier uses memory layout to**:
- **Map addresses**: Convert guest memory addresses to witness polynomial indices
- **Validate I/O**: Check that inputs/outputs are within their designated regions
- **Reconstruct I/O device**: Create the JoltDevice with correct memory regions
- **Verify outputs**: Ensure claimed outputs come from the output memory region

**Why not committed?**:
- Memory layout is **deterministic** from the program binary and configuration
- Both prover and verifier compute the same layout from public information
- No need to hide it - it's part of the program definition
- Committing would add unnecessary cryptographic overhead

**Security implication**:
- The verifier trusts that memory layout matches the actual program
- This is fine because both prover and verifier process the same ELF binary during preprocessing
- If a malicious prover tries to use a different layout, their proof will be invalid

**4. Generators (Dory SRS)**:

The verifier receives a **structured reference string** for polynomial commitments:

```rust
generators = DoryVerifierSetup {
    // Elliptic curve points for verification (BN254 curve)
    g2_vec: Vec<G_2>,      // sqrt(2^max_log_n) G_2 points
    g_fin: G_2,            // 1 G_2 point

    max_log_n: 20,        // Can verify polynomials up to 2^20 coefficients
}
```

**Dory SRS size calculation**:
- Each G_2 point on BN254 = 64 bytes (compressed)
- For max_log_n = 20: needs sqrt(2^20) = 1024 G_2 points
- Verifier SRS size = (1024 + 1) \times  64 bytes = **64 KB**

**What this enables**:
- Verifier can check opening proofs: "polynomial P committed as C evaluates to v at point r"
- Uses Dory's recursive inner-product argument (details in Stage 5 verification below)

**Full preprocessing size** (for our toy program):
```
Verifier preprocessing size:

- Generators (Dory SRS):       ~64 KB  (1024 G_2 points for max_log_n=20)
- Bytecode (full):             ~40 bytes (2 instructions \times  ~20 bytes/instruction)
- RAM initial state:           ~16 bytes (2 words)
- Memory layout:               ~80 bytes
----------------------------------------
Total:                         ~64 KB

Compare to program size:       8 bytes

Ratio: ~8000x larger (dominated by Dory SRS)
```

**Note**: The preprocessing is large due to the Dory SRS, not the bytecode. The actual program data (bytecode + RAM + layout) is only ~136 bytes.

**SRS size scales with max polynomial degree**:
- max_log_n=18: 512 G_2 points = 32 KB
- max_log_n=20: 1024 G_2 points = 64 KB
- max_log_n=24: 4096 G_2 points = 256 KB

**Key insight**: Preprocessing is expensive (one-time cost), but enables efficient verification of all future executions of the same program.

***

### Understanding Memory Address Spaces: RAM vs. Registers

Now that we've covered preprocessing, let's clarify two separate address spaces that Jolt tracks:

#### 1. RAM Address Space (Physical Memory Addresses)

These are the **actual RISC-V memory addresses** that appear in load/store instructions:

**Complete RAM address map**:
```
Physical Address     Purpose                         Size        Initial State

--------------------------------------------------------------------------------
0x52000000          Trusted advice start            4 KB        0 (or provided)
0x52001000          Trusted advice end
0x53000000          Untrusted advice start          4 KB        0 (or provided)
0x53001000          Untrusted advice end
0x50000000          Input region start              4 KB        0 (filled with inputs)
0x50001000          Input region end
0x51000000          Output region start             4 KB        0 (written during execution)
0x51001000          Panic flag address              8 bytes     0
0x51001008          Termination flag address        8 bytes     0
0x51001010          I/O region end
0x7fff0000          Stack top (grows down)          64 KB       0
0x80000000          RAM/DRAM start                  1 MB        Program instructions
                    (Program code lives here)
0x80100000          RAM/DRAM end
```

**The Problem: Sparse vs. Dense Addresses**:

RISC-V uses **sparse physical addresses** with huge gaps:
```
Address Space:
0x50000000  \leftarrow  Input region
   ... [MASSIVE 512 MB GAP] ...
0x80000000  \leftarrow  RAM/code region
```

But **Twist memory checking** needs **dense, contiguous indices**:
```
Twist array: [0, 1, 2, 3, 4, ..., K-1]
             ↑  ↑  ↑  ↑  ↑
             No gaps! Every index from 0 to K-1
```

**Why Twist needs dense indices:**
- Twist proves: `final[i] = init[i] + sum of increments[i]` for each memory cell `i`
- The polynomials are indexed `0, 1, 2, ..., K-1` (dense)
- Can't have a polynomial with indices `[0x50000000, 0x80000000]` (huge sparse gaps waste space)

**Solution: Address Remapping** (happens during **trace generation**, not preprocessing!)

**Remap function** ([`jolt-core/src/zkvm/ram/mod.rs:94`](../jolt-core/src/zkvm/ram/mod.rs#L94)):
```rust
pub fn remap_address(address: u64, memory_layout: &MemoryLayout) -> Option<u64> {
    if address == 0 {
        return None;  // Zero address = no memory access
    }

    if address >= memory_layout.trusted_advice_start {
        // Map sparse physical address \rightarrow  dense witness index
        Some((address - memory_layout.trusted_advice_start) / 8 + 1)
    } else {
        panic!("Unexpected address {address}")
    }
}
```

**Concrete example**:
```
Physical Address (sparse)    \rightarrow     Witness Index (dense)

---------------------------------------------------------
0x80000000 (first RAM byte)  \rightarrow     1
0x80000008 (second doubleword) \rightarrow   2
0x80000010 (third doubleword)  \rightarrow   3
0x52000000 (trusted advice)    \rightarrow   computed from offset
```

**When does remapping happen?**
- **NOT during preprocessing** (preprocessing just defines memory layout)
- **During trace generation** (prover side): When execution accesses memory, physical addresses are remapped to witness indices
- **During verification**: Verifier never sees physical addresses! Only sees witness polynomials indexed `[0..K-1]`

**What preprocessing provides for remapping:**
- `memory_layout` contains address boundaries (trusted_advice_start, ram_start, etc.)
- Used by remap function to compute offsets
- Both prover and verifier know the same memory layout (public)

#### 2. Register Address Space (Separate from RAM!)

##### What Are Registers Physically/Concretely?

**In a real CPU:**
```

+---------------------------------------------------------+
|                      CPU DIE                             |
|  +-------------------------------------------------+   |
|  |             ALU (Arithmetic Logic Unit)          |   |
|  |          (Does ADD, MUL, XOR, etc.)             |   |
|  +-------------------------------------------------+   |
|                          ↕                               |
|  +-------------------------------------------------+   |
|  |         REGISTER FILE (32 registers)            |   |
|  |   +------┬------┬------┬------┬-----┬------+   |   |
|  |   | x0=0 | x1   | x2   | x3   | ... | x31  |   |   |
|  |   +------┴------┴------┴------┴-----┴------+   |   |
|  |   \leftarrow  32 slots, each 64 bits (8 bytes)            |   |
|  |   \leftarrow  PHYSICALLY: Flip-flops on CPU silicon       |   |
|  |   \leftarrow  SPEED: ~1 CPU cycle access (< 1 nanosecond) |   |
|  +-------------------------------------------------+   |

+---------------------------------------------------------+
                         ↕ (very slow by comparison)

+---------------------------------------------------------+
|                        RAM CHIPS                         |
|  (Off CPU die, connected via memory bus)                |
|  SPEED: ~100 CPU cycles access (~100 nanoseconds)       |

+---------------------------------------------------------+
```

**Key physical differences:**

| Aspect | Registers | RAM |
|--------|-----------|-----|
| **Location** | Inside CPU core (on-die) | Separate chips (off-die) |
| **Technology** | SRAM/flip-flops (transistors configured as latches) | DRAM (capacitors that need refreshing) |
| **Size** | 32 registers \times  8 bytes = **256 bytes total** | Gigabytes (GB) |
| **Speed** | ~0.3 nanoseconds (1 cycle) | ~100 nanoseconds (100+ cycles) |
| **Access** | Direct wiring to ALU | Via memory bus |
| **Power** | Always powered/fast | Lower power but slower |

**Why registers exist:**
- CPU can't operate directly on RAM (too slow)
- Need fast scratch space for immediate computations
- Example: `ADD x10, x11, x12` reads from registers x11, x12 in 1 cycle, not from RAM (which would take 100+ cycles)

##### In Jolt's Emulator (tracer/)

When Jolt **emulates** RISC-V, registers are just a Rust array:

```rust
// tracer/src/emulator/cpu.rs (simplified)
pub struct CPU {
    pub registers: [u64; 32],  // 32 RISC-V registers (just a Rust array!)
    pub pc: u64,               // Program counter
    pub memory: Memory,        // Separate from registers
}
```

**Physically in the tracer**: Registers live in the tracer's RAM (ironic!), but they **simulate** the CPU's register file.

##### In Jolt's Verification (Twist Polynomial)

**RISC-V has 32 general-purpose registers** + **32 Jolt virtual registers** = **64 total**

**Complete register map**:
```
Register Index   RISC-V Name    Purpose (CONVENTION, not enforced)    Initial Value

--------------------------------------------------------------------------------------
0                x0 (zero)      Hardwired to 0 (ONLY enforced one!)   0 (always!)
1                x1 (ra)        Return address (convention)           0
2                x2 (sp)        Stack pointer (convention)            0x7fffffff
3                x3 (gp)        Global pointer (convention)           varies
4                x4 (tp)        Thread pointer (convention)           varies
5                x5 (t0)        Temporary (convention)                0
6                x6 (t1)        Temporary (convention)                0
7                x7 (t2)        Temporary (convention)                0
8                x8 (s0/fp)     Saved / Frame pointer (convention)    0
9                x9 (s1)        Saved register (convention)           0
10               x10 (a0)       Argument / return value (convention)  input[0]
11               x11 (a1)       Argument / return value (convention)  input[1]
12-17            x12-x17        Arguments (a2-a7) (convention)        inputs
18-27            x18-x27        Saved registers (s2-s11) (convention) 0
28-31            x28-x31        Temporaries (t3-t6) (convention)      0
32-63            virtual_0-31   Jolt virtual registers                0
                                (used in virtual sequences)
```

**CRITICAL CLARIFICATION:**

**Only x0 is actually enforced by hardware:**
-  **x0 (zero)**: Always reads as 0, writes are ignored (hardwired)
-  **x1-x31**: Can be used for ANYTHING! Not enforced!

**The "Purpose" column shows CONVENTION, not restrictions:**
- **Convention**: Software agreement (compilers, ABI) on how to use registers
- **Not enforced**: CPU doesn't care if you use x1 for addition instead of return address
- **Example**: You could write `ADD x1, x2, x3` (using x1 as destination) - totally valid!

**Why have conventions?**
- **Function calls**: Caller and callee agree where arguments/return values are
- **Stack management**: Everyone agrees x2 = stack pointer
- **Interoperability**: Code from different compilers can call each other

**What IS hardwired:**
```
x0 = 0 (ALWAYS!)
  \rightarrow  Reading x0 returns 0
  \rightarrow  Writing to x0 does nothing (value stays 0)
  \rightarrow  This is enforced by the CPU hardware
```

**What is NOT hardwired (x1-x31):**
```
x1 = "return address" \rightarrow  Just a convention!
  \rightarrow  You CAN write: ADD x1, x5, x6 (use x1 as temp)
  \rightarrow  CPU doesn't care!
  \rightarrow  But you'll break function calls if you do this

x10 = "return value" \rightarrow  Just a convention!
  \rightarrow  Compilers agree to put return values here
  \rightarrow  But hardware doesn't enforce it
  \rightarrow  You could return values in x17 if you wanted (but breaks ABI)
```

**Example breaking convention (valid but bad):**
```assembly
# Convention says x10 is return value
# But I can totally do this:
ADD x10, x11, x12   # Use x10 as regular temp register
# Hardware:  Valid!
# ABI:  You just broke function calling convention!
```

**Which registers are used for ADD/MUL/etc?**

**Short answer**: ANY registers! No convention restricts which registers you use for operations.

**The convention is about DATA PLACEMENT, not OPERATION USAGE:**

| Register | Convention Says | Meaning |
|----------|----------------|---------|
| x10-x17 | Function arguments/return values | **WHERE** to put data before/after calls |
| x5-x7, x28-x31 | Temporaries | Can be freely used **within** a function |
| x8-x9, x18-x27 | Saved registers | Must be preserved **across** function calls |
| x1 | Return address | **WHERE** return address goes |
| x2 | Stack pointer | **WHERE** stack pointer is tracked |

**Examples of valid ADD operations (all legal):**
```assembly
ADD x5, x6, x7      # Using temporaries (common)
ADD x10, x11, x12   # Using argument registers (valid!)
ADD x20, x21, x22   # Using saved registers (valid if you saved them)
ADD x1, x2, x3      # Using return addr & stack pointer (BAD idea, but valid!)
```

**Real example from compiled code:**
```assembly
my_function:              # Inputs: a in x10, b in x11 (convention)
    ADD x5, x10, x11      # Compute a+b, store in temp register x5
    MUL x6, x5, x10       # Compute (a+b)*a, store in temp register x6
    ADD x10, x6, x11      # Final result in x10 (return value convention)
    RET                   # Return (x1 has return address)
```

**The compiler chooses registers based on:**
1. **Availability**: Which registers are free right now?
2. **Convention**: Use x5-x7, x28-x31 for temps (don't need saving)
3. **Optimization**: Minimize register moves

**Summary:**
-  NO "ADD must use specific registers" convention
-  Convention only says: "Put args in x10-x17, return in x10"
-  You can use ANY register for ADD/MUL/SUB/etc.
-  Compiler decides which registers based on availability

**Where do registers live in Jolt's architecture?**
- **During execution** (tracer): In the emulator's CPU struct as a Rust array
- **During proving**: Encoded into register witness polynomials (RdInc, RS1Ra, RS2Ra, RDWa)
- **During verification**: Proven via separate Twist instance (NOT part of RAM checking!)

##### How Many Registers Does the CPU Actually Have?

**Important distinction:**

**Architectural Registers** (what RISC-V ISA defines):
- RISC-V specification: **32 general-purpose registers** (x0-x31)
- These are what the **programmer sees** in the instruction set
- Example: `ADD x10, x11, x12` uses architectural registers

**Jolt's registers:**
- 32 RISC-V architectural registers + **32 Jolt virtual registers** = **64 total**
- The 32 virtual registers (indices 32-63) are **not part of RISC-V**
- They're a Jolt extension for virtual instruction sequences
- Used as scratch space in virtual sequences (invisible to regular RISC-V code)

**Physical Registers** (what's actually in a real CPU):
- Modern CPUs have **MANY MORE** physical registers than 32!
- Example: Intel Core i7 has ~180 physical registers
- Example: AMD Ryzen has ~180 physical registers
- Example: Apple M1 has ~600+ physical registers

**Why the mismatch?**

This is called **register renaming** (an optimization technique):

```
Architectural View (programmer sees):

+------------------------------------+
|  32 registers: x0, x1, ..., x31    |  \leftarrow  RISC-V ISA specification

+------------------------------------+

Physical Reality (inside modern CPU):

+------------------------------------+
|  180+ physical registers           |  \leftarrow  Actual hardware (still RISC-V!)

|                                     |
|  Mapping table:                    |  \leftarrow  Microarchitecture optimization

|    x0  \rightarrow  physical reg 47           |     (invisible to programmer)

|    x1  \rightarrow  physical reg 102          |
|    x10 \rightarrow  physical reg 5            |
|    ...                             |

+------------------------------------+
```

**Note**: This is still RISC-V! The hardware implements the RISC-V ISA spec (32 architectural registers), but the internal implementation (microarchitecture) uses more physical registers for performance. The programmer only sees the 32 architectural registers.

**Why do real CPUs have more physical registers?**
- **Out-of-order execution**: CPU can execute instructions in parallel if they don't conflict
- **Register renaming**: Maps architectural registers to physical registers dynamically
- **Avoids false dependencies**: Multiple instructions can use "x10" simultaneously by mapping to different physical registers

**Example:**
```assembly
ADD x10, x11, x12   # x10 = x11 + x12
MUL x13, x10, x14   # x13 = x10 * x14  (depends on previous x10)
ADD x10, x15, x16   # x10 = x15 + x16  (DIFFERENT x10!)
```

With register renaming:

- First `x10` \rightarrow  physical register 47
- Second `x10` \rightarrow  physical register 102 (different!)
- Both ADD instructions can execute in parallel (no conflict)

**In Jolt's emulator:**
- Only emulates the **32 architectural registers** (what RISC-V specifies)
- Plus 32 virtual registers for Jolt's virtual sequences
- **Does NOT emulate register renaming** (that's a CPU optimization, not part of ISA)
- Simple, sequential execution (no out-of-order, no parallelism)

**Summary:**
- **RISC-V ISA**: 32 architectural registers (the core/specification)
- **Jolt**: 32 RISC-V + 32 virtual = 64 registers (what we track)
- **Real CPU**: 180+ physical registers (optimization invisible to programmer)

**Register addressing in Twist**:

Registers use a separate Twist instance with **fixed parameters**:

- `K = 64` (32 RISC-V + 32 Jolt virtual)
- `d = 1` (no chunking needed)

**No remapping needed** - register indices are already dense `[0..63]`.

**Example instruction**: `ADD x10, x11, x12`
- Reads register `11` (x11 = rs1, first source register)
- Reads register `12` (x12 = rs2, second source register)
- Writes register `10` (x10 = rd, destination register)
- These are **direct indices** into the register Twist instance

**Yes, when you see `ADD r1, r3, r20` in a cycle:**
- `r1`, `r3`, `r20` refer to **architectural registers** (indices 1, 3, 20)
- These map to the **first 32 registers** (x0-x31) in Jolt's 64-register array
- Jolt's emulator (tracer) uses these exact indices to access `cpu.registers[1]`, `cpu.registers[3]`, `cpu.registers[20]`
- The witness polynomials (RS1Ra, RS2Ra, RDWa) encode which registers were accessed each cycle
- Example: Cycle 5 reads registers 3 and 20, writes to register 1

**RISC-V instruction format:**
```
ADD rd, rs1, rs2
    ↓   ↓    ↓
    destination, source1, source2

Example: ADD x1, x3, x20
  \rightarrow  x1 = x3 + x20
  \rightarrow  Register indices: rd=1, rs1=3, rs2=20
  \rightarrow  Jolt tracks these as witness data in register Twist
```

**Comparison**:

| Feature | RAM | Registers |
|---------|-----|-----------|
| **Address space** | Sparse (0x50000000 - 0x80100000) | Dense (0-63) |
| **Remapping** | Required | Not needed |
| **Twist K** | Variable (~4096-1048576) | Fixed (64) |
| **Twist d** | Variable (~2-4) | Fixed (1) |
| **Initial state** | Program code + zeros | Mostly zeros |
| **Accessed by** | Load/Store instructions | All instructions |

**Why separate**?:
- Different access patterns (RAM sparse, registers dense)
- Different sizes (RAM huge, registers tiny)
- Independent Twist instances allow different optimizations
- Registers hardcoded in bytecode (rs1, rs2, rd fields)

***

### Memory Operations Example

Let's trace memory operations for our ADD program:

```rust
#[jolt::provable]
fn add_two_numbers(a: u32, b: u32) -> u32 {
    a + b
}
```

**Compiled to**:
```
0x80000000: ADD x10, x10, x11   // x10 = x10 + x11
0x80000004: RET                 // return
```

**Execution trace**:

| Cycle | PC | Instruction | RAM Access | Register Access |
|-------|-----------|-------------|------------|-----------------|
| 0 | 0x80000000 | (fetch) | Read RAM[0x80000000] \rightarrow  0x00b50533 | - |
| 1 | 0x80000000 | ADD | - | Read reg[10], reg[11]; Write reg[10] |
| 2 | 0x80000004 | (fetch) | Read RAM[0x80000004] \rightarrow  0x00008067 | - |
| 3 | 0x80000004 | RET | - | Read reg[1] (return address) |

**Witness polynomials generated**:

**RAM**:
- `RamInc[0]` = 0 (fetch is a read, not a write)
- `RamInc[1]` = 0 (no RAM access)
- `RamInc[2]` = 0 (fetch is a read)
- `RamInc[3]` = 0 (no RAM access)
- `RamRa[0][0]` = one-hot encoding of remapped address for 0x80000000

**Registers**:
- `RdInc[0]` = 0 (fetch doesn't touch registers)
- `RdInc[1]` = result of a + b (written to x10)
- `RdInc[2]` = 0
- `RdInc[3]` = 0
- `ra_rs1[1]` = one-hot encoding of register 10 (first operand)
- `ra_rs2[1]` = one-hot encoding of register 11 (second operand)
- `wa_rd[1]` = one-hot encoding of register 10 (destination)

### 2. Public Inputs/Outputs

```rust
pub fn verify<F, PCS, FS>(
    inputs_bytes: &[u8],
    trusted_advice_commitment: Option<<PCS as CommitmentScheme>::Commitment>,
    outputs_bytes: &[u8],
    proof: JoltProof<F, PCS, FS>,
    preprocessing: &JoltVerifierPreprocessing<F, PCS>,
)
```

**Mathematical interpretation**:
- These are **public** values that both prover and verifier agree on
- The verifier will check that the claimed execution trace is consistent with these I/O values

### 3. The Proof (`JoltProof`)

**Location**: [`jolt-core/src/zkvm/dag/proof_serialization.rs`](../jolt-core/src/zkvm/dag/proof_serialization.rs)

#### Proof Structure (What the Verifier Receives)

```rust
pub struct JoltProof<F: JoltField, PCS: CommitmentScheme, FS: Transcript> {
    opening_claims: Claims<F>,
    commitments: Vec<PCS::Commitment>,
    proofs: Proofs<F, PCS, FS>,
    untrusted_advice_commitment: Option<PCS::Commitment>,
    trace_length: usize,
    ram_K: usize,
    bytecode_d: usize,
    twist_sumcheck_switch_index: usize,
}

pub type Proofs<F, PCS, FS> = BTreeMap<ProofKeys, ProofData<F, PCS, FS>>;

pub enum ProofKeys {
    Stage1Sumcheck,           // Spartan outer sumcheck only
    Stage2Sumcheck,           // Spartan + Registers + RAM + Lookups (batched)
    Stage3Sumcheck,           // Spartan + Registers + RAM + Lookups (batched)
    Stage4Sumcheck,           // RAM + Bytecode + Lookups (batched)
    ReducedOpeningProof,      // Stage 5: Dory batched opening
    TrustedAdviceProof,       // Optional
    UntrustedAdviceProof,     // Optional
}

pub enum ProofData<F, PCS, FS> {
    SumcheckProof(SumcheckInstanceProof<F, FS>),
    ReducedOpeningProof(ReducedOpeningProof<F, PCS, FS>),
    OpeningProof(PCS::Proof),
}

pub struct SumcheckInstanceProof<F, FS> {
    compressed_polys: Vec<CompressedUniPoly<F>>,
}

pub struct ReducedOpeningProof<F, PCS, FS> {
    sumcheck_proof: SumcheckInstanceProof<F, FS>,
    sumcheck_claims: Vec<F>,
    joint_opening_proof: PCS::Proof,
}
```

**`ReducedOpeningProof` fields**:
- **`sumcheck_proof`**: Proves the random linear combination of all polynomial openings
  - This sumcheck verifies: $\sum_{i} r_i \cdot \widetilde{P_i}(\vec{x}) = \text{claimed\_value}$
  - Batches ~50 opening claims into a single sumcheck over a "joint polynomial"
- **`sumcheck_claims`**: The final evaluation points (challenges) from the sumcheck
  - After sumcheck completes, need to check the joint polynomial at these random points
- **`joint_opening_proof`**: Dory PCS proof that the joint polynomial opens correctly
  - Type is `PCS::Proof`, which for Dory is `DoryProofData`

**For Dory PCS** (`PCS::Proof = DoryProofData`):

```rust
pub struct DoryProofData {
    sigma: usize,
    dory_proof_data: DoryProof<G1, G2, GT>,
}
```

**`DoryProofData` fields**:
- **`sigma`**: The number of rows in the Dory matrix representation
  - Dory views polynomial coefficients as a matrix (rows \times  columns)
  - This parameter allows batching polynomials of different sizes by padding rows
- **`dory_proof_data`**: The actual Dory proof from the external `dory` crate
  - Contains the recursive inner-product argument proof
  - Proves $\langle \text{polynomial coefficients}, \text{evaluation basis} \rangle = \text{claimed value}$
  - Consists of logarithmically-many group elements (G_1 and G_2 points)
  - Verifier uses pairings to check the inner-product relation

**Dory verification** (final step):
- Uses bilinear pairings: $e: \mathbb{G}_1 \times \mathbb{G}_2 \to \mathbb{G}_T$
- Checks that commitment opens to claimed evaluation via pairing equation
- Logarithmic verification time: O(log n) pairings for n-sized polynomial
- No trusted setup required (transparent)

**What sumcheck instances are in each stage?**

Each `Stage*Sumcheck` contains a **batched sumcheck proof** (multiple sumcheck instances combined):

| Stage | Components | Specific Sumchecks (22 total across stages 1-4) |
|-------|------------|-------------------|
| **Stage 1** (1) | Spartan only | 1. **Outer sumcheck**: Proves $\sum_x \text{eq}(\tau, x) \cdot (Az(x) \cdot Bz(x) - Cz(x)) = 0$ |
| **Stage 2** (6) | Spartan + Registers + RAM + Instruction Lookups | 1. **Spartan InnerSumcheck** (batches Az, Bz, Cz with RLC)<br>2. **Registers ReadWriteChecking**<br>3. **RAM RafEvaluation**<br>4. **RAM ReadWriteChecking**<br>5. **RAM OutputSumcheck**<br>6. **Instruction Lookups BooleanitySumcheck** |
| **Stage 3** (8) | Spartan + Registers + RAM + Instruction Lookups | 1. **Spartan PCSumcheck**<br>2. **Spartan ProductVirtualizationSumcheck**<br>3. **Registers ValEvaluationSumcheck**<br>4. **RAM ValEvaluationSumcheck**<br>5. **RAM ValFinalSumcheck**<br>6. **RAM HammingBooleanitySumcheck**<br>7. **Instruction Lookups ReadRafSumcheck**<br>8. **Instruction Lookups HammingWeightSumcheck** |
| **Stage 4** (7) | RAM + Bytecode + Instruction Lookups | 1. **RAM HammingWeightSumcheck**<br>2. **RAM BooleanitySumcheck**<br>3. **RAM RaSumcheck**<br>4. **Bytecode ReadRafSumcheck**<br>5. **Bytecode BooleanitySumcheck**<br>6. **Bytecode HammingWeightSumcheck**<br>7. **Instruction Lookups RaSumcheck** |
| **Stage 5** (1) | All committed polynomials | **Dory batched opening proof** for all ~25-30 accumulated polynomial evaluations (not a sumcheck) |

**Note**: Az, Bz, Cz are conceptually 3 separate claims but proven together in 1 batched sumcheck instance (Spartan InnerSumcheck) using random linear combination. This table shows actual implementation counts verified from code.

**Why this staging?**
- **Dependencies**: Later stages depend on claims from earlier stages
- **Batching**: Multiple independent sumchecks in the same stage are batched together
- **Parallelization**: Sumchecks in the same stage can be computed in parallel (prover-side)

**What is "Advice"?**

Advice = auxiliary private data the prover provides to the guest program (beyond the public inputs). Think of it as "witness data" for the program.

**TRUSTED ADVICE:**
- Prover commits to trusted advice **before** proving
- Commitment sent to verifier as part of preprocessing
- Used for: Private inputs that must be binding (e.g., private keys, secrets)
- Memory region: `0x52000000-0x52001000` (4 KB)

**UNTRUSTED ADVICE:**
- Prover provides untrusted advice **during** proving (no prior commitment)
- Verifier receives commitment in the proof
- Used for: Non-deterministic witness data, optimization hints
- Example: Square root witness, intermediate computation results
- Memory region: `0x53000000-0x53001000` (4 KB)

**Why two types?**
- Trusted: For values that must be fixed before proving starts
- Untrusted: For values the prover can choose freely during proving
- Both are proven correct via memory checking (Twist)
- Optional: Most programs don't use advice (thus `Option<...>`)

**How are polynomials "compressed"?**

In sumcheck, prover sends univariate $g(X) = c_0 + c_1 X + c_2 X^2 + \ldots$ each round.

**Key observation**: Verifier already knows $g(0) + g(1)$ from previous round's claim! (The sumcheck relation enforces: $g(0) + g(1) = \text{previous\_claim}$)

**Compression trick**:
- Full polynomial: $g(X) = c_0 + c_1 X + c_2 X^2 + c_3 X^3$
- Send to verifier: $[c_0, c_2, c_3]$ (omit $c_1$, the linear term!)
- Verifier recovers: $c_1 = (g(0) + g(1)) - 2c_0 - c_2 - c_3$

**Example** (degree 3):
```
Prover has: g(X) = 5 + 7X + 3X² + 2X³
Knows: g(0) + g(1) = 5 + 17 = 22 (from previous claim)
Sends: [5, 3, 2]  (3 field elements instead of 4)
Verifier recovers: c_1 = 22 - 10 - 3 - 2 = 7 
```

**Savings**: Degree-$d$ polynomial \rightarrow  send $d$ elements instead of $d+1$ (one field element saved per round)

**Code**: See [jolt-core/src/poly/unipoly.rs:243-248](../jolt-core/src/poly/unipoly.rs#L243) (compress) and lines 379-385 (decompress)

#### What Each Component Contains

**1. Commitments** (~50 Dory commitments, each 192 bytes):

Commitments to witness polynomials:

- $C_{\widetilde{L}}$: Commitment to $\widetilde{L}(j)$ (left instruction input)
- $C_{\widetilde{R}}$: Commitment to $\widetilde{R}(j)$ (right instruction input)
- $C_{\widetilde{\Delta}_{\text{rd}}}$: Commitment to $\widetilde{\Delta}_{\text{rd}}(j)$ (register write increment)
- $C_{\widetilde{\Delta}_{\text{ram}}}$: Commitment to $\widetilde{\Delta}_{\text{ram}}(j)$ (RAM write increment)
- $C_{\text{InstructionRa}(i)}$: Commitments to instruction lookup address chunks ($i = 0, \ldots, 15$)
- ... (~46 more commitments total)

**2. Stage 1-4 Sumcheck Proofs** (batched):

Each `SumcheckInstanceProof` contains:

- `compressed_polys`: $[g_0(X), g_1(X), \ldots, g_{\nu-1}(X)]$
- Where $\nu = \log_2 T$ (number of variables for trace length $T$)
- Each $g_i(X)$ is the prover's univariate polynomial for round $i$

**3. Stage 5 Reduced Opening Proof** (Dory):

`ReducedOpeningProof` contains:

- Cross-term commitments from recursive inner-product argument
- Final evaluations at random point
- $O(\log n)$ group elements for polynomial of degree $n$

**4. Opening Claims**:

Map from polynomial to claimed evaluation:

- $(\text{LeftInstructionInput}, \vec{r}) \to v_1$: Claim "$\widetilde{L}(\vec{r}) = v_1$"
- $(\text{RdInc}, \vec{r}) \to v_2$: Claim "$\widetilde{\Delta}_{\text{rd}}(\vec{r}) = v_2$"
- ... (~50 claims total, one per committed polynomial)

#### Toy Example: 2-Cycle Execution

**Setup:**
- T = 2 cycles (so log T = 1 variable)
- Simplified: only track 3 polynomials (LeftInput, RightInput, RdInc)

**Proof structure:**
```rust
JoltProof {
    // 3 commitments (in reality ~50)
    commitments: [
        C_L = Com(L̃),    // 192 bytes (Dory GT element)
        C_R = Com(R̃),    // 192 bytes
        C_\Delta = Com(\Deltã),    // 192 bytes
    ],

    // Stage 1 sumcheck proof (batched)
    proofs[Stage1Sumcheck]: SumcheckInstanceProof {
        compressed_polys: [
            g_0(X) = [a_0, a_2]  // Round 0: degree-2 univariate
        ]
    },

    // Stage 2 sumcheck proof
    proofs[Stage2Sumcheck]: SumcheckInstanceProof {
        compressed_polys: [
            g_0(X) = [b_0, b_2]  // Another univariate
        ]
    },

    // ... Stages 3-4 similar ...

    // Stage 5: Dory opening proof
    proofs[ReducedOpeningProof]: ReducedOpeningProof {
        cross_terms: [U_0],        // log(2) = 1 cross-term commitment
        final_evals: [e_1, e_2, e_3] // Evaluations at random point
    },

    // Final claims (what we're proving)
    opening_claims: {
        (LeftInput, r=0.7) \rightarrow  42,       // "L̃(0.7) = 42"
        (RightInput, r=0.7) \rightarrow  13,      // "R̃(0.7) = 13"
        (RdInc, r=0.7) \rightarrow  55,           // "\Deltã(0.7) = 55"
    },

    trace_length: 2,
    ram_K: 1024,
    bytecode_d: 2,
}
```

#### What the Verifier Does With This Proof

**Step 1**: Verify commitments are well-formed (in correct group)

**Step 2**: Run Stage 1-4 sumcheck verification
- For each stage, use `compressed_polys` to verify prover's messages
- Check that claims reduce correctly using random challenges

**Step 3**: Verify Dory opening proof (Stage 5)
- Check that all committed polynomials open to claimed values
- Uses `cross_terms` and recursive inner-product argument
- Final check: multi-exponentiation in GT group

**Step 4**: Accept if all checks pass!

#### Size Characteristics

For trace length T = 1024 (log T = 10):

```
Component                     Size

-------------------------------------------------
Commitments (50 \times  192 bytes)  ~10 KB
Stage 1 sumcheck (10 rounds)  ~500 bytes
Stage 2 sumcheck (10 rounds)  ~500 bytes
Stage 3 sumcheck (10 rounds)  ~500 bytes
Stage 4 sumcheck (10 rounds)  ~500 bytes
Stage 5 Dory opening          ~2 KB (log_2(max_poly_size))
Opening claims                ~3 KB (50 claims \times  64 bytes)

-------------------------------------------------
Total                         ~17 KB
```

**Asymptotic**: O(log T) proof size due to:
- Sumcheck: O(log T) rounds per stage
- Dory: O(log n) opening proof size

***

## Verification Pipeline

### Entry Point

**Location**: [`jolt-core/src/guest/verifier.rs:33-68`](../jolt-core/src/guest/verifier.rs#L33-L68)

```rust
pub fn verify<F, PCS, FS>(
    inputs_bytes: &[u8],
    trusted_advice_commitment: Option<<PCS as CommitmentScheme>::Commitment>,
    outputs_bytes: &[u8],
    proof: JoltProof<F, PCS, FS>,
    preprocessing: &JoltVerifierPreprocessing<F, PCS>,
) -> Result<(), ProofVerifyError>
```

This wrapper:

1. Reconstructs the `JoltDevice` (I/O memory regions) from inputs/outputs
2. Calls `JoltRV64IMAC::verify()` which calls `JoltDAG::verify()`

### Main Verification Logic

**Location**: [`jolt-core/src/zkvm/dag/jolt_dag.rs:383-570`](../jolt-core/src/zkvm/dag/jolt_dag.rs#L383-L570)

```rust
pub fn verify<'a, F, ProofTranscript, PCS>(
    mut state_manager: StateManager<'a, F, ProofTranscript, PCS>,
) -> Result<(), anyhow::Error>
```

#### What is `StateManager` and Why Does the Verifier Use It?

**Location**: [`jolt-core/src/zkvm/dag/state_manager.rs`](../jolt-core/src/zkvm/dag/state_manager.rs)

The `StateManager` is a **unified data structure** used by both prover and verifier to track state across the 5 proof stages. It operates in one of two modes:

```rust
pub struct StateManager<'a, F, ProofTranscript, PCS> {
    // Shared by both prover and verifier:
    pub transcript: ProofTranscript,              // Fiat-Shamir transcript
    pub proofs: BTreeMap<ProofKeys, ProofData>,   // Stage 1-5 proofs
    pub commitments: Vec<PCS::Commitment>,        // Polynomial commitments
    pub program_io: JoltDevice,                   // Public inputs/outputs
    pub ram_K: usize,                             // Memory size (from proof)

    // Only ONE of these is Some:
    pub prover_state: Option<ProverState<'a, F, PCS>>,
    pub verifier_state: Option<VerifierState<'a, F, PCS>>,
}
```

**For the verifier**, the `StateManager` is constructed from the `JoltProof` via:

```rust
let state_manager = proof.to_verifier_state_manager(preprocessing, program_io);
```

**What the verifier's StateManager contains** (all from the proof):
- **`commitments`**: All ~50 polynomial commitments (from `JoltProof.commitments`)
- **`proofs`**: The 5 stage proofs (from `JoltProof.proofs`)
- **`opening_claims`**: Claimed polynomial evaluations (from `JoltProof.opening_claims`)
- **`transcript`**: Fresh Fiat-Shamir transcript (initialized with same seed as prover)
- **`verifier_state.preprocessing`**: Only contains **commitments** to bytecode/tables (not actual polynomials)
- **`verifier_state.trace_length`**: Just the number $T$ (from `JoltProof.trace_length`)
- **`verifier_state.accumulator`**: Accumulates claimed evaluations during verification

**What the verifier does NOT have** (these are only in prover's StateManager):
-  Full execution trace (`Vec<Cycle>`)
-  Actual polynomials (witness data)
-  Final memory state
-  Prover preprocessing (contains full polynomial data)

**Why this design?** Code reuse! Both prover and verifier execute the same DAG stages (Stage 1-5), but with different operations:
- **Prover**: Computes sumcheck using actual polynomials
- **Verifier**: Verifies sumcheck using claimed evaluations from proof

The `StateManager` provides a unified interface for both, checking at runtime which mode it's in:

- `state.get_prover_data()` - Only prover calls (panics for verifier)
- `state.get_verifier_data()` - Only verifier calls (panics for prover)

#### Verification Flow

**High-level structure**:

```
Initialize DAG components (Spartan, RAM, Registers, Bytecode, Lookups)
    ↓
Fiat-Shamir: append commitments to transcript
    ↓
Stage 1: Verify Spartan outer sumcheck
    ↓
Stage 2: Verify batched sumcheck (Spartan, Registers, RAM, Lookups)
    ↓
Stage 3: Verify batched sumcheck (Spartan, Registers, RAM, Lookups)
    ↓
Stage 4: Verify batched sumcheck (RAM, Bytecode, Lookups)
    ↓
Stage 5: Verify batched opening proof
```

**Key observation**: The verifier's code mirrors the prover's structure, but instead of generating sumcheck proofs, it **verifies** them.

***

## Stage-by-Stage Mathematical Analysis

### Stage 0: Transcript Initialization (Fiat-Shamir)

**Location**: [`jolt-core/src/zkvm/dag/jolt_dag.rs:392-416`](../jolt-core/src/zkvm/dag/jolt_dag.rs#L392-L416)

```rust
state_manager.fiat_shamir_preamble();

// Append commitments to transcript
let commitments = state_manager.get_commitments();
let transcript = state_manager.get_transcript();
for commitment in commitments.borrow().iter() {
    transcript.borrow_mut().append_serializable(commitment);
}

// Append advice commitments
if let Some(ref untrusted_advice_commitment) = state_manager.untrusted_advice_commitment {
    transcript.borrow_mut().append_serializable(untrusted_advice_commitment);
}
if let Some(ref trusted_advice_commitment) = state_manager.trusted_advice_commitment {
    transcript.borrow_mut().append_serializable(trusted_advice_commitment);
}
```

**Mathematical meaning**:

The **Fiat-Shamir transform** converts an interactive proof into a non-interactive one:

- Interactive: Verifier sends random challenges to prover
- Non-interactive: Challenges derived deterministically from transcript hash

**Transcript state**:
```
H = Hash(public_inputs || commitments || advice_commitments)
```

All future challenges are derived by extending this hash. This binds the prover to their commitments before generating any proofs.

**Security**: Because prover cannot predict hash outputs, they cannot "optimize" their proof for specific challenges. Binding commitments first prevents the prover from tailoring witness polynomials to challenges.

***

### Understanding the Core Pattern: $\sum_t f(t) \cdot \text{eq}(r, t) = f(r)$

**Before diving into the stages, let's understand a pattern that appears everywhere in Jolt.**

#### Toy Example with Concrete Numbers

Imagine we have 4 cycles ($T = 4 = 2^2$, so $\log T = 2$ bits):

| Cycle $t$ | PC value $f(t)$ |
|-----------|-----------------|
| 00        | 100             |
| 01        | 104             |
| 10        | 108             |
| 11        | 200             |

**Question**: What is PC at the random challenge point $r = (r_1, r_2) = (0.7, 0.3)$?

**Answer**: We need $\widetilde{f}(0.7, 0.3)$ - the multilinear extension of $f$ evaluated at $(0.7, 0.3)$.

#### Computing via the Sum-Eq Pattern

$$\sum_{t \in \{0,1\}^2} f(t) \cdot \widetilde{\text{eq}}(r, t) = \widetilde{f}(r)$$

**Expand the sum**:
$$f(0,0) \cdot \widetilde{\text{eq}}(r, (0,0)) + f(0,1) \cdot \widetilde{\text{eq}}(r, (0,1)) + f(1,0) \cdot \widetilde{\text{eq}}(r, (1,0)) + f(1,1) \cdot \widetilde{\text{eq}}(r, (1,1))$$

**Recall eq polynomial definition**:
$$\widetilde{\text{eq}}((r_1, r_2), (t_1, t_2)) = (r_1 \cdot t_1 + (1-r_1)(1-t_1)) \cdot (r_2 \cdot t_2 + (1-r_2)(1-t_2))$$

**Compute each term**:

1. $\widetilde{\text{eq}}((0.7, 0.3), (0,0)) = (1-0.7)(1-0) \cdot (1-0.3)(1-0) = 0.3 \cdot 0.7 = 0.21$
2. $\widetilde{\text{eq}}((0.7, 0.3), (0,1)) = (1-0.7)(1-0) \cdot (0.3 \cdot 1) = 0.3 \cdot 0.3 = 0.09$
3. $\widetilde{\text{eq}}((0.7, 0.3), (1,0)) = (0.7 \cdot 1) \cdot (1-0.3)(1-0) = 0.7 \cdot 0.7 = 0.49$
4. $\widetilde{\text{eq}}((0.7, 0.3), (1,1)) = (0.7 \cdot 1) \cdot (0.3 \cdot 1) = 0.7 \cdot 0.3 = 0.21$

**The sum**:
$$\sum = 100 \cdot 0.21 + 104 \cdot 0.09 + 108 \cdot 0.49 + 200 \cdot 0.21$$
$$= 21 + 9.36 + 52.92 + 42 = 125.28$$

**This equals** $\widetilde{f}(0.7, 0.3)$ **by the definition of multilinear extension!**

The eq polynomial acts as **Lagrange interpolation weights** that combine the 4 corner values into the evaluation at the interior point $(0.7, 0.3)$.

#### Why You'll See This Pattern Everywhere

**In the stages below, you'll encounter patterns like**:

1. **PC Shift Check (Stage 3.1)**: $\sum_t \text{PC}(t) \cdot \widetilde{\text{eq}}_{+1}(r, t) = \text{NextPC}(r)$
   - Left side: Extracts PC value at $r+1$ using eq+1 polynomial
   - Right side: The "next PC" value claimed at point $r$
   - Check: These must be equal (shift consistency)

2. **Product Virtualization (Stage 3.2)**: $\sum_j \text{Left}(j) \cdot \text{Right}(j) \cdot \widetilde{\text{eq}}(r, j) = \text{Product}(r)$
   - Left side: Sum extracts value of (Left \times  Right) polynomial at $r$
   - Right side: The claimed Product evaluation at $r$
   - Check: Product polynomial equals Left \times  Right

3. **Register Val Evaluation (Stage 3.4)**: $\sum_{j'} \text{inc}(r', j') \cdot \text{wa}(r', j') \cdot \text{LT}(j', r_{\text{cycle}}) = \text{val}(r')$
   - Left side: Sum of all increments to register $r'$ before cycle $r_{\text{cycle}}$
   - Right side: Claimed value in register $r'$ at cycle $r_{\text{cycle}}$
   - Check: Value equals sum of increments (since registers start at 0)

**The universal pattern**: $\sum_{\text{all points}} \text{data}(\text{point}) \cdot \text{selector}(\text{point}) = \text{claimed\_value}$

- **selector** (eq, eq+1, LT) picks out specific point(s)
- **data** is the committed/virtual polynomial
- **claimed_value** is what we're checking

**Security**: By Schwartz-Zippel, if the claimed value is wrong, the sum won't match at the random challenge point with high probability.

***

## Jolt Sumcheck DAG: Complete Stage-by-Stage Map

Before diving into each stage, here's a complete visualization of all sumchecks across all 5 stages and how they connect.

### Visual Map of All Sumchecks

```

+-----------------------------------------------------------------------------+
|                           STAGE 1: R1CS CONSTRAINTS                          |
|                       "Do all CPU instructions follow the rules?"            |
|                                                                               |
|  +--------------------------------------------------------------------+     |
|  | 1.1: Spartan Outer Sumcheck (R1CS Constraint Checking)             |     |
|  |                                                                     |     |
|  | PROVES: Az ∘ Bz - Cz = 0 at ALL cycles (R1CS satisfied)           |     |
|  | INTUITION: Every CPU operation respects algebraic constraints      |     |
|  | OUTPUT: Need to verify Az(r), Bz(r), Cz(r) at random point r      |     |
|  |         ↓                                                           |     |
|  | 1.2: Three Inner Sumchecks (Matrix-Vector Products)                |     |
|  |                                                                     |     |
|  | PROVES: Az, Bz, Cz computed correctly from witness z               |     |
|  | INTUITION: The "left side", "right side", and "output" of each     |     |
|  |            instruction are the correct values from the trace        |     |
|  | OUTPUT: Virtual polynomials representing:                          |     |
|  |   • Instruction outputs (LookupOutput)                             |     |
|  |   • Instruction inputs (LeftOperand, RightOperand)                 |     |
|  |   • Program counter values (PC, NextPC, IsNoop)                    |     |
|  |   • Register addresses being accessed (rd, rs1, rs2)               |     |
|  +--------------------------------------------------------------------+     |

+-----------------------------------------------------------------------------+
                                      ↓
                   Virtual polynomial claims flow to Stage 2
                                      ↓

+-----------------------------------------------------------------------------+
|                      STAGE 2: MEMORY CONSISTENCY START                       |
|              "Do reads return the correct most-recent-write values?"         |
|                                                                               |
|  +-----------------------------------------------------------------+       |
|  | 2.1: Spartan Inner Sumchecks (Completes Stage 1)                |       |
|  |                                                                   |       |
|  | PROVES: Az = sum(A_row · z), same for Bz, Cz (3 separate checks)|       |
|  | INTUITION: The matrix multiplications are computed correctly      |       |
|  | OUTPUT: All witness components verified as inputs to constraints  |       |
|  +-----------------------------------------------------------------+       |
|                                                                               |
|  +-------------------------------+  +---------------------------------+    |
|  | 2.2: RAM Read/Write Checking  |  | 2.3: Registers Read/Write       |    |
|  |      (Twist Protocol)         |  |      Checking (Twist Protocol)  |    |
|  |                               |  |                                 |    |
|  | PROVES: RAM reads return the  |  | PROVES: Register reads return   |    |
|  |   last value written to that  |  |   the last value written to     |    |
|  |   address (memory consistency)|  |   that register (consistency)   |    |
|  | INTUITION: No memory gets     |  | INTUITION: Register file is     |    |
|  |   "corrupted" - if you write  |  |   correct - if you write 42 to  |    |
|  |   42 to addr 0x1000, reading  |  |   register x1, then read x1,    |    |
|  |   0x1000 later gives 42       |  |   you get 42                    |    |
|  | METHOD: Grand product of      |  | METHOD: Combines 2 reads + 1    |    |
|  |   fingerprints (time-ordered) |  |   write per cycle into single   |    |
|  | OUTPUT: RamVal claim (needs   |  |   fingerprint product           |    |
|  |   verification in Stage 3)    |  | OUTPUT: RegVal claim (verified  |    |
|  |                               |  |   in Stage 3)                   |    |
|  +-------------------------------+  +---------------------------------+    |
|                                                                               |
|  +-----------------------------------------------------------------+       |
|  | 2.4: Instruction Lookups Booleanity (Shout Protocol Start)       |       |
|  |                                                                   |       |
|  | PROVES: Lookup table access indicators are 0 or 1 (Boolean)      |       |
|  | INTUITION: When looking up ADD(x,y) in the table, we point to    |       |
|  |   EXACTLY ONE entry - not 0.5 entries, not 2 entries. This is    |       |
|  |   the first step: each bit of the pointer must be 0 or 1         |       |
|  | WHY: Prevents prover from "cheating" with fractional lookups      |       |
|  | OUTPUT: Booleanity verified for all 16 chunks \times  256 entries      |       |
|  +-----------------------------------------------------------------+       |

+-----------------------------------------------------------------------------+
                                      ↓
                  Output claims flow to Stage 3
                                      ↓

+-----------------------------------------------------------------------------+
|                    STAGE 3: MEMORY & LOOKUP FINALIZATION                     |
|           "Finish verifying memory consistency and instruction lookups"      |
|                                                                               |
|  +--------------------------+  +--------------------------------------+    |
|  | 3.1: PC Shift Check      |  | 3.2: Multiplication Product Check    |    |
|  |                          |  |                                      |    |
|  | PROVES: NextPC(cycle t)  |  | PROVES: Product = Left \times  Right       |    |
|  |   = PC(cycle t+1) at a   |  |   (for MUL/MULH instructions)        |    |
|  |   random point           |  | INTUITION: Multiplication outputs    |    |
|  | INTUITION: PC advances   |  |   are correct - if you compute       |    |
|  |   correctly cycle-to-    |  |   5 \times  7, the result is actually 35   |    |
|  |   cycle (no jumps to     |  | WHY: Product used in R1CS, must      |    |
|  |   invalid addresses)     |  |   be verified separately             |    |
|  | INPUT: From Stage 2.1    |  | INPUT: From Stage 1 (Spartan)        |    |
|  +--------------------------+  +--------------------------------------+    |
|                                                                               |
|  +------------------------------+  +-----------------------------------+   |
|  | 3.3: RAM Evaluation (Twist)  |  | 3.4: Registers Evaluation (Twist) |   |
|  |                              |  |                                   |   |
|  | PROVES: Final RAM state =    |  | PROVES: Register value = sum of   |   |
|  |   Initial state + All writes |  |   all writes to that register     |   |
|  | INTUITION: Memory "adds up"  |  |   before the read                 |   |
|  |   correctly - starting from  |  | INTUITION: Register values come   |   |
|  |   zeros, after N writes, the |  |   from accumulating writes (not   |   |
|  |   final value is the sum of  |  |   magically appearing)            |   |
|  |   all those writes           |  | COMPLETES: 2-stage Twist protocol |   |
|  | COMPLETES: 2-stage Twist for |  |   for register consistency        |   |
|  |   RAM consistency proof      |  | INPUT: val_claim from Stage 2.3   |   |
|  | INPUT: val_claim from 2.2    |  |                                   |   |
|  +------------------------------+  +-----------------------------------+   |
|                                                                               |
|  +-----------------------------------------------------------------+       |
|  | 3.5: Instruction Lookups (Shout Protocol Continuation)           |       |
|  |                                                                   |       |
|  | 3.5a: Hamming Weight (One-Hot Property)                          |       |
|  | PROVES: Exactly ONE table entry accessed per lookup              |       |
|  | INTUITION: You can't look up "half of ADD and half of SUB" -     |       |
|  |   each instruction accesses exactly one table entry              |       |
|  | WHY: Combined with booleanity, proves valid table access         |       |
|  | INPUT: From Stage 2.4 (booleanity check)                         |       |
|  |                                                                   |       |
|  | 3.5b: Read RAF (Table Value Correctness)                         |       |
|  | PROVES: Looked-up values match the actual table entries          |       |
|  | INTUITION: If you look up ADD(5, 7) in the addition table,       |       |
|  |   you actually get 12 (not some made-up number)                  |       |
|  | METHOD: Prefix-suffix sumcheck handles giant 2^128 tables        |       |
|  | WHY: Core of Jolt's "lookup-centric" zkVM approach               |       |
|  | INPUT: From Stage 1 (LookupOutput, operands)                     |       |
|  +-----------------------------------------------------------------+       |

+-----------------------------------------------------------------------------+
                                      ↓
                  Output claims flow to Stage 4
                                      ↓

+-----------------------------------------------------------------------------+
|                      STAGE 4: FINAL CHECKS & CLEANUP                         |
|                   "Tie up loose ends from chunked encodings"                 |
|                                                                               |
|  +-------------------------------------------------------------------+     |
|  | 4.1: RAM Hamming Weight (Address Chunking Verification)           |     |
|  |                                                                    |     |
|  | PROVES: Chunked address pieces combine correctly                  |     |
|  | INTUITION: Large memory addresses split into small chunks (e.g.,  |     |
|  |   64-bit addr = 8 chunks of 8 bits). This proves the chunks       |     |
|  |   actually represent the full address (not garbage)               |     |
|  | WHY: Twist uses chunking for efficiency, must verify correctness  |     |
|  | INPUT: From Stage 3.3 (RAM evaluation)                            |     |
|  +-------------------------------------------------------------------+     |
|                                                                               |
|  +-------------------------------------------------------------------+     |
|  | 4.2: Bytecode Read Checking (Program Integrity)                   |     |
|  |                                                                    |     |
|  | PROVES: At each cycle, executed the ACTUAL bytecode at that PC    |     |
|  | INTUITION: Prover can't lie about what instruction was at address |     |
|  |   0x1000 - the bytecode is committed in preprocessing, and every  |     |
|  |   read must match the committed values                            |     |
|  | WHY: Prevents "fake program" attacks where prover executes        |     |
|  |   different instructions than what's in the committed bytecode    |     |
|  | METHOD: Shout offline memory checking (3-stage batched)           |     |
|  | INPUT: From Stage 1 (PC values) + preprocessing (bytecode)        |     |
|  +-------------------------------------------------------------------+     |
|                                                                               |
|  +-------------------------------------------------------------------+     |
|  | 4.3: Instruction Lookups Ra Virtualization (Chunking Finalization)|     |
|  |                                                                    |     |
|  | PROVES: Virtual "ra" polynomial = product of all chunk polynomials|     |
|  | INTUITION: 128-bit lookup indices split into 16 chunks. Stage 3   |     |
|  |   used a "virtual ra" for efficiency. Now prove that virtual ra   |     |
|  |   equals the product of the actual committed chunk polynomials    |     |
|  | WHY: Connects the efficient virtual polynomial used in Stage 3    |     |
|  |   with the actual committed data (soundness)                      |     |
|  | COMPLETES: 3-stage Shout protocol for instruction lookups         |     |
|  | INPUT: From Stage 3.5b (Read RAF sumcheck)                        |     |
|  +-------------------------------------------------------------------+     |

+-----------------------------------------------------------------------------+
                                      ↓
      All VIRTUAL polynomials verified via sumcheck chains!
      Now verify all COMMITTED polynomials via cryptographic openings
                                      ↓

+-----------------------------------------------------------------------------+
|                  STAGE 5: CRYPTOGRAPHIC COMMITMENT VERIFICATION              |
|              "Prove the committed data matches the claimed evaluations"      |
|                                                                               |
|  +-------------------------------------------------------------------+     |
|  | 5a: Opening Batching Sumcheck (Efficiency Trick)                  |     |
|  |                                                                    |     |
|  | PROVES: Batch ~50 polynomial opening claims into 1 combined claim |     |
|  | INTUITION: Instead of 50 separate cryptographic proofs (expensive),|     |
|  |   randomly combine them into ONE proof. If prover lies about even |     |
|  |   one value, the combined proof fails (Schwartz-Zippel)           |     |
|  | METHOD: Random linear combination with eq polynomial weighting    |     |
|  | INPUT: All committed polynomial evaluation claims from Stages 1-4 |     |
|  | OUTPUT: Single opening claim Q(r*) = v* to verify                 |     |
|  +-------------------------------------------------------------------+     |
|                                      ↓                                        |
|  +-------------------------------------------------------------------+     |
|  | 5b: Dory PCS Opening Verification (Cryptographic Check)           |     |
|  |                                                                    |     |
|  | PROVES: Committed polynomials actually have claimed values         |     |
|  | INTUITION: Stages 1-4 assumed polynomial evaluations were correct.|     |
|  |   Now cryptographically VERIFY those evaluations using commitments|     |
|  | METHOD: Dory polynomial commitment scheme (logarithmic verifier)   |     |
|  | INPUT: Commitments from proof + claimed evaluations               |     |
|  | OUTPUT: Accept/Reject based on cryptographic check                |     |
|  +-------------------------------------------------------------------+     |
|                                                                               |
|   COMPLETE: All virtual AND committed polynomials verified!               |
|     The execution trace is valid!                                           |

+-----------------------------------------------------------------------------+
```

### Key Dependencies and Data Flow

**Virtual vs Committed Polynomials**:
- **Virtual polynomials** (proven via sumcheck chains):
  - PC, UnexpandedPC, IsNoop, NextPC
  - LookupOutput, LeftOperand, RightOperand
  - RamVal, RegVal, various fingerprints
  - Never committed, verified through sumcheck dependencies

- **Committed polynomials** (proven via Dory PCS in Stage 5):
  - Execution trace data: LeftInstructionInput, RightInstructionInput
  - Memory/Register increments: RamInc, RdInc, RsInc
  - Address indicators: RamRa, BytecodeRa, InstructionRa
  - ~50 total polynomials, batched in Stage 5

**Critical Dependencies**:
1. **Stage 1 \rightarrow  Stage 2**: Virtual polynomial claims (PC, LookupOutput, operands)
2. **Stage 2 \rightarrow  Stage 3**: Val claims from Twist read/write checking
3. **Stage 3 \rightarrow  Stage 4**: Evaluation claims for finalization
4. **Stages 1-4 \rightarrow  Stage 5**: All committed polynomial opening claims accumulated

**Parallelization**: Within each stage, sumchecks are batched and verified together using random linear combinations (single set of challenges per stage).

**Why 5 stages?** The dependency graph creates a partial ordering - a sumcheck cannot execute until its input claims are verified. The 5 stages represent the minimal number of "layers" in this dependency DAG.

***

### Stage 1: Spartan Outer Sumcheck

**Location**: [`jolt-core/src/zkvm/dag/jolt_dag.rs:418-428`](../jolt-core/src/zkvm/dag/jolt_dag.rs#L418-L428)

```rust
let mut spartan_dag = SpartanDag::<F>::new::<ProofTranscript>(padded_trace_length);
let mut lookups_dag = LookupsDag::default();
let mut registers_dag = RegistersDag::default();
let mut ram_dag = RamDag::new_verifier(&state_manager);
let mut bytecode_dag = BytecodeDag::default();
spartan_dag
    .stage1_verify(&mut state_manager)
    .context("Stage 1")?;
```

**Important**: In Stage 1, **only** `spartan_dag.stage1_verify()` is called. The other DAGs are **initialized but not used** until later stages:
- `lookups_dag`, `registers_dag`, `ram_dag`, `bytecode_dag` are created here
- They will be used in Stages 2-4 via their `.stage{N}_verifier_instances()` methods
- This initialization pattern keeps all DAG state in scope for the entire verification process

**What is Spartan?**

Spartan is a SNARK for R1CS (Rank-1 Constraint Systems). R1CS constraints have the form:
$$Az \circ Bz = Cz$$

where:

- $A, B, C$ are constraint matrices (known to verifier via preprocessing)
- $z$ is the witness vector (known only to prover)
- $\circ$ is element-wise multiplication (Hadamard product)

**Jolt's R1CS constraints** (~30 per cycle):
- PC update constraints (ensure program counter advances correctly)
- Linking constraints (e.g., ensure RAM value read matches register value written)
- Arithmetic constraints for instructions proven via R1CS instead of lookups

#### Exact Mathematical Equation Being Verified

**Location**: [`jolt-core/src/zkvm/spartan/mod.rs:204-243`](../jolt-core/src/zkvm/spartan/mod.rs#L204-L243)

The **prover's claim** is that the R1CS constraints are satisfied at a random point $\tau$:

$$\boxed{\sum_{x \in \{0,1\}^{\log T}} \widetilde{\text{eq}}(\tau, x) \cdot \left( \widetilde{Az}(x) \cdot \widetilde{Bz}(x) - \widetilde{Cz}(x) \right) = 0}$$

**Variables:**
- $T$ = trace length (number of execution cycles)
- $\tau \in \mathbb{F}^{\log T}$ = random challenge from transcript (binds evaluation point)
- $\widetilde{\text{eq}}(\tau, x)$ = multilinear extension of equality function
  - Evaluates to 1 when $\tau = x$, 0 otherwise
  - Formula: $\widetilde{\text{eq}}(\tau, x) = \prod_{i=1}^{\log T} (\tau_i x_i + (1-\tau_i)(1-x_i))$
- $\widetilde{Az}(x)$, $\widetilde{Bz}(x)$, $\widetilde{Cz}(x)$ = multilinear extensions of the R1CS evaluation vectors
  - $Az$ means "matrix $A$ times witness vector $z$"
  - Each is a polynomial over execution cycles $x \in \{0,1\}^{\log T}$

**Why this equation proves correctness:**
- If R1CS is satisfied at every cycle: $Az(x) \cdot Bz(x) = Cz(x)$ for all $x \in \{0,1\}^{\log T}$
- Then the difference $(Az(x) \cdot Bz(x) - Cz(x))$ is zero everywhere
- The weighted sum (weighted by $\widetilde{\text{eq}}$) is therefore zero
- If prover lies about even one cycle, the sum is non-zero with high probability (Schwartz-Zippel)

#### Verification Steps (Code Implementation)

**Location**: [`jolt-core/src/zkvm/spartan/mod.rs:204-243`](../jolt-core/src/zkvm/spartan/mod.rs#L204-L243)

1. **Extract challenge $\tau$** from the accumulator (from previous stage or preprocessing)
   ```rust
   let tau = /* extracted from state_manager */;
   ```

2. **Run sumcheck verification** over $\log T$ rounds
   ```rust
   let (claim_outer_final, outer_sumcheck_r) =
       outer_sumcheck_proof.verify(
           F::zero(),  // Initial claim: sum should be 0
           num_rounds_x,  // log(T) rounds
           3,  // degree = 3 (from Az·Bz-Cz product)
           transcript
       )?;
   ```

   **Per-round check** (inside `verify`):
   - Read univariate polynomial $g_j(X_j)$ from proof
   - Check: $C_{j-1} \stackrel{?}{=} g_j(0) + g_j(1)$ (sumcheck consistency)
   - Sample challenge $r_j$ from transcript
   - Compute: $C_j = g_j(r_j)$ (new claim for next round)

3. **Final claim verification** (after all sumcheck rounds complete)

   After $\log T$ rounds, we have challenges $\vec{r} = (r_1, \ldots, r_{\log T})$. The verifier must check:

   $$\boxed{C_{\text{final}} \stackrel{?}{=} \widetilde{\text{eq}}(\tau, \vec{r}) \cdot \left( \widetilde{Az}(\vec{r}) \cdot \widetilde{Bz}(\vec{r}) - \widetilde{Cz}(\vec{r}) \right)}$$

   **Code** ([`mod.rs:239-243`](../jolt-core/src/zkvm/spartan/mod.rs#L239-L243)):
   ```rust
   let tau_bound_rx = EqPolynomial::<F>::mle(&tau, &outer_sumcheck_r_reversed);
   let claim_outer_final_expected = tau_bound_rx * (claim_Az * claim_Bz - claim_Cz);
   if claim_outer_final != claim_outer_final_expected {
       return Err(anyhow::anyhow!("Invalid outer sumcheck claim"));
   }
   ```

   Where:

   - `tau_bound_rx` = $\widetilde{\text{eq}}(\tau, \vec{r})$
   - `claim_Az` = claimed value of $\widetilde{Az}(\vec{r})$ (to be proven later)
   - `claim_Bz` = claimed value of $\widetilde{Bz}(\vec{r})$ (to be proven later)
   - `claim_Cz` = claimed value of $\widetilde{Cz}(\vec{r})$ (to be proven later)

   **How does the verifier compute** $\widetilde{\text{eq}}(\tau, \vec{r})$**?**

   The verifier can compute this herself using the closed-form formula (no preprocessing needed):

   $$\widetilde{\text{eq}}(\tau, \vec{r}) = \prod_{i=1}^{\log T} \left(\tau_i \cdot r_i + (1-\tau_i) \cdot (1-r_i)\right)$$

   **Code**: [`jolt-core/src/poly/eq_poly.rs:24-32`](../jolt-core/src/poly/eq_poly.rs#L24-L32)
   ```rust
   pub fn mle(tau: &[F], r: &[F]) -> F {
       let mut result = F::one();
       for i in 0..tau.len() {
           result *= tau[i] * r[i] + (F::one() - tau[i]) * (F::one() - r[i]);
       }
       result
   }
   ```

   **Where do $\tau$ and $\vec{r}$ come from?**
   - $\tau$: Random challenge sampled from transcript **before Stage 1** (via Fiat-Shamir)
   - $\vec{r} = (r_1, \ldots, r_{\log T})$: Random challenges generated **during the sumcheck rounds** above

   **Computational cost**: Only $O(\log T)$ multiplications - very cheap!

   **Important**: The eq polynomial is **NOT part of preprocessing**. It's computed on-the-fly during verification using only values the verifier already has (\tau from transcript, r̄ from her own random challenges).

4. **Accumulate opening claims** for later verification

   The verifier now has three unverified claims that will be proven in Stage 2-3:

   - $\widetilde{Az}(\vec{r}) = \text{claim\_Az}$
   - $\widetilde{Bz}(\vec{r}) = \text{claim\_Bz}$
   - $\widetilde{Cz}(\vec{r}) = \text{claim\_Cz}$

   These are **virtual polynomials** (not committed), so they'll be verified via subsequent sumchecks, not PCS openings.

**Key mathematical property**: The sumcheck reduces verifying $2^{\log T}$ R1CS constraints to checking three polynomial evaluation claims, which will be verified in Stage 2.

**Soundness**: If the prover cheats (violates R1CS at any cycle), the probability of passing all checks is at most $\frac{3 \cdot \log T}{|\mathbb{F}|}$ (union bound over Schwartz-Zippel failures).

***

### Stage 2: Batched Sumcheck Verification

**Location**: [`jolt-core/src/zkvm/dag/jolt_dag.rs:430-461`](../jolt-core/src/zkvm/dag/jolt_dag.rs#L430-L461)

#### Code Walkthrough: Gathering and Verifying Sumcheck Instances

**Key Concept: Two Different Objects in Sumcheck Verification**

There are **two distinct objects** involved in sumcheck verification:

1. **`SumcheckInstance`** (the "verifier logic")
   - **Type**: `Box<dyn SumcheckInstance<F, ProofTranscript>>`
   - **Location**: [`jolt-core/src/subprotocols/sumcheck.rs:38-68`](../jolt-core/src/subprotocols/sumcheck.rs#L38-L68) (trait definition)
   - **What it contains**:
     - The **initial claim** $C_0$ (what sum we're verifying)
     - The **polynomial evaluator** (can compute final evaluation given challenges)
     - The **expected final check logic** (verifies $C_{\text{final}} \stackrel{?}{=} \text{expected}$)
     - State for binding challenges round-by-round
   - **Created by**: Each component (Spartan, RAM, Registers, Lookups) via `.stage{N}_verifier_instances()`
   - **Purpose**: Tells the verifier **what to verify** and **how to check the final claim**

2. **`SumcheckInstanceProof`** (the "prover's messages")
   - **Type**: `SumcheckInstanceProof<F, ProofTranscript>`
   - **Location**: [`jolt-core/src/subprotocols/sumcheck.rs:646-659`](../jolt-core/src/subprotocols/sumcheck.rs#L646-L659) (struct definition)
   - **What it contains**:
     - `compressed_polys: Vec<CompressedUniPoly<F>>` - the prover's univariate polynomials $g_1(X), g_2(X), \ldots, g_k(X)$
     - One polynomial per sumcheck round
     - Each polynomial encoded as evaluations at $0, 2, 3, \ldots, d$ (point 1 implicit)
   - **Created by**: Prover during proof generation
   - **Extracted from**: `state_manager.proofs` (sent from prover to verifier)
   - **Purpose**: Contains the **prover's messages** for each sumcheck round

**Analogy**:
- **`SumcheckInstance`** = The verifier's calculator (knows what to compute and how to verify)
- **`SumcheckInstanceProof`** = The prover's homework submission (the actual claimed polynomials)

**How they work together**:

For each round $j$:

1. Extract $g_j(X)$ from `SumcheckInstanceProof` (prover's message)
2. Use `SumcheckInstance` to get $C_{j-1}$ (previous claim)
3. Check: $C_{j-1} \stackrel{?}{=} g_j(0) + g_j(1)$
4. Sample $r_j$ from transcript
5. Call `SumcheckInstance.bind(r_j)` to update state
6. Compute $C_j = g_j(r_j)$

After all rounds:

7. Call `SumcheckInstance.final_claims()` to verify $C_{\text{final}} \stackrel{?}{=} \text{expected}$

**In batched sumcheck**:
- **Multiple `SumcheckInstance`s**: One per individual sumcheck (7-10 in Stage 2)
- **Single `SumcheckInstanceProof`**: Contains combined polynomials $G_j(X) = \sum_i \rho_i \cdot g_j^{(i)}(X)$

***

#### Why Do We Need Individual Instances If We Only Check One Batched Polynomial?

**Great question!** This is a subtle but crucial aspect of batched sumcheck.

**Short answer**: The verifier checks the **batched polynomial during rounds**, but needs the **individual instances for the final check**.

**The Two-Phase Structure of Batched Sumcheck**:

**Phase 1: Verify the batched polynomial (Lines 370-371)**

During the sumcheck rounds, the verifier only checks the single batched polynomial:

```rust
let (output_claim, r_sumcheck) =
    proof.verify(claim, max_num_rounds, max_degree, transcript)?;
```

This verifies: $\sum_i \rho_i \cdot C_0^{(i)} \stackrel{?}{=} G_1(0) + G_1(1)$, then $G_1(r_1)$, then $G_2(0) + G_2(1)$, etc.

**Where**: [`jolt-core/src/subprotocols/sumcheck.rs:370-371`](../jolt-core/src/subprotocols/sumcheck.rs#L370-L371)

**Phase 2: Compute the expected final claim from individual instances (Lines 373-397)**

After all rounds complete, the verifier must check that the final batched claim matches:

```rust
let expected_output_claim = sumcheck_instances
    .iter()
    .zip(batching_coeffs.iter())
    .map(|(sumcheck, coeff)| {
        let r_slice = &r_sumcheck[max_num_rounds - sumcheck.num_rounds()..];
        let claim = sumcheck.expected_output_claim(opening_accumulator.clone(), r_slice);
        claim * coeff
    })
    .sum();

if output_claim != expected_output_claim {
    return Err(ProofVerifyError::SumcheckVerificationError);
}
```

**Where**: [`jolt-core/src/subprotocols/sumcheck.rs:373-401`](../jolt-core/src/subprotocols/sumcheck.rs#L373-L401)

**Why this is necessary**:

Each individual `SumcheckInstance` knows **how to compute its expected final evaluation**:

$$\text{expected}^{(i)} = \text{sumcheck}_i.\texttt{expected\_output\_claim}(\vec{r})$$

For example:

- **Spartan Inner**: $\text{expected} = \left(\widetilde{A}(\vec{r}_x, \vec{r}_y) + \gamma \cdot \widetilde{B}(\vec{r}_x, \vec{r}_y) + \gamma^2 \cdot \widetilde{C}(\vec{r}_x, \vec{r}_y)\right) \cdot \widetilde{z}(\vec{r}_y)$
- **RAM Twist**: $\text{expected} = \sum_{\text{chunks}} \text{fingerprint}(\text{addr}_a[i], \text{ts}_a[i], \text{val}_a[i])$
- **Instruction Lookups**: $\text{expected} = \text{prefixsum} \cdot \text{suffixsum}$

Each instance has **different logic** for computing the expected value! The batched polynomial proof only verifies the **random linear combination**, but the final check must ensure **each individual sumcheck would pass**.

**The Mathematical Reason**:

Batched sumcheck proves:
$$\sum_i \rho_i \cdot \left(\sum_{x \in \{0,1\}^{n_i}} P^{(i)}(x)\right) = \sum_i \rho_i \cdot C_0^{(i)}$$

After $k$ rounds with challenges $\vec{r}$, this reduces to:
$$\sum_i \rho_i \cdot P^{(i)}(\vec{r}_i) \stackrel{?}{=} \sum_i \rho_i \cdot \text{expected}^{(i)}(\vec{r}_i)$$

The prover sent one **batched univariate polynomial** per round:
$$G_j(X) = \sum_i \rho_i \cdot g_j^{(i)}(X)$$

But the verifier cannot unbatch the final claim! She needs each `SumcheckInstance` to compute $\text{expected}^{(i)}(\vec{r}_i)$ individually, then check:
$$\text{output\_claim} \stackrel{?}{=} \sum_i \rho_i \cdot \text{expected}^{(i)}(\vec{r}_i)$$

**Key insight**: The batching only affects **proof size** (one polynomial per round instead of $n$). The verifier still needs to:
1. Track all individual initial claims $C_0^{(i)}$
2. Compute all individual final expectations $\text{expected}^{(i)}(\vec{r}_i)$
3. Check the random linear combination matches

**Why not just track the batched claim?**

Because the **final check logic is different for each sumcheck**! For example:

- Spartan needs to evaluate $\widetilde{A}, \widetilde{B}, \widetilde{C}, \widetilde{z}$ at $\vec{r}$
- RAM Twist needs to compute fingerprints for address-ordered memory
- Lookups need to compute prefix/suffix products

The `SumcheckInstance` trait encapsulates this **per-component verification logic**:

```rust
trait SumcheckInstance {
    fn input_claim(&self) -> F;           // C_0^(i)
    fn expected_output_claim(&self, r: &[F]) -> F;  // expected^(i)(r)
    fn cache_openings_verifier(...);      // Accumulate polynomial opening claims
}
```

**The Security Reason: Preventing Cancellation Attacks**

You've identified the **critical security concern**! Here's why we must check individual final claims:

**Attack scenario without individual checks**:

Suppose the verifier only checks the batched polynomial during rounds, and accepts if:
$$G_j(0) + G_j(1) = \text{batched\_claim}_{j-1}$$

A malicious prover could:

1. **Cheat on sumcheck 1**: Send incorrect $g_1^{(1)}(X)$ that fails its sumcheck
2. **Cheat on sumcheck 2**: Send incorrect $g_1^{(2)}(X)$ that also fails its sumcheck
3. **But craft them so the errors cancel**: $\rho_1 \cdot g_1^{(1)}(X) + \rho_2 \cdot g_1^{(2)}(X) = G_1(X)$ is valid!

**Example with numbers**:
- Sumcheck 1 should have $g_1^{(1)}(0) = 10$, but prover sends $g_1^{(1)}(0) = 15$ (cheating by +5)
- Sumcheck 2 should have $g_2^{(1)}(0) = 20$, but prover sends $g_2^{(1)}(0) = 15$ (cheating by -5)
- Batching coefficients: $\rho_1 = 1, \rho_2 = 1$
- Batched polynomial: $G_1(0) = 1 \cdot 15 + 1 \cdot 15 = 30$  (correct!)
- But individual sumchecks are both wrong! 

**The fix**: Check individual final claims

After all rounds, verify:
$$\text{output\_claim} \stackrel{?}{=} \sum_i \rho_i \cdot \text{expected}^{(i)}(\vec{r}_i)$$

This ensures:

- **Each individual sumcheck** must evaluate to the correct final value
- Errors cannot cancel out in the random linear combination
- Security holds by Schwartz-Zippel: probability of cancellation is $\leq \frac{d}{|\mathbb{F}|}$

**Code location of this critical check**: [`jolt-core/src/subprotocols/sumcheck.rs:399-401`](../jolt-core/src/subprotocols/sumcheck.rs#L399-L401)

```rust
if output_claim != expected_output_claim {
    return Err(ProofVerifyError::SumcheckVerificationError);
}
```

**Why this is secure**:
- The batching coefficients $\rho_i$ are sampled **after** the prover commits to the claims
- Via Fiat-Shamir: $\rho_i = \text{Hash}(\text{transcript})$
- Prover cannot predict $\rho_i$ when constructing the proof
- Any error in $\text{expected}^{(i)}(\vec{r}_i)$ propagates to $\sum_i \rho_i \cdot \text{expected}^{(i)}(\vec{r}_i)$ with high probability

**Summary**:
- **Batched proof**: One polynomial per round (efficiency in proof size and communication)
- **Individual instances**: Needed for computing the final expected claim (each has unique verification logic)
- **Individual final checks**: **CRITICAL for security** - prevents cancellation attacks
- **Verification cost**: Still $O(n)$ in number of sumchecks, but **communication** and **transcript size** are $O(1)$ per round

**Code References for Batched Sumcheck**:
- **Batching coefficient generation**: [`jolt-core/src/subprotocols/sumcheck.rs:348`](../jolt-core/src/subprotocols/sumcheck.rs#L348)
  ```rust
  let batching_coeffs: Vec<F> = transcript.challenge_vector(sumcheck_instances.len());
  ```

- **Initial claim batching**: [`jolt-core/src/subprotocols/sumcheck.rs:359-368`](../jolt-core/src/subprotocols/sumcheck.rs#L359-L368)
  ```rust
  let claim: F = sumcheck_instances
      .iter()
      .zip(batching_coeffs.iter())
      .map(|(sumcheck, coeff)| {
          let input_claim = sumcheck.input_claim();
          transcript.append_scalar(&input_claim);
          input_claim.mul_pow_2(max_num_rounds - num_rounds) * coeff
      })
      .sum();
  ```

- **Batched polynomial verification (rounds)**: [`jolt-core/src/subprotocols/sumcheck.rs:370-371`](../jolt-core/src/subprotocols/sumcheck.rs#L370-L371)
  ```rust
  let (output_claim, r_sumcheck) =
      proof.verify(claim, max_num_rounds, max_degree, transcript)?;
  ```

- **Individual expected claim computation**: [`jolt-core/src/subprotocols/sumcheck.rs:373-397`](../jolt-core/src/subprotocols/sumcheck.rs#L373-L397)
  ```rust
  let expected_output_claim = sumcheck_instances
      .iter()
      .zip(batching_coeffs.iter())
      .map(|(sumcheck, coeff)| {
          let r_slice = &r_sumcheck[max_num_rounds - sumcheck.num_rounds()..];
          let claim = sumcheck.expected_output_claim(opening_accumulator.clone(), r_slice);
          claim * coeff
      })
      .sum();
  ```

- **Security-critical final check**: [`jolt-core/src/subprotocols/sumcheck.rs:399-401`](../jolt-core/src/subprotocols/sumcheck.rs#L399-L401)

**Prover-side batching** (for comparison):
- **Prover univariate polynomial generation**: [`jolt-core/src/subprotocols/sumcheck.rs:229-252`](../jolt-core/src/subprotocols/sumcheck.rs#L229-L252)
  - Each instance computes its own $g_j^{(i)}(X)$
- **Linear combination to create batched polynomial**: [`jolt-core/src/subprotocols/sumcheck.rs:254-262`](../jolt-core/src/subprotocols/sumcheck.rs#L254-L262)
  ```rust
  let batched_univariate_poly: UniPoly<F> =
      univariate_polys.iter().zip(batching_coeffs.iter()).fold(
          UniPoly::from_coeff(vec![]),

          |mut batched_poly, (poly, coeff)| {
              batched_poly += &(poly * coeff);
              batched_poly
          },
      );
  ```

- **Individual claim updates**: [`jolt-core/src/subprotocols/sumcheck.rs:272-275`](../jolt-core/src/subprotocols/sumcheck.rs#L272-L275)
  - Each instance tracks its own claim separately even though only batched poly is sent

**Reference**: Jim Posen's ["Perspectives on Sumcheck Batching"](https://hackmd.io/s/HyxaupAAA) (cited in code at line 173-175)

***

#### Zero-Knowledge: Why Don't the Claims Leak Witness Information?

**Excellent question!** The claims in the sumcheck proof correspond to polynomial evaluations, and these are **sent in plain** (not as commitments). So why doesn't this leak information about the witness?

**First: When and How Are These Plain Evaluations Sent?**

The prover sends plain evaluations in **two distinct phases**:

**Phase A: During sumcheck rounds** (Stages 1-4)
- **NOT sent**: Individual polynomial evaluations
- **Sent**: Only univariate round polynomials $g_j(X)$ (compressed as evaluations at $0, 2, 3, \ldots, d$)
- These are NOT evaluations of the witness polynomials themselves
- These are partial sums that aggregate over remaining variables

**Phase B: After each sumcheck completes** (`cache_openings_prover`)
- **When**: After all rounds of a sumcheck complete
- **What**: Plain evaluations of witness polynomials at the random challenges $\vec{r}$
- **How**: Via `accumulator.append_dense()`, `append_sparse()`, or `append_virtual()`
- **Sent to transcript**: `transcript.append_scalar(&claim)` or `transcript.append_scalars(claims)`

**Code walkthrough of Phase B**:

1. **Prover side** - After sumcheck completes, `cache_openings_prover()` is called:

   **Example: Spartan Inner sumcheck** ([`jolt-core/src/zkvm/spartan/inner.rs:270-287`](../jolt-core/src/zkvm/spartan/inner.rs#L270-L287))
   ```rust
   fn cache_openings_prover(
       &self,
       accumulator: Rc<RefCell<ProverOpeningAccumulator<F>>>,
       transcript: &mut T,
       opening_point: OpeningPoint<BIG_ENDIAN, F>,
   ) {
       let prover_state = self.prover_state.as_ref().expect("Prover state not initialized");

       // Evaluate witness segments at random point r_y
       let ops_addr_eval = prover_state.ops_addr_poly.final_sumcheck_claim();
       let mem_addr_eval = prover_state.mem_addr_poly.final_sumcheck_claim();
       // ... more evaluations ...

       // Send plain evaluations to transcript!
       accumulator.borrow_mut().append_dense(
           transcript,
           vec![CommittedPolynomial::OpsAddr, CommittedPolynomial::MemAddr, ...],
           SumcheckId::SpartanInner,
           opening_point.r.clone(),
           &[ops_addr_eval, mem_addr_eval, ...],
       );
   }
   ```

2. **Inside `append_dense()`** ([`jolt-core/src/poly/opening_proof.rs:704-712`](../jolt-core/src/poly/opening_proof.rs#L704-L712)):
   ```rust
   pub fn append_dense<T: Transcript>(
       &mut self,
       transcript: &mut T,
       polynomials: Vec<CommittedPolynomial>,
       sumcheck: SumcheckId,
       opening_point: Vec<F::Challenge>,
       claims: &[F],  // \leftarrow  The plain evaluations!
   ) {
       transcript.append_scalars(claims);  // \leftarrow  SENT TO TRANSCRIPT (prover \rightarrow  verifier)
       // ... store in accumulator for later opening proof ...
   }
   ```

3. **Verifier side** - Verifier receives these from transcript:

   **During `cache_openings_verifier()`** ([`jolt-core/src/poly/opening_proof.rs:1108-1157`](../jolt-core/src/poly/opening_proof.rs#L1108-L1157)):
   ```rust
   pub fn append_dense<T: Transcript>(
       &mut self,
       transcript: &mut T,
       polynomials: Vec<CommittedPolynomial>,
       sumcheck: SumcheckId,
       opening_point: Vec<F::Challenge>,
   ) {
       // Read the claims from transcript (prover \rightarrow  verifier communication)
       let claims: Vec<F> = transcript.challenge_vector(polynomials.len());

       // Store these claimed evaluations
       for (label, claim) in polynomials.iter().zip(claims.iter()) {
           self.openings.insert(
               OpeningId::Committed(*label, sumcheck),
               (OpeningPoint::new(opening_point.clone()), *claim),
           );
       }
   }
   ```

4. **Later verification** - These claims are proven correct in Stage 5:

   In the batched opening proof (Stage 5), the verifier checks that these plain evaluations are actually correct by verifying opening proofs against the polynomial commitments.

**Summary of what gets sent**:

| Phase | What's Sent | Format | Example |
|-------|-------------|--------|---------|
| Sumcheck rounds | Univariate polys $g_j(X)$ | Compressed evals at $0, 2, 3, \ldots, d$ | Part of `SumcheckInstanceProof` |
| After sumcheck | Witness evals $\widetilde{P}(\vec{r})$ | Plain field elements | Via `transcript.append_scalar(claim)` |
| Stage 5 | Opening proof | Dory PCS proof | Proves the plain evals are correct |

**Key point**: The plain evaluations are **NOT part of `SumcheckInstanceProof`**. They are sent **separately after the sumcheck completes** via the `cache_openings_prover/verifier` mechanism, and are accumulated in the `OpeningAccumulator` for later verification in Stage 5.

***

**Answer: Evaluation at Random Points Reveals Nothing**

The key insight is that **polynomial evaluations at random points are information-theoretically hiding**.

**1. The Witness is Hidden in Polynomial Structure**

The witness values (secret execution trace) are embedded as **coefficients** of multilinear polynomials:

- Example: $\widetilde{Z}(y_1, y_2, \ldots, y_k)$ is the MLE of witness vector $Z$
- If $Z = [z_0, z_1, z_2, z_3]$, then:
  $$\widetilde{Z}(y_1, y_2) = z_0 \cdot (1-y_1)(1-y_2) + z_1 \cdot (1-y_1)y_2 + z_2 \cdot y_1(1-y_2) + z_3 \cdot y_1 y_2$$

**2. Random Point Evaluation = Random Linear Combination**

When the verifier samples random challenges $\vec{r} = (r_1, r_2, \ldots, r_k)$ from a large field $\mathbb{F}$, the evaluation $\widetilde{Z}(\vec{r})$ is a **random linear combination** of all witness values:

$$\widetilde{Z}(\vec{r}) = \sum_{i=0}^{2^k-1} z_i \cdot \text{basis}_i(\vec{r})$$

where $\text{basis}_i(\vec{r})$ are the Lagrange basis polynomials evaluated at $\vec{r}$.

**Why this is safe**:
- The coefficients $\text{basis}_i(\vec{r})$ are effectively **random** (verifier chooses $\vec{r}$ uniformly)
- The result is a **single field element** that looks completely random
- Knowing $\widetilde{Z}(\vec{r})$ gives **zero information** about individual $z_i$ values
- It's like asking "what's the weighted average of your bank balance and your age with random weights?" - the answer reveals nothing about either value

**3. Information-Theoretic Security via Schwartz-Zippel**

From information theory:

- A degree-$d$ polynomial over $\mathbb{F}$ can produce at most $d$ distinct outputs on domain $\{0,1\}^k$
- But it can produce $|\mathbb{F}|$ distinct outputs when evaluated at random points
- If $|\mathbb{F}| \gg 2^k$, then evaluation at random $\vec{r}$ reveals **nothing** about evaluations on Boolean hypercube
- Example: Even if witness has only 1024 values ($k=10$), evaluation over $\mathbb{F}_{2^{256}}$ gives $2^{256}$ possible outputs

**4. The Role of Polynomial Commitments (Final Stage)**

The sumcheck protocol itself is **NOT zero-knowledge**. Here's the full picture:

**During sumcheck rounds (Stages 1-4)**:
- Claims are **plain evaluations** like $\widetilde{Z}(\vec{r}_y) = 42$
- These are safe because $\vec{r}_y$ is random
- But technically, sumcheck is an **interactive proof**, not a ZK proof

**At final evaluation (Stage 5)**:
- Verifier needs proof that claimed evaluations are correct
- This is where **Polynomial Commitment Scheme (PCS)** comes in
- Dory PCS provides **binding** and optionally **hiding** commitments
- **Hiding commitment**: $\text{Commit}(\widetilde{Z}) = \widetilde{Z} \cdot G + r \cdot H$ (Pedersen-style)
  - Includes random blinding factor $r$ chosen by prover
  - Even $\text{Commit}(0)$ looks random every time
- Opening proof: Proves $\widetilde{Z}(\vec{r}_y) = 42$ **without revealing** $\widetilde{Z}$ itself

**5. Jolt-Specific: What Gets Committed vs What Doesn't**

In Jolt's implementation:

**Committed polynomials** (binding + hiding):
- Witness-related: $\widetilde{Z}$ (registers, RAM, intermediate values)
- Committed during preprocessing via Dory PCS
- Evaluations proven in Stage 5 with opening proofs

**Virtual polynomials** (no commitment):
- Derived from committed polynomials via sumcheck reductions
- Example: $\widetilde{Az}(\vec{r})$ proven via sumcheck, not direct commitment
- Still safe because evaluations at random points

**Public polynomials** (no hiding needed):
- R1CS matrices $\widetilde{A}, \widetilde{B}, \widetilde{C}$ (part of circuit, not witness)
- Lookup tables (predefined instruction tables)
- Anyone can compute these at any point

**6. Summary: Why Plain Evaluations Are Safe**

| What is Revealed | Why It's Safe |
|------------------|---------------|
| $\widetilde{Z}(\vec{r}_y) = 42$ | Random point \rightarrow  random linear combination of all witness values |
| Univariate polys $g_j(X)$ | Define behavior over random subspace, not Boolean hypercube |
| Final claims $C_j$ | Each is evaluation at random point chosen by verifier |
| Batching coefficients $\rho_i$ | Public randomness from transcript (Fiat-Shamir) |

**The core principle**:
- **Boolean hypercube evaluations** $\{g(0,0,0), g(0,0,1), \ldots\}$ \rightarrow  reveal witness structure 
- **Random point evaluations** $g(r_1, r_2, r_3)$ where $r_i \in \mathbb{F}$ \rightarrow  reveal nothing 

**References**:
- **Schwartz-Zippel Lemma**: Two different degree-$d$ polynomials agree on $\leq d$ points
- **Sumcheck security**: See [Theory/The Sum-check Protocol.md:98-114](../Theory/The%20Sum-check%20Protocol.md#L98-L114)
- **Spartan hiding commitments**: See [Theory/Spartan.md:481-499](../Theory/Spartan.md#L481-L499)

***

**Step 1: Collect all Stage 2 sumcheck instances from each component**

```rust
let stage2_instances: Vec<_> = std::iter::empty()
    .chain(spartan_dag.stage2_verifier_instances(&mut state_manager))
    .chain(registers_dag.stage2_verifier_instances(&mut state_manager))
    .chain(ram_dag.stage2_verifier_instances(&mut state_manager))
    .chain(lookups_dag.stage2_verifier_instances(&mut state_manager))
    .collect();
```

**What this does**:
- **Line 1**: Start with empty iterator
- **Line 2**: Add Spartan's Stage 2 sumchecks (inner sumcheck for matrix-vector products)
  - `stage2_verifier_instances()` returns `Vec<Box<dyn SumcheckInstance<F, ProofTranscript>>>`
  - Each box contains one sumcheck instance with its claim and evaluation logic
- **Line 3**: Add Registers' Stage 2 sumchecks (Twist read-checking for rs1/rs2/rd)
  - Returns ~3 sumcheck instances (one per register read/write polynomial)
- **Line 4**: Add RAM's Stage 2 sumchecks (Twist read/write-checking for memory)
  - Returns sumcheck instances for RAM consistency
- **Line 5**: Add Instruction Lookups' Stage 2 sumchecks (Shout read-checking)
  - First phase of proving instruction execution via lookups
- **Line 6**: Collect all into a single `Vec<Box<dyn SumcheckInstance<...>>>`

**Result**: `stage2_instances` contains ~7-10 sumcheck instances to be verified together

**Step 2: Convert to trait object references for batching**

```rust
let stage2_instances_ref: Vec<&dyn SumcheckInstance<F, ProofTranscript>> =
    stage2_instances.iter()
        .map(|instance| &**instance as &dyn SumcheckInstance<F, ProofTranscript>)
        .collect();
```

**What this does**:
- **Line 1**: Iterate over the boxed instances
- **Line 2**: For each `Box<dyn SumcheckInstance>`:
  - `**instance` dereferences twice: once for Box, once for the reference
  - `as &dyn SumcheckInstance` creates a **trait object reference**
- **Line 3**: Collect into `Vec<&dyn SumcheckInstance<...>>`

**What is a "trait object"?** (Rust concept)

A **trait object** (`&dyn Trait` or `Box<dyn Trait>`) is Rust's way of achieving **runtime polymorphism** (like interfaces in other languages):

- **Trait**: `SumcheckInstance` - defines interface (methods like `input_claim()`, `expected_output_claim()`)
- **Trait object**: `&dyn SumcheckInstance` - a fat pointer containing:
  1. **Pointer to data**: The actual instance (could be `SpartanInner`, `RamTwist`, `Lookups`, etc.)
  2. **Pointer to vtable**: Virtual method table for dynamic dispatch
- **Dynamic dispatch**: Method calls resolved at runtime, not compile time
- **Why needed here**: Different sumcheck types (Spartan, RAM, Lookups) all implement `SumcheckInstance`, but have different concrete types

**Example**:
```rust
// Without trait objects (compile-time polymorphism)
let spartan_instance: SpartanInner = ...;
let ram_instance: RamTwist = ...;
// Can't put these in the same Vec! They're different types.

// With trait objects (runtime polymorphism)
let instances: Vec<&dyn SumcheckInstance> = vec![
    &spartan_instance as &dyn SumcheckInstance,  // \leftarrow  Trait object
    &ram_instance as &dyn SumcheckInstance,      // \leftarrow  Trait object
];
// Works! All have same type: &dyn SumcheckInstance
```

**Why needed**: `BatchedSumcheck::verify()` expects `Vec<&dyn SumcheckInstance>`, not `Vec<Box<dyn SumcheckInstance>>`. This converts ownership (Box) to borrowing (&dyn).

**Step 3: Extract the proof from state manager**

```rust
let proofs = state_manager.proofs.borrow();
let stage2_proof = /* extract from proofs */;
```

**What this does**:
- **Line 1**: Borrow the proofs from state manager (RefCell interior mutability)
  - `state_manager.proofs` is a `RefCell<Vec<JoltProof>>`
  - `.borrow()` returns `Ref<Vec<JoltProof>>`
- **Line 2**: Extract the Stage 2 batched sumcheck proof from the proofs vector
  - Actual code: `proofs[proof_index].sumcheck_proofs[stage2_index]`
  - This proof contains all the prover's univariate polynomials for all batched sumchecks

**Result**: `stage2_proof` is a `BatchedProof` containing the sumcheck round polynomials

**Step 4: Run batched sumcheck verification**

```rust
let _r_stage2 = BatchedSumcheck::verify(
    stage2_proof,
    stage2_instances_ref,
    Some(opening_accumulator.clone()),
    &mut *transcript.borrow_mut(),
).context("Stage 2")?;
```

**What this does**:
- **Argument 1** (`stage2_proof`): The batched proof from the prover
  - Contains univariate polynomials `G_j(X)` for each round `j`
  - Each `G_j(X)` is a random linear combination of all individual sumcheck round polynomials
- **Argument 2** (`stage2_instances_ref`): All the sumcheck instances being verified
  - Each instance knows its own claim, polynomial evaluators, and final check logic
- **Argument 3** (`Some(opening_accumulator.clone())`): Accumulates polynomial opening claims
  - **What it is**: `VerifierOpeningAccumulator<F>` - a data structure that collects all polynomial evaluation claims
  - **Type definition**: [`jolt-core/src/poly/opening_proof.rs:586-596`](../jolt-core/src/poly/opening_proof.rs#L586-L596)
    ```rust
    pub struct VerifierOpeningAccumulator<F: JoltField> {
        sumchecks: Vec<OpeningProofReductionSumcheck<F>>,
        pub openings: Openings<F>,  // \leftarrow  The key data structure!
    }

    // Openings is a map from polynomial ID to (point, claimed_evaluation)
    pub type Openings<F> = BTreeMap<OpeningId, (OpeningPoint<BIG_ENDIAN, F>, F)>;

    pub enum OpeningId {
        Committed(CommittedPolynomial, SumcheckId),  // e.g., RAM values, registers
        Virtual(VirtualPolynomial, SumcheckId),      // e.g., Az, Bz, Cz (not committed)
        UntrustedAdvice,
        TrustedAdvice,
    }
    ```

  - **How it's populated**: As sumchecks complete, each `SumcheckInstance` calls `cache_openings_verifier()` which adds entries to `accumulator.openings`
  - **What gets added**: Polynomial evaluation claims like "Polynomial P (committed to earlier) evaluates to value v at point r⃗"
  - **These will be proven in Stage 5** (batched opening proof using Dory PCS)
- **Argument 4** (`&mut *transcript.borrow_mut()`): Fiat-Shamir transcript
  - `.borrow_mut()` gets mutable reference from RefCell
  - `*` dereferences the `RefMut`
  - `&mut` takes mutable reference for the function
  - Used to generate random challenges for each sumcheck round

**What `BatchedSumcheck::verify()` does internally**:
1. Sample batching coefficients $\rho_1, \ldots, \rho_n$ from transcript
2. For each round $j = 1, \ldots, k$:
   - Read combined polynomial $G_j(X)$ from proof
   - Check: $\sum_i \rho_i \cdot C_{j-1}^{(i)} \stackrel{?}{=} G_j(0) + G_j(1)$
   - Sample challenge $r_j$ from transcript
   - Update each instance's claim: $C_j^{(i)} = g_j^{(i)}(r_j)$
3. After all rounds, call each instance's `final_claims()` method to verify final evaluations
4. **Accumulate opening claims** - for each instance, call `cache_openings_verifier()`:
   - This reads plain evaluation claims from transcript (sent by prover)
   - Adds them to `opening_accumulator.openings` map
   - These claims will be proven in Stage 5

**Concrete Example: What Happens to the Accumulator**

**BEFORE `BatchedSumcheck::verify()` (Stage 2)**:
```rust
opening_accumulator.openings = BTreeMap {
    // Empty or contains claims from Stage 1 (Spartan Outer)
    Committed(PC, SpartanOuter) => (point: [r_1, r_2, ...], claim: 42),
    Virtual(SpartanAz, SpartanOuter) => (point: [r_1, r_2, ...], claim: 123),
    // ... a few entries from Stage 1
}
```

**DURING `BatchedSumcheck::verify()` - Step 4 (lines 384-391)**:

For each sumcheck instance (Spartan Inner, RAM Twist, Registers, Lookups):

```rust
// Code: jolt-core/src/subprotocols/sumcheck.rs:384-391
if let Some(opening_accumulator) = &opening_accumulator {
    sumcheck.cache_openings_verifier(  // \leftarrow  Called for EACH instance
        opening_accumulator.clone(),
        transcript,
        sumcheck.normalize_opening_point(r_slice),
    );
}
```

**Inside `cache_openings_verifier()`** - Example: Spartan Inner:

```rust
// Code: jolt-core/src/poly/opening_proof.rs:1108-1157
pub fn append_dense<T: Transcript>(
    &mut self,
    transcript: &mut T,
    polynomials: Vec<CommittedPolynomial>,
    sumcheck: SumcheckId,
    opening_point: Vec<F::Challenge>,
) {
    // Read claims from transcript (prover sent these earlier!)
    let claims: Vec<F> = transcript.challenge_vector(polynomials.len());

    // Add each claim to the accumulator
    for (label, claim) in polynomials.iter().zip(claims.iter()) {
        self.openings.insert(
            OpeningId::Committed(*label, sumcheck),
            (OpeningPoint::new(opening_point.clone()), *claim),
        );
    }
}
```

**AFTER `BatchedSumcheck::verify()` (Stage 2 complete)**:
```rust
opening_accumulator.openings = BTreeMap {
    // Stage 1 entries (still there)
    Committed(PC, SpartanOuter) => (point: [r_1, r_2, ...], claim: 42),
    Virtual(SpartanAz, SpartanOuter) => (point: [r_1, r_2, ...], claim: 123),

    // NEW: Stage 2 entries added by cache_openings_verifier()
    // From Spartan Inner sumcheck:
    Committed(OpsAddr, SpartanInner) => (point: [r_y_1, r_y_2, ...], claim: 789),
    Committed(MemAddr, SpartanInner) => (point: [r_y_1, r_y_2, ...], claim: 456),
    Committed(RAMRd, SpartanInner) => (point: [r_y_1, r_y_2, ...], claim: 234),
    // ... ~15 more witness segment claims

    // From RAM Twist sumcheck:
    Committed(RAMAddressA, RAMReadChecking) => (point: [r_addr, r_ts], claim: 567),
    Committed(RAMTimestampA, RAMReadChecking) => (point: [r_addr, r_ts], claim: 890),
    Virtual(RAMInit, RAMReadChecking) => (point: [r_addr, r_ts], claim: 111),
    // ... more RAM claims

    // From Registers Twist sumcheck:
    Committed(RegistersRs1, RegistersReading) => (point: [r_cycle], claim: 333),
    Committed(RegistersRs2, RegistersReading) => (point: [r_cycle], claim: 444),
    // ... more register claims

    // From Instruction Lookups (Shout):
    Virtual(LookupDimIndexA, LookupReadChecking) => (point: [r_cycle], claim: 555),
    // ... more lookup claims
}
```

**Visual Structure: What's Actually Stored in the Accumulator**

To be crystal clear, the `opening_accumulator.openings` map stores:

```rust
BTreeMap<OpeningId, (OpeningPoint, FieldElement)>
   ↓           ↓              ↓            ↓
 Key: ID    Value: (evaluation point, claimed evaluation)
```

**Each entry looks like**:

```

+-------------------------------------------------------------+
| Key: OpeningId                                              |
|   - Which polynomial? (e.g., OpsAddr, RAMAddressA, ...)    |
|   - Which sumcheck? (e.g., SpartanInner, RAMReadChecking)  |

+-------------------------------------------------------------+
| Value: (OpeningPoint, F)                                    |
|   - Point: [r_1, r_2, r_3, ...] (random challenges)           |
|   - Claim: 789 (plain field element)                       |

+-------------------------------------------------------------+
```

**Concrete example entry**:

```rust
OpeningId::Committed(
    CommittedPolynomial::OpsAddr,  // \leftarrow  Which polynomial
    SumcheckId::SpartanInner       // \leftarrow  Which sumcheck
) => (
    OpeningPoint { r: [r_y_1, r_y_2, r_y_3, ...] },  // \leftarrow  Evaluation point
    789                                             // \leftarrow  CLAIMED evaluation (plain)
)
```

**What's NOT in the accumulator**:
-  **NOT the polynomial commitment** (e.g., Pedersen commitment, KZG commitment)
-  **NOT the polynomial itself** (the coefficients)
-  **NOT the proof that the claim is correct**

**Where are the commitments stored?**

Commitments are stored **separately** in a different data structure:

```rust
// Location: JoltDAG maintains commitments
commitments: RefCell<Vec<PCS::Commitment>>

// Indexed by AllCommittedPolynomials enum
commitments[CommittedPolynomial::OpsAddr.to_index()] = <commitment>
commitments[CommittedPolynomial::RAMAddressA.to_index()] = <commitment>
// ...
```

**How they connect in Stage 5**:

```
Opening Accumulator Entry:

+--------------------------------------------+
| OpeningId::Committed(OpsAddr, SpartanInner)|
|   point: [r_y_1, r_y_2, ...]                |
|   claim: 789                                |

+--------------------------------------------+
         |

         | Stage 5 matches this with...
         ↓
Commitment Storage:

+--------------------------------------------+
| commitments[OpsAddr.to_index()]            |
|   = Commit(OpsAddr_polynomial)             |

+--------------------------------------------+
         |

         | Dory PCS opening proof verifies...
         ↓

+--------------------------------------------+
| "Polynomial committed to at index OpsAddr  |
|  evaluates to 789 at point [r_y_1, r_y_2]"  |

+--------------------------------------------+
```

**Key Observations**:

1. **Claims come from transcript**: The verifier reads these plain field element claims via `transcript.challenge_vector()` - the prover sent them earlier via `transcript.append_scalars()`

2. **Accumulator stores CLAIMS, not COMMITMENTS**:
   - **In accumulator**: Polynomial ID + evaluation point + claimed value
   - **Elsewhere**: Actual commitments (from preprocessing or sent with proof)

3. **Map grows throughout verification**: Each stage adds more entries. By Stage 4, there might be ~50-100 entries total.

4. **Two types of claims**:
   - **Committed**: References to polynomials with commitments (will need PCS opening proofs in Stage 5)
   - **Virtual**: References to derived polynomials without commitments (already verified by sumcheck reduction)

5. **Stage 5 uses this map**: The batched opening proof in Stage 5 matches each `Committed` entry with its actual commitment, then verifies all openings together using Dory PCS.

**Return value**: `_r_stage2` contains the final random challenges $\vec{r} = (r_1, \ldots, r_k)$ from the sumcheck rounds. This is ignored (hence `_` prefix) because each instance has already received these challenges internally.

**Error handling**: `.context("Stage 2")?` adds context if verification fails, propagating the error up.

***

#### Exact Mathematical Equations for Stage 2 Sumchecks

**Note**: For detailed explanation of batched sumcheck protocol, security considerations (cancellation attacks), and complete code references, see [Why Do We Need Individual Instances If We Only Check One Batched Polynomial?](#why-do-we-need-individual-instances-if-we-only-check-one-batched-polynomial) above.

**Stage 2 contains sumchecks from**:
- **Spartan inner sumcheck**: Further reduces R1CS evaluation
- **Registers read/write checking**: First Twist sumcheck for registers
- **RAM read/write checking**: First Twist sumcheck for RAM
- **Instruction lookups**: First Shout sumcheck for instruction execution

#### Exact Mathematical Equations for Stage 2 Sumchecks

##### 2.1: Spartan Inner Sumcheck

**Location**: [`jolt-core/src/zkvm/spartan/inner.rs:221-261`](../jolt-core/src/zkvm/spartan/inner.rs#L221-L261)

**Equation being verified**:

$$\boxed{\sum_{y \in \{0,1\}^{n}} \left(\widetilde{A}_{\text{small}}(\vec{r}_x, y) + \gamma \cdot \widetilde{B}_{\text{small}}(\vec{r}_x, y) + \gamma^2 \cdot \widetilde{C}_{\text{small}}(\vec{r}_x, y)\right) \cdot \widetilde{z}(y) = \text{claim}\_Az + \gamma \cdot \text{claim}\_Bz + \gamma^2 \cdot \text{claim}\_Cz}$$

**Variables**:
- $\vec{r}_x$ = challenges from Stage 1 outer sumcheck (binds cycle index)
- $\gamma$ = random batching coefficient from transcript (combines A, B, C matrices)
- $\widetilde{A}_{\text{small}}$, $\widetilde{B}_{\text{small}}$, $\widetilde{C}_{\text{small}}$ = MLEs of uniform R1CS constraint matrices
  - "Small" because Jolt uses same ~30 constraints every cycle (uniform constraints)
- $\widetilde{z}(y)$ = MLE of witness vector (registers, RAM values, intermediate results)
- $n$ = number of witness variables

**Why this equation**: This sumcheck verifies that $\widetilde{Az}(\vec{r}_x)$, $\widetilde{Bz}(\vec{r}_x)$, $\widetilde{Cz}(\vec{r}_x)$ evaluate to the claimed values from Stage 1. It expands the matrix-vector products into explicit sums over witness variables.

**Final check** (lines 248-260):
```rust
let eval_a = key.evaluate_uniform_a_at_point(rx_var, ry_var);
let eval_b = key.evaluate_uniform_b_at_point(rx_var, ry_var);
let eval_c = key.evaluate_uniform_c_at_point(rx_var, ry_var);
let left_expected = eval_a + gamma * eval_b + gamma.square() * eval_c;
let eval_z = key.evaluate_z_mle_with_segment_evals(&claimed_witness_evals, ry_var, true);
expected_claim = left_expected * eval_z;
```

After sumcheck, verifier checks: $C_{\text{final}} \stackrel{?}{=} \text{left\_expected} \cdot \text{eval\_z}$

##### 2.2: RAM Read/Write Checking (Twist)

**Location**: [`jolt-core/src/zkvm/ram/read_write_checking.rs`](../jolt-core/src/zkvm/ram/read_write_checking.rs)

**Three sumcheck instances in Stage 2** (all batched together):

**2.2a: RAF Evaluation Sumcheck**

Verifies that the RAM fingerprint accumulator $\widetilde{\text{raf}}$ is correctly computed.

**2.2b: Read/Write Checking Sumcheck** - The Main RAM Consistency Check

**Location**: [`jolt-core/src/zkvm/ram/read_write_checking.rs:1025-1055`](../jolt-core/src/zkvm/ram/read_write_checking.rs#L1025-L1055)

**The Actual Equation Being Verified**:

```rust
fn expected_output_claim(&self, accumulator, r) -> F {
    let eq_eval_cycle = EqPolynomial::mle_endian(r_prime, &r_cycle);
    let (_, ra_claim) = accumulator.get_virtual_polynomial_opening(...);
    let (_, val_claim) = accumulator.get_virtual_polynomial_opening(...);
    let (_, inc_claim) = accumulator.get_committed_polynomial_opening(...);

    eq_eval_cycle * ra_claim * (val_claim + gamma * (val_claim + inc_claim))
}
```

**The sumcheck proves**:

$$\boxed{\sum_{(j,k) \in \{0,1\}^{\log T + \log K}} \widetilde{\text{eq}}(r', j) \cdot \widetilde{\text{ra}}(j, k) \cdot \left(\widetilde{\text{Val}}(j, k) + \gamma \cdot (\widetilde{\text{Val}}(j, k) + \widetilde{\Delta}(j))\right) = C_0}$$

**Variables**:
- $T$ = trace length (cycles), $K$ = memory size (addresses)
- $\widetilde{\text{eq}}(r', j)$ = equality polynomial (binds to specific cycle from outer sumcheck)
- $\widetilde{\text{ra}}(j, k)$ = **read address one-hot** (1 if cycle $j$ accesses address $k$, else 0)
- $\widetilde{\text{Val}}(j, k)$ = memory value at address $k$, cycle $j$
- $\widetilde{\Delta}(j)$ = **increment** written at cycle $j$ (**committed** polynomial)
- $\gamma$ = random fingerprint challenge from transcript

**Understanding Each Polynomial**:

1. **$\widetilde{\text{ra}}(j, k)$ - Read Address (Virtual)**:
   - **How computed**: Both prover and verifier derive from bytecode + memory layout
     - Bytecode: "cycle $j$ accesses $\text{base} + \text{offset}$"
     - Memory layout: maps guest address \rightarrow  witness index $k$
   - **One-hot property**: Exactly one $k$ has value 1 per cycle $j$
   - **NOT committed**: Virtual polynomial, proven via Shout in Stage 4

2. **$\widetilde{\text{Val}}(j, k)$ - Memory Value (Virtual)**:
   - **How verifier computes**:
     $$\text{Val}(j, k) = \text{Val}_{\text{init}}(k) + \sum_{j' < j} \widetilde{\text{ra}}(j', k) \cdot \widetilde{\Delta}(j')$$

   - Start with initial state (preprocessing), apply all prior increments
   - **NOT committed**: Derived from initial state + $\widetilde{\Delta}$

3. **$\widetilde{\Delta}(j)$ - Write Increment (Committed)**:
   - **Domain**: Just cycle index (NOT address!) - $\{0,1\}^{\log T}$
   - **Value**: What was written at cycle $j$
   - **Example**: If cycle 5 writes 42: $\Delta(5) = 42$, read-only cycle: $\Delta(7) = 0$
   - **Committed**: Yes! This is part of the execution trace witness
   - **Code**: [`read_write_checking.rs:98-149`](../jolt-core/src/zkvm/ram/read_write_checking.rs#L98-L149) computes this

**Connection to Twist Theory** (from @docs/Jolt.md):

Standard Twist fingerprint: $f(i) = \gamma_0 + \gamma_1 \cdot \text{addr} + \gamma_2 \cdot \text{ts} + \gamma_3 \cdot \text{val}$

The equation above implements the **fractional grand product**:

$$\prod_{j=0}^{T-1} \frac{f_{\text{init}}(j) + f_{\text{write}}(j)}{f_{\text{read}}(j) + f_{\text{final}}(j)} = 1$$

In our equation:

- $\widetilde{\text{Val}}(j, k_j)$ = value **before** write
- $\widetilde{\text{Val}}(j, k_j) + \widetilde{\Delta}(j)$ = value **after** write
- The multiplicative form $\text{before} \cdot (\text{before} + \gamma \cdot \text{after})$ encodes the ratio

Taking logarithms: $\sum \log(\text{numerator}/\text{denominator}) = 0$, which is what the sumcheck verifies.

**Why this equation looks different from docs/Jolt.md**:

The full Twist protocol spans **multiple stages**:

- **Stage 2** (this sumcheck): Time-ordered memory consistency
- **Stage 3**: Address-ordered consistency
- **Stage 4**: Prove address-ordered is permutation of time-ordered

**Final Verification Check** (after sumcheck completes):

With challenges $\vec{r} = (r_{\text{cycle}}, r_{\text{addr}})$:

$$C_{\text{final}} \stackrel{?}{=} \widetilde{\text{eq}}(r', r_{\text{cycle}}) \cdot \widetilde{\text{ra}}(r) \cdot \left(\widetilde{\text{Val}}(r) + \gamma \cdot (\widetilde{\text{Val}}(r) + \widetilde{\Delta}(r_{\text{cycle}}))\right)$$

Where:

- $\widetilde{\text{eq}}(r', r_{\text{cycle}})$ - verifier computes (no commitment)
- $\widetilde{\text{ra}}(r)$, $\widetilde{\text{Val}}(r)$ - claimed evaluations (read from transcript, virtual)
- $\widetilde{\Delta}(r_{\text{cycle}})$ - committed polynomial (proven in Stage 5 via Dory PCS)

**Zero-Knowledge Property**: The verifier never learns actual memory values! She only verifies that fingerprints balance, proving consistency without revealing witness data.

***

### DETAILED EXPLANATION: The Complete Twist Protocol

**The Missing Piece: Twist's Permutation Argument**

Twist proves memory consistency by showing **two orderings** of memory operations are permutations:

1. **Time-ordered** $T$: Operations by cycle number
2. **Address-ordered** $A$: Same operations sorted by address

**Example**: `[(0x100, ts:0, val:5), (0x200, ts:1, val:10), (0x100, ts:2, val:15)]` in time order becomes `[(0x100, ts:0, val:5), (0x100, ts:2, val:15), (0x200, ts:1, val:10)]` in address order.

**Method**: Grand products of fingerprints must match:
$$\prod_{\text{op} \in T} f(\text{op}) = \prod_{\text{op} \in A} f(\text{op})$$

where $f = \gamma_0 + \gamma_1 \cdot \text{addr} + \gamma_2 \cdot \text{ts} + \gamma_3 \cdot \text{val}$

***

**How Stage 2's Equation Fits Into This**

**To be extra clear**: The equation `val + gamma * (val + inc)` encodes:

**Component breakdown**:
- `val` = $\widetilde{\text{Val}}(j, k)$ = value **before** write at cycle $j$ = **DENOMINATOR term** (read value)
- `val + inc` = value **after** write = **NUMERATOR term** (written value)
- **Ordering**: **TIME-ORDERED** (indexed by cycle $j$, NOT address-ordered)

**Full expression**:
$$\text{val} + \gamma \cdot (\text{val} + \text{inc}) = \text{val}(1 + \gamma) + \gamma \cdot \text{inc}$$

This is a **multiplicative encoding** that works in finite fields without needing logarithms, while still encoding the ratio structure from Twist's fractional identity:
$$\prod_{k} \frac{f_{\text{init}}(k) + f_{\text{writes}}(k)}{f_{\text{reads}}(k) + f_{\text{final}}(k)} = 1$$

**Stage 2 (this sumcheck)** builds the **time-ordered** product. **Stages 3-4 complete the comparison**:

**Stage 3: Val Evaluation** ([`val_evaluation.rs`](../jolt-core/src/zkvm/ram/val_evaluation.rs))
- Computes **address-ordered** fingerprint product
- Same operations, different ordering!

**Stage 4: Ra Virtualization** ([`ra_virtual.rs`](../jolt-core/src/zkvm/ram/ra_virtual.rs))
- **THE PERMUTATION PROOF**: Proves $\widetilde{\text{ra}}_{\text{time}}$ and $\widetilde{\text{ra}}_{\text{addr}}$ represent identical operations
- Uses Shout to show: $\{\text{accessed pairs in time order}\} = \{\text{accessed pairs in address order}\}$

**Stage 4: Hamming Weight & Booleanity** ([`hamming_weight.rs`](../jolt-core/src/zkvm/ram/hamming_weight.rs), [`booleanity.rs`](../jolt-core/src/zkvm/ram/booleanity.rs))
- Prove $\widetilde{\text{ra}}$ is one-hot: $\sum_k \text{ra}(j,k) = 1$ and $\text{ra}(j,k) \in \{0,1\}$

***

**Concrete Example**

Memory operations:
```
Cycle 0: WRITE 0x100 \leftarrow  5
Cycle 1: READ  0x100 \rightarrow  ? (should be 5)
Cycle 2: WRITE 0x100 \leftarrow  10
Cycle 3: READ  0x100 \rightarrow  ? (should be 10)
```

**Time-ordered fingerprints** (Stage 2): $P_T = f(100,0,5) \cdot f(100,1,5) \cdot f(100,2,10) \cdot f(100,3,10)$

**Address-ordered fingerprints** (Stage 3): $P_A = f(100,0,5) \cdot f(100,1,5) \cdot f(100,2,10) \cdot f(100,3,10)$

**If read is correct**: $P_T = P_A$ 

**If Cycle 1 reads wrong value (99)**:
- $P_T = f(100,0,5) \cdot f(100,1,\mathbf{99}) \cdot \ldots$
- $P_A = f(100,0,5) \cdot f(100,1,\mathbf{5}) \cdot \ldots$
- $P_T \neq P_A$ \rightarrow  Verification fails! 

***

**Summary: The Complete Picture**

1. **Stage 2** (this section): Compute **time-ordered** fingerprint products
2. **Stage 3**: Compute **address-ordered** fingerprint products \rightarrow  completes the comparison
3. **Stage 4**: Prove orderings are permutations + one-hot properties
4. **Result**: Products match + permutation proven \rightarrow  memory is consistent!

The equation `val + gamma * (val + inc)` is one component of the time-ordered fingerprint, which Stages 3-4 will verify matches the address-ordered version.

##### 2.3: Registers Read/Write Checking (Twist)

**Location**: [`jolt-core/src/zkvm/registers/read_write_checking.rs`](../jolt-core/src/zkvm/registers/read_write_checking.rs)

Registers use the **same Twist protocol as RAM**, but with key differences:

- **Fixed size**: $K=64$ registers (32 RISC-V standard + 32 virtual)
- **Three operations per cycle**: Two reads (`rs1`, `rs2`) + one write (`rd`)
- **Chunking parameter**: $d=1$ (no chunking needed, K is small)

***

#### The Complete Sumcheck Equation (Stage 2)

**Equation being verified** ([`read_write_checking.rs:1157-1160`](../jolt-core/src/zkvm/registers/read_write_checking.rs#L1157-L1160)):

$$\boxed{\sum_{(k,j) \in \{0,1\}^{\log K} \times \{0,1\}^{\log T}} \widetilde{\text{eq}}(\vec{r}'_{\text{cycle}}, j) \cdot \left[\widetilde{\text{wa}}_{\text{rd}}(k,j) \cdot (\text{inc}(k,j) + \text{val}(k,j)) + \gamma \cdot \widetilde{\text{ra}}_{\text{rs1}}(k,j) \cdot \text{val}(k,j) + \gamma^2 \cdot \widetilde{\text{ra}}_{\text{rs2}}(k,j) \cdot \text{val}(k,j)\right] = C}$$

**Variables**:
- $\widetilde{\text{wa}}_{\text{rd}}(k, j)$ = Write address polynomial for destination register `rd`
  - Virtual polynomial (not committed)
  - 1 if instruction at cycle $j$ writes to register $k$, 0 otherwise
  - One-hot: $\sum_k \widetilde{\text{wa}}_{\text{rd}}(k,j) = 1$ for each cycle $j$

- $\widetilde{\text{ra}}_{\text{rs1}}(k, j)$, $\widetilde{\text{ra}}_{\text{rs2}}(k, j)$ = Read address polynomials for source registers
  - Virtual polynomials (not committed)
  - 1 if instruction at cycle $j$ reads from register $k$, 0 otherwise
  - One-hot: $\sum_k \widetilde{\text{ra}}_{\text{rs1}}(k,j) = 1$

- $\text{val}(k, j)$ = Value in register $k$ **before** cycle $j$ executes
  - Virtual polynomial (materialized during sumcheck after first phase)
  - Computed from checkpoints: values at chunk boundaries + deltas within chunks

- $\text{inc}(k, j)$ = Increment to register $k$ at cycle $j$
  - **Committed polynomial** (part of execution trace commitment)
  - $\text{inc}(k,j) = \text{post\_value} - \text{pre\_value}$ if $\widetilde{\text{wa}}_{\text{rd}}(k,j) = 1$, else 0
  - `CommittedPolynomial::RdInc` in code

- $\widetilde{\text{eq}}(\vec{r}'_{\text{cycle}}, j)$ = Equality polynomial at challenge point from Spartan
  - $\vec{r}'_{\text{cycle}}$ comes from Stage 1 Spartan sumcheck
  - Connects registers to R1CS constraints

- $\gamma$ = Random batching coefficient from transcript
- $C$ = Initial claim from Spartan (batched register values)

***

#### Initial Claim Computation

**Code** ([`read_write_checking.rs:236-249`](../jolt-core/src/zkvm/registers/read_write_checking.rs#L236-L249)):

```rust
let (r_cycle, rs1_rv_claim) = accumulator.borrow()
    .get_virtual_polynomial_opening(VirtualPolynomial::Rs1Value, SumcheckId::SpartanOuter);
let (_, rs2_rv_claim) = accumulator.borrow()
    .get_virtual_polynomial_opening(VirtualPolynomial::Rs2Value, SumcheckId::SpartanOuter);
let (_, rd_wv_claim) = accumulator.borrow()
    .get_virtual_polynomial_opening(VirtualPolynomial::RdWriteValue, SumcheckId::SpartanOuter);

let transcript = &mut *state_manager.transcript.borrow_mut();
let gamma: F = transcript.challenge_scalar();
let input_claim = rd_wv_claim + gamma * rs1_rv_claim + gamma.square() * rs2_rv_claim;
```

**Mathematical interpretation**:
$$C = \text{rd\_wv\_claim} + \gamma \cdot \text{rs1\_rv\_claim} + \gamma^2 \cdot \text{rs2\_rv\_claim}$$

These are **virtual polynomial openings from Stage 1 Spartan**, representing the register values used in R1CS constraints.

***

#### Expected Output Claim (Final Check)

**Code** ([`read_write_checking.rs:1157-1160`](../jolt-core/src/zkvm/registers/read_write_checking.rs#L1157-L1160)):

```rust
eq_eval_cycle

    * (rd_wa_claim * (inc_claim + val_claim)
        + self.gamma * rs1_ra_claim * val_claim
        + self.gamma_sqr * rs2_ra_claim * val_claim)
```

**Breakdown**:

1. **Extract opening point** (lines 1125-1128): Reconstructs $\vec{r}_{\text{cycle}}$ from sumcheck challenges

2. **Get eq evaluation** (lines 1129-1134):
   ```rust
   let eq_eval_cycle = EqPolynomial::mle_endian(&r_prime, &r_cycle);
   ```
   Computes $\widetilde{\text{eq}}(\vec{r}'_{\text{cycle}}, \vec{r}_{\text{cycle}})$

3. **Get claimed evaluations** (lines 1136-1155):
   - `val_claim` = $\widetilde{\text{val}}(\vec{r}_k, \vec{r}_j)$
   - `rs1_ra_claim` = $\widetilde{\text{ra}}_{\text{rs1}}(\vec{r}_k, \vec{r}_j)$
   - `rs2_ra_claim` = $\widetilde{\text{ra}}_{\text{rs2}}(\vec{r}_k, \vec{r}_j)$
   - `rd_wa_claim` = $\widetilde{\text{wa}}_{\text{rd}}(\vec{r}_k, \vec{r}_j)$
   - `inc_claim` = $\widetilde{\text{inc}}(\vec{r}_k, \vec{r}_j)$ (committed polynomial)

4. **Final computation**:
$$\widetilde{\text{eq}}(\vec{r}', \vec{r}_{\text{cycle}}) \cdot \left[\widetilde{\text{wa}}_{\text{rd}} \cdot (\text{inc} + \text{val}) + \gamma \cdot \widetilde{\text{ra}}_{\text{rs1}} \cdot \text{val} + \gamma^2 \cdot \widetilde{\text{ra}}_{\text{rs2}} \cdot \text{val}\right]$$

***

#### How This Fits Into Twist's Permutation Argument

**Component breakdown** (similar to RAM):

**Write term**: $\widetilde{\text{wa}}_{\text{rd}}(k,j) \cdot (\text{inc}(k,j) + \text{val}(k,j))$
- $\text{val}(k,j)$ = value **before** write = **DENOMINATOR term**
- $\text{inc}(k,j) + \text{val}(k,j)$ = value **after** write = **NUMERATOR term**
- **Ordering**: TIME-ORDERED (indexed by cycle $j$)

**Read terms**: $\gamma \cdot \widetilde{\text{ra}}_{\text{rs1}}(k,j) \cdot \text{val}(k,j) + \gamma^2 \cdot \widetilde{\text{ra}}_{\text{rs2}}(k,j) \cdot \text{val}(k,j)$
- Two reads per cycle (R-type instructions read from `rs1` and `rs2`)
- Both read the **before-write** value
- Batched with different powers of $\gamma$ to treat as independent reads

**Stage 2 (this sumcheck)** builds the **time-ordered** fingerprint products. **Stage 3 completes the comparison** (see Section 3.2 below).

***

#### Key Difference from RAM

Unlike RAM (which has separate read-checking, write-checking, and output sumchecks), **registers combine all three into one sumcheck**:

- **Two read-checking fingerprints**: $\gamma \cdot \widetilde{\text{ra}}_{\text{rs1}} \cdot \text{val}$ and $\gamma^2 \cdot \widetilde{\text{ra}}_{\text{rs2}} \cdot \text{val}$
- **One write-checking fingerprint**: $\widetilde{\text{wa}}_{\text{rd}} \cdot (\text{inc} + \text{val})$
- **All batched together** using powers of $\gamma$

This is possible because:

1. Registers are small ($K=64$), so no chunking needed
2. Register addresses are **hardcoded in bytecode** (no need for separate one-hot checks)
3. Soundness of bytecode sumcheck ensures $\widetilde{\text{ra}}$ and $\widetilde{\text{wa}}$ are one-hot

##### 2.4: Instruction Lookups (Shout) - Booleanity Sumcheck

**Location**: [`jolt-core/src/zkvm/instruction_lookups/booleanity.rs`](../jolt-core/src/zkvm/instruction_lookups/booleanity.rs)

Instruction lookups use **Shout** (offline memory checking) to prove that every instruction execution accessed the correct entry in a pre-computed lookup table. Unlike Twist (which handles read-write memory), Shout handles **read-only lookups** into static tables.

**Key challenge**: Instruction tables are **massive** - size $2^{128}$ (two 64-bit operands). Naively committing to or summing over such tables is impossible.

**Jolt's solution**: **Chunked decomposition** + **prefix-suffix sumcheck**
- Decompose 128-bit lookup index into $D=16$ chunks of 8 bits each
- Each chunk indexes into a tiny $2^8 = 256$-entry sub-table
- Prove each chunk lookup is correct via Shout sumcheck

***

#### The Complete Sumcheck Equation (Stage 2)

**Equation being verified** ([`booleanity.rs:239-253`](../jolt-core/src/zkvm/instruction_lookups/booleanity.rs#L239-L253)):

$$\boxed{\sum_{k \in \{0,1\}^{\log K_{\text{chunk}}}} \sum_{j \in \{0,1\}^{\log T}} \left[\sum_{i=0}^{D-1} \gamma^i \cdot \widetilde{\text{ra}}_i(k,j) \cdot (1 - \widetilde{\text{ra}}_i(k,j))\right] \cdot \widetilde{\text{eq}}(\vec{r}_{\text{addr}}, k) \cdot \widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, j) = 0}$$

**Variables**:
- $D = 16$ = Number of chunks (128-bit index divided into 16 \times  8-bit chunks)
- $K_{\text{chunk}} = 2^8 = 256$ = Size of each sub-table
- $T$ = Trace length (number of cycles)

- $\widetilde{\text{ra}}_i(k, j)$ = Read address polynomial for chunk $i$
  - **Committed polynomial** (`CommittedPolynomial::InstructionRa(i)`)
  - One-hot: equals 1 if instruction at cycle $j$ accesses sub-table entry $k$ in chunk $i$
  - Property: $\sum_k \widetilde{\text{ra}}_i(k,j) = 1$ for each cycle $j$ (exactly one entry accessed)

- $\widetilde{\text{eq}}(\vec{r}_{\text{addr}}, k)$ = Equality polynomial over address dimension
  - $\vec{r}_{\text{addr}}$ = Random challenge point (length $\log K_{\text{chunk}} = 8$)
  - Computes $\prod_{b=0}^{7} (r_b \cdot k_b + (1-r_b) \cdot (1-k_b))$

- $\widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, j)$ = Equality polynomial over cycle dimension
  - $\vec{r}_{\text{cycle}}$ = Random challenge from Spartan Stage 1
  - Connects instruction lookups to R1CS constraints

- $\gamma$ = Batching coefficient from transcript
  - Used to batch all $D=16$ chunk sumchecks together
  - Security: Different chunks become linearly independent

**What does this equation prove?**

The expression $\widetilde{\text{ra}}_i(k,j) \cdot (1 - \widetilde{\text{ra}}_i(k,j))$ equals **0 if and only if** $\widetilde{\text{ra}}_i(k,j) \in \{0,1\}$.

**Summing to 0 proves**: All $\widetilde{\text{ra}}_i$ polynomials are **Boolean** (only take values 0 or 1), which is required for them to be valid one-hot encodings.

***

#### Initial Claim

**Code** ([`booleanity.rs:161-163`](../jolt-core/src/zkvm/instruction_lookups/booleanity.rs#L161-L163)):

```rust
fn input_claim(&self) -> F {
    F::zero()
}
```

**Initial claim is always 0** - we're proving a booleanity constraint, not checking a specific sum.

***

#### Expected Output Claim (Final Check)

**Code** ([`booleanity.rs:215-253`](../jolt-core/src/zkvm/instruction_lookups/booleanity.rs#L215-L253)):

```rust
fn expected_output_claim(
    &self,
    accumulator: Option<Rc<RefCell<VerifierOpeningAccumulator<F>>>>,
    r_prime: &[F::Challenge],
) -> F {
    let accumulator = accumulator.as_ref().unwrap();
    let ra_claims = (0..D).map(|i| {
        accumulator.borrow()
            .get_committed_polynomial_opening(
                CommittedPolynomial::InstructionRa(i),
                SumcheckId::InstructionBooleanity,
            )
            .1
    });
    let r_cycle = accumulator.borrow()
        .get_virtual_polynomial_opening(
            VirtualPolynomial::LookupOutput,
            SumcheckId::SpartanOuter,
        )
        .0.r.clone();

    EqPolynomial::<F>::mle(
        r_prime,
        &self.r_address.iter().cloned().rev()
            .chain(r_cycle.into_iter().rev()).collect::<Vec<_>>(),
    ) * ra_claims.zip(self.gamma.iter())
        .map(|(ra, gamma)| *gamma * ra * (F::one() - ra))
        .fold(F::zero(), |acc, x| acc + x)
}
```

**Breakdown**:

1. **Get $D=16$ claimed evaluations** (lines 221-229):
   ```rust
   let ra_claims = (0..D).map(|i| {
       accumulator.borrow().get_committed_polynomial_opening(
           CommittedPolynomial::InstructionRa(i), ...
       ).1
   });
   ```
   Each $\text{ra\_claim}_i = \widetilde{\text{ra}}_i(\vec{r}_k, \vec{r}_j)$

2. **Get cycle challenge** (lines 230-238): $\vec{r}_{\text{cycle}}$ from Spartan

3. **Compute eq polynomial** (lines 240-245):
   $$\widetilde{\text{eq}}(\vec{r}'_{\text{addr}} || \vec{r}'_{\text{cycle}}, \vec{r}_{\text{addr}} || \vec{r}_{\text{cycle}})$$
   Combines address and cycle dimensions

4. **Compute batched booleanity** (lines 246-248):
   $$\sum_{i=0}^{15} \gamma^i \cdot \text{ra\_claim}_i \cdot (1 - \text{ra\_claim}_i)$$

5. **Final result**:
   $$\text{expected\_claim} = \widetilde{\text{eq}}(\vec{r}', \vec{r}) \cdot \sum_{i=0}^{15} \gamma^i \cdot \widetilde{\text{ra}}_i(\vec{r}) \cdot (1 - \widetilde{\text{ra}}_i(\vec{r}))$$

If this matches the sumcheck output, the verifier accepts that all $\widetilde{\text{ra}}_i$ are Boolean.

***

#### Why Booleanity Is Critical

**The Shout protocol requires**:
1. $\widetilde{\text{ra}}_i(k,j) \in \{0,1\}$ (proved by this sumcheck)
2. $\sum_k \widetilde{\text{ra}}_i(k,j) = 1$ for each $j$ (proved by Hamming weight sumcheck in Stage 3)
3. Together: $\widetilde{\text{ra}}_i$ is **one-hot** (exactly one 1 per row, rest 0s)

**One-hot encoding means**: At cycle $j$, the instruction accessed exactly one entry $k$ in sub-table $i$.

***

#### How This Fits Into Multi-Stage Instruction Lookup

**Stage 2: Booleanity** (this sumcheck)
- Proves: $\widetilde{\text{ra}}_i$ values are Boolean (0 or 1)

**Stage 3: Read Raf + Hamming Weight** (see Section 3.5)
- **Read Raf**: Proves lookups return correct table values
- **Hamming Weight**: Proves $\sum_k \widetilde{\text{ra}}_i(k,j) = 1$ (one-hot property)

**Stage 4: Ra Sumcheck** (see Section 4.X)
- Completes the Shout permutation argument
- Proves accessed entries match execution trace

**Together**: Complete instruction lookup verification - every instruction execution correctly accessed the lookup table and used the correct output value.

***

#### Key Insight: Chunked Decomposition

**Why chunk into $D=16$ pieces?**

**Naive approach** (infeasible):
- Commit to full $2^{128}$-entry table
- Prove lookups into this giant table
- Cost: Astronomical (cannot even store table)

**Jolt's approach** (tractable):
- **Decompose**: 128-bit index = 16 \times  8-bit chunks
- **16 small lookups**: Each into 256-entry sub-table
- **Composition proof**: Prove chunks wire together correctly (via R1CS constraints in Spartan)
- **Cost**: Linear in trace length $T$, independent of table size $2^{128}$

**Example**: Computing $\text{ADD}(a, b)$ where $a, b$ are 64-bit
- Lookup index: $(a, b)$ = 128 bits
- Chunk 0: $a[7:0]$ (low 8 bits of $a$) \rightarrow  lookup $\text{ADD}_0(a[7:0], b[7:0])$
- Chunk 1: $a[15:8]$ + carry \rightarrow  lookup $\text{ADD}_1(a[15:8], b[15:8], c_0)$
- ... (16 chunks total)
- Composition: Prove final result assembled correctly from 16 sub-results + carries

**Verification code location**: [`jolt-core/src/subprotocols/sumcheck.rs:185-261`](../jolt-core/src/subprotocols/sumcheck.rs#L185-L261)

```rust
pub fn verify<F, ProofTranscript>(
    proof: &BatchedSumcheckProof<F, ProofTranscript>,
    sumcheck_instances: Vec<&dyn SumcheckInstance<F, ProofTranscript>>,
    opening_accumulator: Option<Rc<RefCell<VerifierOpeningAccumulator<F>>>>,
    transcript: &mut ProofTranscript,
) -> Result<Vec<F::Challenge>, ProofVerifyError>
```

**Step-by-step**:

1. **Read initial claims** from all sumcheck instances
   ```rust
   let initial_claims: Vec<F> = sumcheck_instances.iter()
       .map(|instance| instance.input_claim())
       .collect();
   ```

2. **Verify coefficient polynomials** (if instances have different degrees)
   - Some sumchecks may have degree 2, others degree 3
   - Coefficient polynomials allow batching mixed-degree sumchecks

3. **For each round** $j$:
   ```rust
   for round in 0..num_rounds {
       // Read combined univariate polynomial from proof
       let combined_poly: CompressedUniPoly<F> = /* from proof */;

       // Compute expected sum for this round
       let expected_sum: F = /* combine all previous_claims with batching coefficients */;

       // Consistency check
       let computed_sum = combined_poly.evaluate(&F::zero()) +
                          combined_poly.evaluate(&F::one());
       if computed_sum != expected_sum {
           return Err(ProofVerifyError::SumcheckFailed);
       }

       // Sample challenge
       let r_j = transcript.challenge_scalar();

       // Update all claims
       for (i, instance) in sumcheck_instances.iter_mut().enumerate() {
           instance.bind(r_j, round);
           previous_claims[i] = instance.compute_expected_claim(r_j);
       }
   }
   ```

4. **Final step**: Accumulate opening claims
   ```rust
   for instance in sumcheck_instances {
       instance.cache_openings_verifier(
           opening_accumulator.clone(),
           transcript,
           r_sumcheck.clone(),
       );
   }
   ```

**Mathematical guarantee**: If all consistency checks pass, then with probability $\geq 1 - \frac{\nu d}{|\mathbb{F}|}$, the initial claims were correct (where $\nu$ = num rounds, $d$ = max degree).

***

### Stage 3: More Batched Sumchecks

**Location**: [`jolt-core/src/zkvm/dag/jolt_dag.rs:463-492`](../jolt-core/src/zkvm/dag/jolt_dag.rs#L463-L492)

Stage 3 verification is structurally identical to Stage 2, but with different sumcheck instances:

```rust
let stage3_instances: Vec<_> = std::iter::empty()
    .chain(spartan_dag.stage3_verifier_instances(&mut state_manager))
    .chain(registers_dag.stage3_verifier_instances(&mut state_manager))
    .chain(lookups_dag.stage3_verifier_instances(&mut state_manager))
    .chain(ram_dag.stage3_verifier_instances(&mut state_manager))
    .collect();

let _r_stage3 = BatchedSumcheck::verify(
    stage3_proof,
    stage3_instances_ref,
    Some(opening_accumulator.clone()),
    &mut *transcript.borrow_mut(),
).context("Stage 3")?;
```

**Stage 3 contains**:
- **Spartan PC sumcheck**: Verifies program counter increments correctly
- **Spartan product virtualization**: Verifies R1CS matrix product claims
- **Registers evaluation sumcheck**: Twist evaluation check (proves final register state)
- **RAM evaluation sumcheck**: Twist evaluation check (proves final RAM state)
- **Instruction lookups continuation**: Further Shout sumchecks

#### Exact Mathematical Equations for Stage 3 Sumchecks

##### 3.1: Spartan PC Sumcheck

**Location**: [`jolt-core/src/zkvm/spartan/pc.rs`](../jolt-core/src/zkvm/spartan/pc.rs)

This sumcheck proves **control flow consistency** - that the "next PC" values computed in Stage 1's R1CS constraints correctly match the actual PC values in the execution trace.

***

#### The Complete Sumcheck Equation

**Equation being verified**:

$$\boxed{\sum_{t \in \{0,1\}^{\log T}} \left[\widetilde{\text{UnexpandedPC}}(t) + \gamma \cdot \widetilde{\text{PC}}(t) + \gamma^2 \cdot \widetilde{\text{IsNoop}}(t)\right] \cdot \widetilde{\text{eq}}_{\text{+1}}(\vec{r}_{\text{cycle}}, t) = \widetilde{\text{NextUnexpandedPC}}(\vec{r}_{\text{cycle}}) + \gamma \cdot \widetilde{\text{NextPC}}(\vec{r}_{\text{cycle}}) + \gamma^2 \cdot \widetilde{\text{NextIsNoop}}(\vec{r}_{\text{cycle}})}$$

**Key distinction**:
- **Left side**: Sums over actual trace values (PC, UnexpandedPC, IsNoop) at **all cycles $t$**, weighted by eq+1 polynomial
- **Right side**: "Next*" values evaluated at the **single random point** $\vec{r}_{\text{cycle}}$ from Stage 1

**Important**: This is a **probabilistic check at ONE random point**, not a check at every cycle!

**How it works**:
1. $\vec{r}_{\text{cycle}}$ is a random challenge from Stage 1 (picked by verifier via Fiat-Shamir)
2. The eq+1 polynomial $\widetilde{\text{eq}}_{\text{+1}}(\vec{r}_{\text{cycle}}, t)$ acts as a "selector" that picks out cycle $\vec{r}_{\text{cycle}} + 1$
3. The sum collapses to: $\sum_t \text{PC}(t) \cdot \widetilde{\text{eq}}_{\text{+1}}(\vec{r}_{\text{cycle}}, t) = \text{PC}(\vec{r}_{\text{cycle}} + 1)$
4. This equals the MLE property: evaluating at a random point extracts one combination of values

So the equation proves:
$$\widetilde{\text{NextPC}}(\vec{r}_{\text{cycle}}) = \widetilde{\text{PC}}(\vec{r}_{\text{cycle}} + 1)$$

**at the single random point** $\vec{r}_{\text{cycle}}$ (not at every cycle).

**Security**: By Schwartz-Zippel lemma, if NextPC \neq  PC(shifted) at many cycles, the equality is unlikely to hold at a random $\vec{r}_{\text{cycle}}$. So checking one random point is sufficient with high probability.

**Variables**:
- $\widetilde{\text{PC}}(t)$ = MLE of program counter at cycle $t$
  - **Virtual polynomial** (NOT committed, generated from trace during proving)
  - **Prover**: Extracts from trace via `preprocessing.bytecode.get_pc(cycle)` ([`r1cs/inputs.rs:591`](../jolt-core/src/zkvm/r1cs/inputs.rs#L591))
  - **Verifier**: Reads claimed evaluation from transcript (sent via `append_virtual`)
  - Points to bytecode address being executed

- $\widetilde{\text{UnexpandedPC}}(t)$ = MLE of unexpanded PC (before inline expansion)
  - **Virtual polynomial** (NOT committed)
  - **Prover**: Extracts from trace via `cycle.instruction().normalize().address` ([`r1cs/inputs.rs:590`](../jolt-core/src/zkvm/r1cs/inputs.rs#L590))
  - **Verifier**: Reads claimed evaluation from transcript
  - For virtual instruction sequences (inlines), UnexpandedPC stays constant while PC increments
  - For regular instructions, UnexpandedPC = PC

- $\widetilde{\text{IsNoop}}(t)$ = MLE of no-op flag
  - **Virtual polynomial** (NOT committed)
  - **Prover**: Extracts from trace via circuit flags ([`r1cs/inputs.rs:592`](../jolt-core/src/zkvm/r1cs/inputs.rs#L592))
  - **Verifier**: Reads claimed evaluation from transcript
  - Equals 1 if cycle $t$ is padding (trace padded to power of 2), 0 otherwise

- $\widetilde{\text{eq}}_{\text{+1}}(\vec{r}_{\text{cycle}}, t)$ = **"Equality-plus-one" polynomial**
  - Multilinear extension of: $f(t) = 1$ if $t = \vec{r}_{\text{cycle}} + 1$, else $f(t) = 0$
  - This is the **key to the shift check**: it selects the value at the **next** cycle
  - Implementation: [`jolt-core/src/poly/eq_plus_one_poly.rs`](../jolt-core/src/poly/eq_plus_one_poly.rs)

- $\gamma$ = Batching coefficient from transcript
  - Used to batch three separate shift checks into one

- $\widetilde{\text{NextPC}}(\vec{r}_{\text{cycle}})$ = MLE of "next PC" from R1CS witness
  - **Virtual polynomial** from Stage 1 Spartan Outer sumcheck
  - **How it's computed** ([`r1cs/inputs.rs:144-148`](../jolt-core/src/zkvm/r1cs/inputs.rs#L144-L148)):
    - **Prover**: Extracts PC from the **next cycle** in the trace: `preprocessing.bytecode.get_pc(next_cycle)`
    - **R1CS constraints** then verify this value is *correct* based on instruction type
    - This is the **actual** PC value from cycle $t+1$ in the trace
  - R1CS constraints check (see [`constraints.rs:442-495`](../jolt-core/src/zkvm/r1cs/constraints.rs#L442-L495)):
    - Jump instruction: `NextUnexpandedPC == LookupOutput` (jump target)
    - Branch taken: `NextUnexpandedPC == UnexpandedPC + Imm`
    - Normal instruction: `NextUnexpandedPC == UnexpandedPC + 4` (or +2 for compressed)
    - Inline sequence: `NextPC == PC + 1`

- $\widetilde{\text{NextUnexpandedPC}}(\vec{r}_{\text{cycle}})$ = MLE of "next unexpanded PC"
  - **Virtual polynomial** from Stage 1
  - **Prover**: Extracts from next cycle: `next_cycle.instruction().normalize().address` ([`r1cs/inputs.rs:150-154`](../jolt-core/src/zkvm/r1cs/inputs.rs#L150-L154))
  - R1CS constraints verify this matches what the current instruction dictates

- $\widetilde{\text{NextIsNoop}}(\vec{r}_{\text{cycle}})$ = MLE of "next is-noop flag"
  - **Virtual polynomial** from Stage 1
  - **Prover**: Extracts from next cycle's flags ([`r1cs/inputs.rs:170-174`](../jolt-core/src/zkvm/r1cs/inputs.rs#L170-L174))
  - Indicates if next cycle is padding (trace padded to power of 2)

***

#### What This Equation Actually Proves

This is **NOT** a "initial + deltas = final" check. Instead, it's a **shift consistency check**:

**The check**: Prove that R1CS constraints' view of "what comes next" matches what actually came next in the trace.

**Two sources of "next PC"**:
1. **From trace (via eq+1 sum)**: $\text{PC}(t+1)$ - what the trace says the next PC is
2. **From R1CS witness**: $\text{NextPC}(t)$ - what was extracted from the next cycle and verified by constraints

**Stage 1** (R1CS constraints):
- Extracts `NextPC` from next_cycle in trace
- Constraints verify: "If this is a jump, then NextUnexpandedPC == jump_target"
- Constraints verify: "If this is normal, then NextUnexpandedPC == UnexpandedPC + 4"
- These constraints ensure NextPC is **self-consistent** with the instruction

**Stage 3** (PC Sumcheck, this section):
- Uses eq+1 polynomial to compute PC value **at cycle $t+1$** from the trace
- Proves: $\text{NextPC}(t) \stackrel{?}{=} \text{PC}(t+1)$
- Ensures: The "next" values extracted for constraints match the actual next cycle

**Together**: Complete control flow verification - R1CS-verified "next" values match actual subsequent trace values.

***

#### Initial Claim

**Code** ([`pc.rs:95-119`](../jolt-core/src/zkvm/spartan/pc.rs#L95-L119)):

```rust
pub fn new_verifier(...) -> Self {
    let gamma: F = state_manager.transcript.borrow_mut().challenge_scalar();
    let gamma_squared = gamma.square();

    // Get the Next* evaluations from the accumulator
    let accumulator = state_manager.get_verifier_accumulator();
    let (_, next_pc_eval) = accumulator.borrow()
        .get_virtual_polynomial_opening(VirtualPolynomial::NextPC, SumcheckId::SpartanOuter);
    let (_, next_unexpanded_pc_eval) = accumulator.borrow()
        .get_virtual_polynomial_opening(VirtualPolynomial::NextUnexpandedPC, SumcheckId::SpartanOuter);
    let (_, next_is_noop_eval) = accumulator.borrow()
        .get_virtual_polynomial_opening(VirtualPolynomial::NextIsNoop, SumcheckId::SpartanOuter);

    let input_claim = next_unexpanded_pc_eval + gamma * next_pc_eval + gamma_squared * next_is_noop_eval;
    ...
}
```

**Mathematical interpretation**:
$$C = \widetilde{\text{NextUnexpandedPC}}(\vec{r}) + \gamma \cdot \widetilde{\text{NextPC}}(\vec{r}) + \gamma^2 \cdot \widetilde{\text{NextIsNoop}}(\vec{r})$$

These "Next*" values come from **Stage 1 Spartan sumcheck**, where R1CS constraints computed the *intended* next PC values. Stage 3 verifies these match the *actual* PC values in the trace.

***

#### Expected Output Claim (Final Check)

**Code** ([`pc.rs:223-257`](../jolt-core/src/zkvm/spartan/pc.rs#L223-L257)):

```rust
fn expected_output_claim(
    &self,
    accumulator: Option<Rc<RefCell<VerifierOpeningAccumulator<F>>>>,
    r: &[F::Challenge],
) -> F {
    let accumulator = accumulator.as_ref().unwrap().borrow();

    // Get r_cycle from the SpartanOuter sumcheck opening point
    let (outer_sumcheck_opening, _) = accumulator
        .get_virtual_polynomial_opening(VirtualPolynomial::NextPC, SumcheckId::SpartanOuter);
    let (r_cycle, _) = outer_sumcheck_opening.r.split_at(self.log_T);

    // Get the shift evaluations from the accumulator
    let (_, unexpanded_pc_eval_at_shift_r) = accumulator
        .get_virtual_polynomial_opening(VirtualPolynomial::UnexpandedPC, SumcheckId::SpartanShift);
    let (_, pc_eval_at_shift_r) = accumulator
        .get_virtual_polynomial_opening(VirtualPolynomial::PC, SumcheckId::SpartanShift);
    let (_, is_noop_eval_at_shift_r) = accumulator
        .get_virtual_polynomial_opening(VirtualPolynomial::OpFlags(CircuitFlags::IsNoop), SumcheckId::SpartanShift);

    let batched_eval_at_shift_r = unexpanded_pc_eval_at_shift_r

        + self.gamma * pc_eval_at_shift_r
        + self.gamma_squared * is_noop_eval_at_shift_r;

    let eq_plus_one_shift_sumcheck = EqPlusOnePolynomial::<F>::new(r_cycle.to_vec()).evaluate(r);

    batched_eval_at_shift_r * eq_plus_one_shift_sumcheck
}
```

**Breakdown**:

1. **Get $\vec{r}_{\text{cycle}}$** (lines 230-236): Extract cycle challenge from Stage 1

2. **Get "shifted" evaluations** (lines 238-247):
   - `unexpanded_pc_eval_at_shift_r` = $\widetilde{\text{UnexpandedPC}}(\vec{r})$ at random point $\vec{r}$
   - `pc_eval_at_shift_r` = $\widetilde{\text{PC}}(\vec{r})$
   - `is_noop_eval_at_shift_r` = $\widetilde{\text{IsNoop}}(\vec{r})$
   - These are **virtual polynomial openings** (accumulated during sumcheck)

3. **Batch the evaluations** (lines 249-251):
   $$\text{batched} = \widetilde{\text{UnexpandedPC}}(\vec{r}) + \gamma \cdot \widetilde{\text{PC}}(\vec{r}) + \gamma^2 \cdot \widetilde{\text{IsNoop}}(\vec{r})$$

4. **Compute eq+1 polynomial** (lines 253-254):
   $$\widetilde{\text{eq}}_{\text{+1}}(\vec{r}_{\text{cycle}}, \vec{r})$$
   This evaluates the "next cycle selector" at the random point

5. **Final result** (line 256):
   $$\text{expected\_claim} = \text{batched} \cdot \widetilde{\text{eq}}_{\text{+1}}(\vec{r}_{\text{cycle}}, \vec{r})$$

If this matches the sumcheck output, the verifier accepts that NextPC values equal the actual next-cycle PC values.

***

#### Why This Matters

**Stage 1 (Spartan Outer)**: R1CS constraints computed:
- $\widetilde{\text{NextPC}}$ = What the next PC *should be* based on instruction type
- Jump instruction \rightarrow  NextPC = jump target
- Normal instruction \rightarrow  NextPC = PC + 4

**Stage 3 (PC Sumcheck)**: Proves:
- $\widetilde{\text{NextPC}}(\vec{r})$ actually equals $\widetilde{\text{PC}}(\vec{r} + 1)$ in the trace
- No gaps or inconsistencies in control flow
- Trace follows program execution rules

**Together**: Complete control flow verification for the zkVM.

***

#### Key Point: Virtual Polynomials vs Committed Polynomials

**Important distinction**: PC, UnexpandedPC, and IsNoop are **virtual polynomials** - they are:
-  **Generated**: Prover extracts them from the execution trace
-  **Sent via transcript**: Prover sends claimed evaluations (after sumcheck)
-  **NOT committed**: No polynomial commitment (no Dory commitment)
-  **NOT opened in Stage 5**: Verifier trusts them based on sumcheck soundness

**Why this is secure**:
1. The sumcheck protocol itself provides security via random challenges
2. If prover lies about PC, UnexpandedPC, or IsNoop evaluations, they'll fail to produce consistent univariate polynomials across rounds
3. Schwartz-Zippel lemma: Different polynomials unlikely to agree at random point
4. These virtual polynomials are **derived from trace data**, which IS committed (indirectly via other witness polynomials)

**Contrast with committed polynomials** (e.g., LeftInstructionInput, RightInstructionInput in section 3.2):
-  Explicitly committed using Dory PCS in preprocessing/proving
-  Claimed evaluations sent via transcript
-  **Verified in Stage 5** via batched opening proof

**The tradeoff**: Virtual polynomials save commitment cost but require sumcheck verification. This is Jolt's optimization - avoid committing to everything, use sumchecks to verify virtual claims.

##### 3.2: Spartan Product Virtualization Sumcheck

**Location**: [`jolt-core/src/zkvm/spartan/product.rs`](../jolt-core/src/zkvm/spartan/product.rs)

This sumcheck "**virtualizes**" the Product polynomial - proving that a claimed evaluation of a **product of two polynomials** is correct by checking the individual polynomial evaluations.

**Context**: In Stage 1, Spartan used a virtual polynomial $\widetilde{\text{Product}}(\vec{r}) = \widetilde{\text{Left}}(\vec{r}) \cdot \widetilde{\text{Right}}(\vec{r})$ without committing to Product directly. Stage 3 proves this virtual claim is consistent with the actual committed polynomials.

***

#### The Complete Sumcheck Equation

**Equation being verified**:

$$\boxed{\sum_{j \in \{0,1\}^{\log T}} \widetilde{\text{LeftInput}}(j) \cdot \widetilde{\text{RightInput}}(j) \cdot \widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, j) = C}$$

where $C$ is the initial claim ($\widetilde{\text{Product}}$ evaluation from Stage 1).

**Variables**:
- $\widetilde{\text{LeftInput}}(j)$ = MLE of left operand for instruction at cycle $j$
  - **Committed polynomial** (`CommittedPolynomial::LeftInstructionInput`)
  - Contains the first operand (typically from register `rs1`)
  - **Prover**: Extracts from trace ([`r1cs/inputs.rs:226`](../jolt-core/src/zkvm/r1cs/inputs.rs#L226))

- $\widetilde{\text{RightInput}}(j)$ = MLE of right operand for instruction at cycle $j$
  - **Committed polynomial** (`CommittedPolynomial::RightInstructionInput`)
  - Contains the second operand (typically from register `rs2`)
  - **Prover**: Extracts from trace ([`r1cs/inputs.rs:227`](../jolt-core/src/zkvm/r1cs/inputs.rs#L227))

- $\widetilde{\text{Product}}(\vec{r}_{\text{cycle}})$ = MLE of product Left \times  Right
  - **Virtual polynomial** from Stage 1 (NOT committed)
  - Used in R1CS constraints (e.g., for multiply instructions: RightLookupOperand == Product)
  - **Prover computes**: `Product = F::from_i128(right) * left` ([`r1cs/inputs.rs:228-230`](../jolt-core/src/zkvm/r1cs/inputs.rs#L228-L230))

- $\widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, j)$ = Equality polynomial
  - $\vec{r}_{\text{cycle}}$ = Challenge point from Stage 1
  - Connects this sumcheck to Stage 1's random point

**What does this equation prove?**

$$\widetilde{\text{Product}}(\vec{r}_{\text{cycle}}) = \sum_{j} \widetilde{\text{LeftInput}}(j) \cdot \widetilde{\text{RightInput}}(j) \cdot \widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, j)$$

By the definition of multilinear extensions, this equals:
$$\widetilde{\text{Product}}(\vec{r}_{\text{cycle}}) = \widetilde{\text{LeftInput}}(\vec{r}_{\text{cycle}}) \cdot \widetilde{\text{RightInput}}(\vec{r}_{\text{cycle}})$$

**In plain English**: The claimed product evaluation (at random point $\vec{r}_{\text{cycle}}$) equals the product of the individual input evaluations.

**Note**: This sumcheck ONLY verifies the product relationship. The Product value is later used in R1CS constraints (Stage 1) - for example, multiply instructions require `RightLookupOperand == Product`. This sumcheck bridges the virtual Product polynomial to the committed input polynomials.

***

#### Initial Claim

**Code** ([`product.rs:64-78`](../jolt-core/src/zkvm/spartan/product.rs#L64-L78)):

```rust
pub fn new_verifier(...) -> Self {
    let accumulator = state_manager.get_verifier_accumulator();
    // Get the Product claim from the accumulator
    let (_, input_claim) = accumulator.borrow()
        .get_virtual_polynomial_opening(VirtualPolynomial::Product, SumcheckId::SpartanOuter);
    let (_, _, T) = state_manager.get_verifier_data();

    Self {
        input_claim,
        prover_state: None,
        log_T: T.log_2(),
    }
}
```

**Mathematical interpretation**:
$$C = \widetilde{\text{Product}}(\vec{r}_{\text{cycle}})$$

This comes from **Stage 1 Spartan Outer sumcheck**, where the Product polynomial was used virtually (claimed but not opened).

***

#### Expected Output Claim (Final Check)

**Code** ([`product.rs:208-233`](../jolt-core/src/zkvm/spartan/product.rs#L208-L233)):

```rust
fn expected_output_claim(
    &self,
    accumulator: Option<Rc<RefCell<VerifierOpeningAccumulator<F>>>>,
    r: &[F::Challenge],
) -> F {
    let accumulator = accumulator.as_ref().unwrap().borrow();

    // Get r_cycle from the SpartanOuter sumcheck opening point
    let (outer_sumcheck_opening, _) = accumulator
        .get_virtual_polynomial_opening(VirtualPolynomial::Product, SumcheckId::SpartanOuter);
    let (r_cycle, _) = outer_sumcheck_opening.r.split_at(self.log_T);

    // Get the instruction input evaluations from the accumulator
    let (_, left_input_eval) = accumulator.get_committed_polynomial_opening(
        CommittedPolynomial::LeftInstructionInput,
        SumcheckId::ProductVirtualization,
    );
    let (_, right_input_eval) = accumulator.get_committed_polynomial_opening(
        CommittedPolynomial::RightInstructionInput,
        SumcheckId::ProductVirtualization,
    );

    let eq_eval = EqPolynomial::mle(&r.iter().rev().copied().collect::<Vec<_>>(), r_cycle);
    eq_eval * left_input_eval * right_input_eval
}
```

**Breakdown**:

1. **Get $\vec{r}_{\text{cycle}}$** (lines 216-220): Extract cycle challenge from Stage 1

2. **Get input evaluations** (lines 222-229):
   - `left_input_eval` = $\widetilde{\text{LeftInput}}(\vec{r})$ at random point $\vec{r}$
   - `right_input_eval` = $\widetilde{\text{RightInput}}(\vec{r})$
   - These are **committed polynomial openings** (will be verified in Stage 5 via Dory)

3. **Compute eq polynomial** (line 231):
   $$\widetilde{\text{eq}}(\vec{r}, \vec{r}_{\text{cycle}})$$

4. **Final result** (line 232):
   $$\text{expected\_claim} = \widetilde{\text{eq}}(\vec{r}, \vec{r}_{\text{cycle}}) \cdot \widetilde{\text{LeftInput}}(\vec{r}) \cdot \widetilde{\text{RightInput}}(\vec{r})$$

If this matches the sumcheck output, the verifier accepts that the Product claim is consistent with the committed input polynomials.

***

#### Why Virtualization Matters

**The problem**: In Stage 1, Spartan's R1CS constraints use many polynomial products:
- $\widetilde{\text{Product}} = \widetilde{\text{Left}} \cdot \widetilde{\text{Right}}$
- $\widetilde{\text{Ra\_Rs1\_Val}} = \widetilde{\text{Ra\_Rs1}} \cdot \widetilde{\text{Rs1Value}}$
- ... (dozens of products)

**Naive approach** (expensive):
- Commit to every product polynomial explicitly
- Prove each commitment is correct
- Cost: $O(n)$ commitments, each requiring expensive cryptography

**Virtualization approach** (efficient):
- **Don't commit** to products directly
- Use products virtually in sumchecks
- **Later** (in Stage 3), prove virtual claims equal actual products
- Cost: One sumcheck per virtual polynomial (much cheaper than commitments)

**Result**: Massive savings in preprocessing size and prover time, while maintaining security.

**Degree note**: This sumcheck has **degree 3** (product of two polynomials \times  eq polynomial), higher than most other sumchecks (degree 2).

##### 3.3: RAM Evaluation Sumcheck (Twist)

**Location**: [`jolt-core/src/zkvm/ram/val_evaluation.rs`](../jolt-core/src/zkvm/ram/val_evaluation.rs)

This is **Stage 3's continuation of the Twist protocol for RAM**. After Stage 2 built the time-ordered fingerprint products, Stage 3 computes the **address-ordered** evaluation.

***

#### The Sumcheck Equation

**High-level concept**: Twist's evaluation sumcheck verifies the final memory state equals initial state plus all increments:

$$\sum_{k \in \{0,1\}^{\log K}} \widetilde{\text{Val}}_{\text{final}}(k) = \sum_{k \in \{0,1\}^{\log K}} \left(\widetilde{\text{Val}}_{\text{init}}(k) + \sum_{j \in \{0,1\}^{\log T}} \widetilde{\text{wa}}(k,j) \cdot \widetilde{\Delta}(k,j)\right)$$

**Equation being verified** ([`val_evaluation.rs:274`](../jolt-core/src/zkvm/ram/val_evaluation.rs#L274)):

$$\boxed{\sum_{j' \in \{0,1\}^{\log T}} \widetilde{\text{wa}}(\vec{r}_{\text{addr}}, j') \cdot \widetilde{\text{inc}}(\vec{r}_{\text{addr}}, j') \cdot \widetilde{\text{LT}}(j', \vec{r}_{\text{cycle}}) = \text{val\_claim} - \text{init\_eval}}$$

**Variables**:
- $\vec{r}_{\text{addr}}$ = Random challenge point for memory address (from Stage 2, has length $\log K$)
- $\vec{r}_{\text{cycle}}$ = Random challenge point for cycle (from Stage 2)
- $j'$ = Sumcheck variable ranging over cycles $\{0,1\}^{\log T}$

- $\widetilde{\text{wa}}(\vec{r}_{\text{addr}}, j')$ = Write address polynomial evaluated at $(\vec{r}_{\text{addr}}, j')$
  - Virtual polynomial (computed by verifier)
  - Equals 1 if address $\vec{r}_{\text{addr}}$ is written at cycle $j'$, 0 otherwise (in Boolean hypercube)

- $\widetilde{\text{inc}}(\vec{r}_{\text{addr}}, j')$ = Increment polynomial evaluated at $(\vec{r}_{\text{addr}}, j')$
  - **Committed polynomial** (`CommittedPolynomial::RamInc`)
  - Value written - value before write
  - For reads: increment = 0

- $\widetilde{\text{LT}}(j', \vec{r}_{\text{cycle}})$ = "Less-than" polynomial
  - Multilinear extension of the predicate: $j' < \vec{r}_{\text{cycle}}$
  - Ensures we only sum increments that happened **before** the challenge cycle
  - Formula (lines 257-263):
    ```rust
    let mut lt_eval = F::zero();
    let mut eq_term = F::one();
    for (x, y) in r.iter().zip(r_cycle.r.iter()) {
        lt_eval += (F::one() - x) * y * eq_term;
        eq_term *= F::one() - x - y + *x * y + *x * y;
    }
    ```

- $\text{val\_claim}$ = Claimed evaluation from Stage 2 RAM read/write checking
- $\text{init\_eval}$ = Initial memory value at address $\vec{r}_{\text{addr}}$ (from preprocessing)

***

#### Code Breakdown

**Input claim** ([`val_evaluation.rs:192-193`](../jolt-core/src/zkvm/ram/val_evaluation.rs#L192-L193)):
```rust
fn input_claim(&self) -> F {
    self.claimed_evaluation - self.init_eval
}
```
This is the net change in memory: final value - initial value = sum of all increments.

**Expected output claim** ([`val_evaluation.rs:246-275`](../jolt-core/src/zkvm/ram/val_evaluation.rs#L246-L275)):
```rust
fn expected_output_claim(&self, accumulator: Option<Rc<RefCell<VerifierOpeningAccumulator<F>>>>, r: &[F::Challenge]) -> F {
    let accumulator = accumulator.as_ref().unwrap();

    // Get r_val = (r_addr, r_cycle) from Stage 2
    let (r_val, _) = accumulator.borrow().get_virtual_polynomial_opening(
        VirtualPolynomial::RamVal,
        SumcheckId::RamReadWriteChecking,
    );
    let (_, r_cycle) = r_val.split_at(self.K.log_2());

    // Compute LT(r', r_cycle) where r' is the sumcheck challenge
    let mut lt_eval = F::zero();
    let mut eq_term = F::one();
    for (x, y) in r.iter().zip(r_cycle.r.iter()) {
        lt_eval += (F::one() - x) * y * eq_term;
        eq_term *= F::one() - x - y + *x * y + *x * y;
    }

    // Get inc evaluation at (r_addr, r')
    let (_, inc_claim) = accumulator.borrow().get_committed_polynomial_opening(
        CommittedPolynomial::RamInc,
        SumcheckId::RamValEvaluation,
    );

    // Get wa evaluation at (r_addr, r')
    let (_, wa_claim) = accumulator.borrow()
        .get_virtual_polynomial_opening(
            VirtualPolynomial::RamRa,
            SumcheckId::RamValEvaluation
        );

    // Return inc * wa * LT
    inc_claim * wa_claim * lt_eval
}
```

**Prover message** ([`val_evaluation.rs:196-234`](../jolt-core/src/zkvm/ram/val_evaluation.rs#L196-L234)):
```rust
fn compute_prover_message(&mut self, _round: usize, _previous_claim: F) -> Vec<F> {
    let ps = self.prover_state.as_ref().expect("Prover state not initialized");

    const DEGREE: usize = 3;  // Product of 3 polynomials: inc * wa * lt
    (0..ps.inc.len() / 2)
        .into_par_iter()
        .map(|i| {
            let inc_evals = ps.inc.sumcheck_evals_array::<DEGREE>(i, BindingOrder::HighToLow);
            let wa_evals = ps.wa.sumcheck_evals(i, DEGREE, BindingOrder::HighToLow);
            let lt_evals = ps.lt.sumcheck_evals_array::<DEGREE>(i, BindingOrder::HighToLow);

            // Compute inc * wa * lt for each degree point
            [
                (inc_evals[0] * wa_evals[0]).mul_unreduced::<9>(lt_evals[0]),
                (inc_evals[1] * wa_evals[1]).mul_unreduced::<9>(lt_evals[1]),
                (inc_evals[2] * wa_evals[2]).mul_unreduced::<9>(lt_evals[2]),
            ]
        })
        .reduce(|| [F::Unreduced::zero(); DEGREE], |running, new| {
            [running[0] + new[0], running[1] + new[1], running[2] + new[2]]
        })
        .into_iter()
        .map(F::from_montgomery_reduce)
        .collect()
}
```

***

#### Mathematical Interpretation

**What does this equation prove?**

$$\widetilde{\text{val}}(\vec{r}_{\text{addr}}, \vec{r}_{\text{cycle}}) - \text{init}(\vec{r}_{\text{addr}}) = \sum_{j' < \vec{r}_{\text{cycle}}} \left[\text{increments to address } \vec{r}_{\text{addr}} \text{ at cycles } j' \right]$$

The value at memory address $\vec{r}_{\text{addr}}$ at cycle $\vec{r}_{\text{cycle}}$ equals the initial value plus the sum of all increments (writes) to that address before that cycle.

**The LT polynomial ensures**:
- Only count writes that happened **before** cycle $\vec{r}_{\text{cycle}}$
- This matches the semantics: reads see the most recent write **before** them

**Why this equation**: This is the "evaluation check" in Twist. Combined with Stage 2's read/write checking (which proves time-ordered consistency via fingerprint products), this proves complete memory consistency.

##### 3.4: Registers Evaluation Sumcheck (Twist)

**Location**: [`jolt-core/src/zkvm/registers/val_evaluation.rs`](../jolt-core/src/zkvm/registers/val_evaluation.rs)

This is **Stage 3's continuation of the Twist protocol for registers**. After Stage 2 built the time-ordered fingerprint products, Stage 3 computes the **address-ordered** evaluation.

***

#### The Sumcheck Equation

**What we're proving**: The claimed `val` polynomial evaluation from Stage 2 is consistent with the actual register increments.

**Equation being verified** ([`val_evaluation.rs:218-219`](../jolt-core/src/zkvm/registers/val_evaluation.rs#L218-L219)):

$$\boxed{\sum_{j' \in \{0,1\}^{\log T}} \widetilde{\text{wa}}_{\text{rd}}(\vec{r}') \cdot \widetilde{\text{inc}}(\vec{r}', j') \cdot \widetilde{\text{LT}}(j', \vec{r}_{\text{cycle}}) = \text{val\_claim}}$$

**Variables**:
- $\vec{r}'$ = Random challenge point for register address (from Stage 2, has length $\log K = 6$)
- $\vec{r}_{\text{cycle}}$ = Random challenge point for cycle (from Stage 2)
- $j'$ = Sumcheck variable ranging over cycles $\{0,1\}^{\log T}$

- $\widetilde{\text{wa}}_{\text{rd}}(\vec{r}', j')$ = Write address polynomial evaluated at $(\vec{r}', j')$
  - Virtual polynomial (computed by verifier)
  - Equals 1 if register $\vec{r}'$ is written at cycle $j'$, 0 otherwise (in Boolean hypercube)

- $\widetilde{\text{inc}}(\vec{r}', j')$ = Increment polynomial evaluated at $(\vec{r}', j')$
  - **Committed polynomial** (`CommittedPolynomial::RdInc`)
  - Value written - value before write

- $\widetilde{\text{LT}}(j', \vec{r}_{\text{cycle}})$ = "Less-than" polynomial
  - Multilinear extension of the predicate: $j' < \vec{r}_{\text{cycle}}$
  - Ensures we only sum increments that happened **before** the challenge cycle
  - Formula (lines 200-207):
    ```rust
    let mut lt_eval = F::zero();
    let mut eq_term = F::one();
    for (x, y) in r.iter().zip(r_cycle.r.iter()) {
        lt_eval += (F::one() - x) * y * eq_term;
        eq_term *= F::one() - x - y + *x * y + *x * y;
    }
    ```

- $\text{val\_claim}$ = Input claim from Stage 2 register read/write checking

***

#### Mathematical Interpretation

**What does this equation prove?**

$$\widetilde{\text{val}}(\vec{r}', \vec{r}_{\text{cycle}}) = \sum_{j' < \vec{r}_{\text{cycle}}} \left[\text{increments to register } \vec{r}' \text{ at cycles } j' \right]$$

Since all registers start at 0 (RISC-V convention), the value in register $\vec{r}'$ at cycle $\vec{r}_{\text{cycle}}$ equals the sum of all increments written to it before that cycle.

**The LT polynomial ensures**:
- Only count writes that happened **before** cycle $\vec{r}_{\text{cycle}}$
- This matches the semantics: reads see the most recent write **before** them

***

#### Expected Output Claim (Final Check)

**Code** ([`val_evaluation.rs:209-219`](../jolt-core/src/zkvm/registers/val_evaluation.rs#L209-L219)):

```rust
let (_, inc_claim) = accumulator.borrow().get_committed_polynomial_opening(
    CommittedPolynomial::RdInc,
    SumcheckId::RegistersValEvaluation,
);
let (_, wa_claim) = accumulator.borrow().get_virtual_polynomial_opening(
    VirtualPolynomial::RdWa,
    SumcheckId::RegistersValEvaluation,
);

// Return inc_claim * wa_claim * lt_eval
inc_claim * wa_claim * lt_eval
```

**Breakdown**:

1. **Get committed polynomial evaluation**: `inc_claim` = $\widetilde{\text{inc}}(\vec{r}', \vec{r}_{j'})$
   - This is the claimed evaluation at the random point after sumcheck

2. **Get virtual polynomial evaluation**: `wa_claim` = $\widetilde{\text{wa}}_{\text{rd}}(\vec{r}', \vec{r}_{j'})$
   - Virtual polynomial, verifier can compute

3. **Compute LT polynomial**: `lt_eval` = $\widetilde{\text{LT}}(\vec{r}_{j'}, \vec{r}_{\text{cycle}})$
   - Verifier computes using closed-form formula (lines 200-207)

4. **Final computation**:
$$\text{expected\_output\_claim} = \widetilde{\text{inc}}(\vec{r}', \vec{r}_{j'}) \cdot \widetilde{\text{wa}}_{\text{rd}}(\vec{r}', \vec{r}_{j'}) \cdot \widetilde{\text{LT}}(\vec{r}_{j'}, \vec{r}_{\text{cycle}})$$

If this matches the claimed evaluation from sumcheck, the verifier accepts.

***

#### How This Completes Twist for Registers

**The three-stage Twist protocol**:

1. **Stage 2: Read/Write Checking**
   - Proved: Time-ordered fingerprint products are consistent
   - Claimed: $\widetilde{\text{val}}(\vec{r}', \vec{r}_{\text{cycle}})$ evaluates to some value

2. **Stage 3: Val Evaluation** (this sumcheck)
   - Proves: The claimed $\widetilde{\text{val}}$ evaluation is correct
   - Method: Shows it equals sum of all increments before $\vec{r}_{\text{cycle}}$
   - Uses committed polynomial $\widetilde{\text{inc}}$ (from execution trace)

3. **Result**: Register consistency is proven!
   - Stage 2 proved reads return correct values (via fingerprints)
   - Stage 3 proved those values match actual write increments
   - Combined: All register operations are consistent with the committed trace

***

#### Key Difference from RAM Val Evaluation

**Registers are simpler**:
- No address-ordered vs time-ordered comparison (K=64 is small)
- No chunking needed (d=1)
- No final state commitment (registers start/end at known values)
- LT polynomial handles temporal ordering directly

**RAM is more complex** (see Section 3.3):
- Must handle large address space with chunking
- Requires separate final state commitments
- Uses permutation arguments for address-ordered checking

##### 3.5: Instruction Lookups Continuation (Shout)

**Stage 3 contains two sumchecks** that continue the Shout protocol from Stage 2:

1. **Read RAF Sumcheck**: Proves lookup values are correct
2. **Hamming Weight Sumcheck**: Proves one-hot property (exactly one table entry accessed per cycle)

Together with Stage 2's Booleanity sumcheck, these complete the proof that instruction lookups are correct.

***

#### 3.5a: Hamming Weight Sumcheck

**Location**: [`jolt-core/src/zkvm/instruction_lookups/hamming_weight.rs`](../jolt-core/src/zkvm/instruction_lookups/hamming_weight.rs)

After proving $\widetilde{\text{ra}}_i$ values are Boolean in Stage 2, this sumcheck proves they're **one-hot**: exactly one entry per cycle.

***

##### The Complete Sumcheck Equation

**Equation being verified**:

$$\boxed{\sum_{j \in \{0,1\}^{\log T}} \sum_{i=0}^{D-1} \gamma^i \cdot \widetilde{\text{ra}}_i(K_{\text{chunk}}, j) = \sum_{i=0}^{D-1} \gamma^i}$$

where the left side sums over all table entries $k \in \{0,1\}^{\log K_{\text{chunk}}}$, and $K_{\text{chunk}} = 2^8 = 256$.

**Simplified form** (summing over $k$ collapses to Hamming weight):

$$\boxed{\sum_{j \in \{0,1\}^{\log T}} \sum_{i=0}^{D-1} \gamma^i \cdot \left(\sum_{k=0}^{K_{\text{chunk}}-1} \widetilde{\text{ra}}_i(k,j)\right) = D \cdot T \cdot \gamma \cdot \frac{1 - \gamma^{D}}{1 - \gamma}}$$

**But verifier checks a simpler form**: After sumcheck over cycles $j$, the verifier checks:

$$\boxed{\sum_{i=0}^{D-1} \gamma^i \cdot \widetilde{\text{ra}}_i(\vec{r}_k, \vec{r}_j) = \sum_{i=0}^{D-1} \gamma^i = \gamma \cdot \frac{1 - \gamma^{D}}{1 - \gamma}}$$

**Variables**:
- $D = 16$ = Number of chunks
- $K_{\text{chunk}} = 256$ = Sub-table size
- $T$ = Trace length

- $\widetilde{\text{ra}}_i(k, j)$ = Read address polynomial for chunk $i$
  - **Committed polynomial** (`CommittedPolynomial::InstructionRa(i)`)
  - One-hot property: $\sum_k \widetilde{\text{ra}}_i(k,j) = 1$ for each cycle $j$

- $\gamma$ = Batching coefficient (from transcript)

**What does this equation prove?**

For each cycle $j$ and each chunk $i$, exactly one table entry is accessed:
$$\sum_{k=0}^{255} \widetilde{\text{ra}}_i(k, j) = 1$$

This is the **Hamming weight** property - the sum of a one-hot vector equals 1.

***

##### Initial Claim

**Code** ([`hamming_weight.rs:88-90`](../jolt-core/src/zkvm/instruction_lookups/hamming_weight.rs#L88-L90)):

```rust
fn input_claim(&self) -> F {
    self.gamma.iter().sum()
}
```

**Mathematical interpretation**:
$$C = \sum_{i=0}^{D-1} \gamma^i$$

This is the **expected** sum if each $\widetilde{\text{ra}}_i$ is one-hot for all cycles.

***

##### Expected Output Claim (Final Check)

**Code** ([`hamming_weight.rs:123-144`](../jolt-core/src/zkvm/instruction_lookups/hamming_weight.rs#L123-L144)):

```rust
fn expected_output_claim(
    &self,
    accumulator: Option<Rc<RefCell<VerifierOpeningAccumulator<F>>>>,
    _r: &[F::Challenge],
) -> F {
    let ra_claims = (0..D).map(|i| {
        let accumulator = accumulator.as_ref().unwrap();
        let accumulator = accumulator.borrow();
        accumulator.get_committed_polynomial_opening(
            CommittedPolynomial::InstructionRa(i),
            SumcheckId::InstructionHammingWeight,
        ).1
    });

    self.gamma.iter()
        .zip(ra_claims)
        .map(|(gamma, ra)| ra * gamma)
        .sum()
}
```

**Breakdown**:

1. **Get $D=16$ claimed evaluations** (lines 128-137):
   Each $\text{ra\_claim}_i = \widetilde{\text{ra}}_i(\vec{r}_k, \vec{r}_j)$

2. **Compute batched sum** (lines 139-143):
   $$\sum_{i=0}^{D-1} \gamma^i \cdot \widetilde{\text{ra}}_i(\vec{r})$$

If this equals $\sum_{i=0}^{D-1} \gamma^i$, then each $\widetilde{\text{ra}}_i$ is one-hot.

***

#### 3.5b: Read RAF Sumcheck

**Location**: [`jolt-core/src/zkvm/instruction_lookups/read_raf_checking.rs`](../jolt-core/src/zkvm/instruction_lookups/read_raf_checking.rs)

This is the **core lookup verification** - proving that the values read from lookup tables are correct. Uses the **prefix-suffix sumcheck** algorithm for efficient verification of massive ($2^{128}$-entry) tables.

**Note**: RAF stands for "Read Address Fingerprint" - it's the Shout protocol's way of verifying reads from pre-committed tables.

***

##### High-Level Overview

The sumcheck proves:
$$\sum_j \text{LookupOutput}(j) = \sum_j \sum_i \text{TableValue}_i(\text{LookupIndex}_i(j))$$

In plain English: **The claimed lookup outputs match the table values at the accessed indices**.

**Key challenge**: Tables are size $2^{128}$, can't materialize them!

**Solution**:
- Tables have efficiently-evaluable MLEs (defined by formulas, not stored)
- Prefix-suffix decomposition splits 128-bit index into manageable pieces
- Eight phases of sumcheck, 16 rounds per phase (128 total)

***

##### The Complete Sumcheck Equation

**Two-phase structure**: The sumcheck has LOG_K rounds (128) followed by log(T) rounds.

**Phase 1: Address rounds** (first LOG_K = 128 rounds)
- Binds variables of the lookup index using prefix-suffix decomposition
- Efficiently evaluates table MLEs without materializing giant tables

**Phase 2: Cycle rounds** (last log(T) rounds)

**Equation being verified** ([`read_raf_checking.rs:338-340`](../jolt-core/src/zkvm/instruction_lookups/read_raf_checking.rs#L338-L340)):

$$\boxed{\sum_{j \in \{0,1\}^{\log T}} \widetilde{\text{ra}}(\vec{r}_{\text{addr}}, j) \cdot \widetilde{\text{val}}(\vec{r}_{\text{addr}}, j) \cdot \widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, j) = \text{rv\_claim} + \gamma \cdot \text{raf\_claim}}$$

where:

- $\vec{r}_{\text{addr}}$ = Random challenge point for lookup address (from first 128 rounds)
- $\vec{r}_{\text{cycle}}$ = Random challenge from Stage 1 Spartan
- $j$ = Sumcheck variable ranging over cycles $\{0,1\}^{\log T}$

**Variables**:

- $\widetilde{\text{ra}}(\vec{r}_{\text{addr}}, j)$ = Read address polynomial
  - Virtual polynomial accumulated during prefix-suffix decomposition
  - Indicates which table entries are accessed

- $\widetilde{\text{val}}(\vec{r}_{\text{addr}}, j)$ = Combined value polynomial
  - **Computed from table evaluations and operand polynomials**
  - Formula (lines 470-479):
    ```rust
    val = rv_val_claim  // table lookups weighted by flags

        + (1 - raf_flag) * (\gamma·left_operand + \gamma²·right_operand)
        + raf_flag * \gamma²·identity
    ```

  - Where `rv_val_claim = \Sigmaᵢ table_i(r_addr) · table_flag_i`

- $\widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, j)$ = Equality polynomial
  - Selects the random cycle from Stage 1

- $\text{rv\_claim}$ = Read value claim (from Stage 1 Spartan)
  - Virtual polynomial: `VirtualPolynomial::LookupOutput`

- $\text{raf\_claim}$ = Read address fingerprint claim
  - Computed as: `left_operand_claim + \gamma·right_operand_claim`
  - Virtual polynomials from Stage 1 Spartan

- $\gamma$ = Batching coefficient (from transcript)

***

##### Code Breakdown

**Input claim** ([`read_raf_checking.rs:305-307`](../jolt-core/src/zkvm/instruction_lookups/read_raf_checking.rs#L305-L307)):
```rust
fn input_claim(&self) -> F {
    self.rv_claim + self.gamma * self.raf_claim
}
```

Where:

- `rv_claim` comes from `VirtualPolynomial::LookupOutput` (Stage 1 Spartan, lines 103-106)
- `raf_claim = left_operand_claim + \gamma·right_operand_claim` (line 121)

**Prover message for cycle rounds** ([`read_raf_checking.rs:321-355`](../jolt-core/src/zkvm/instruction_lookups/read_raf_checking.rs#L321-L355)):
```rust
fn compute_prover_message(&mut self, round: usize, _previous_claim: F) -> Vec<F> {
    let ps = self.prover_state.as_mut().unwrap();
    if round < LOG_K {
        // Phase 1: First log(K) rounds (prefix-suffix decomposition)
        self.compute_prefix_suffix_prover_message(round).to_vec()
    } else {
        // Phase 2: Last log(T) rounds
        (0..ps.eq_r_cycle.len() / 2)
            .into_par_iter()
            .map(|i| {
                let eq_evals = ps.eq_r_cycle.sumcheck_evals_array::<DEGREE>(i, ...);
                let ra_evals = ps.ra.as_ref().unwrap().sumcheck_evals_array::<DEGREE>(i, ...);
                let val_evals = ps.combined_val_polynomial.as_ref().unwrap()
                    .sumcheck_evals_array::<DEGREE>(i, ...);

                // Compute eq * ra * val for each degree point
                std::array::from_fn::<F::Unreduced<9>, DEGREE, _>(|i| {
                    let eq_ra = eq_evals[i] * ra_evals[i];
                    eq_ra.mul_unreduced::<9>(val_evals[i])
                })
            })
            .reduce(...)
            .collect()
    }
}
```

**Expected output claim** ([`read_raf_checking.rs:415-481`](../jolt-core/src/zkvm/instruction_lookups/read_raf_checking.rs#L415-L481)):
```rust
fn expected_output_claim(&self, accumulator: Option<Rc<RefCell<VerifierOpeningAccumulator<F>>>>, r: &[F::Challenge]) -> F {
    let (r_address_prime, r_cycle_prime) = r.split_at(LOG_K);

    // Evaluate operand polynomials at r_address_prime (lines 421-425)
    let left_operand_eval = OperandPolynomial::new(LOG_K, OperandSide::Left).evaluate(r_address_prime);
    let right_operand_eval = OperandPolynomial::new(LOG_K, OperandSide::Right).evaluate(r_address_prime);
    let identity_poly_eval = IdentityPolynomial::new(LOG_K).evaluate(r_address_prime);

    // Evaluate all lookup table MLEs at r_address_prime (lines 426-428)
    let val_evals: Vec<_> = LookupTables::<XLEN>::iter()
        .map(|table| table.evaluate_mle::<F, F::Challenge>(r_address_prime))
        .collect();

    let accumulator = accumulator.as_ref().unwrap();

    // Get r_cycle from Stage 1 (lines 432-439)
    let r_cycle = accumulator.borrow()
        .get_virtual_polynomial_opening(VirtualPolynomial::LookupOutput, SumcheckId::SpartanOuter)
        .0.r;
    let eq_eval_cycle = EqPolynomial::mle(&r_cycle, r_cycle_prime);

    // Get ra_claim (lines 442-448)
    let ra_claim = accumulator.borrow()
        .get_virtual_polynomial_opening(VirtualPolynomial::InstructionRa, SumcheckId::InstructionReadRaf)
        .1;

    // Get table flag claims (lines 450-460)
    let table_flag_claims: Vec<F> = (0..LookupTables::<XLEN>::COUNT)
        .map(|i| {
            accumulator.borrow()
                .get_virtual_polynomial_opening(VirtualPolynomial::LookupTableFlag(i), SumcheckId::InstructionReadRaf)
                .1
        })
        .collect();

    // Get raf_flag_claim (lines 463-468)
    let raf_flag_claim = accumulator.borrow()
        .get_virtual_polynomial_opening(VirtualPolynomial::InstructionRafFlag, SumcheckId::InstructionReadRaf)
        .1;

    // Compute rv_val_claim: weighted sum of table values (lines 470-474)
    let rv_val_claim = val_evals.into_iter()
        .zip(table_flag_claims)
        .map(|(claim, val)| claim * val)
        .sum::<F>();

    // Compute combined val (lines 476-479)
    let val_eval = rv_val_claim

        + (F::one() - raf_flag_claim) * (self.gamma * left_operand_eval + self.gamma_squared * right_operand_eval)
        + raf_flag_claim * self.gamma_squared * identity_poly_eval;

    // Final output (line 480)
    eq_eval_cycle * ra_claim * val_eval
}
```

***

##### Mathematical Interpretation

**What does this equation prove?**

The sumcheck verifies that for the random challenge point $(\vec{r}_{\text{addr}}, \vec{r}_{\text{cycle}})$:

$$\text{rv}(\vec{r}_{\text{cycle}}) + \gamma \cdot \text{raf}(\vec{r}_{\text{cycle}}) = \sum_{j} \widetilde{\text{ra}}(\vec{r}_{\text{addr}}, j) \cdot \widetilde{\text{val}}(\vec{r}_{\text{addr}}, j) \cdot \widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, j)$$

After sumcheck completes, the verifier checks:

$$\widetilde{\text{ra}}(\vec{r}_{\text{addr}}, \vec{r}_{\text{cycle}}') \cdot \widetilde{\text{val}}(\vec{r}_{\text{addr}}, \vec{r}_{\text{cycle}}') \cdot \widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, \vec{r}_{\text{cycle}}')$$

where $\vec{r}_{\text{cycle}}'$ are the sumcheck challenges from the log(T) rounds.

**The combined val polynomial** encodes:
1. **Table lookups**: Which lookup table to use (via table flags)
2. **Operands**: Left and right operands for instructions
3. **Identity**: Pass-through for certain instructions

This allows a single sumcheck to verify all instruction lookup types efficiently.

***

##### Why This Completes Instruction Lookups

**Three-stage verification**:

1. **Stage 2: Booleanity** (Section 2.4)
   - Proved: $\widetilde{\text{ra}}_i(k,j) \in \{0,1\}$
   - All address indicators are Boolean

2. **Stage 3a: Hamming Weight** (this section)
   - Proved: $\sum_k \widetilde{\text{ra}}_i(k,j) = 1$
   - Exactly one entry accessed per cycle per chunk

3. **Stage 3b: Read RAF** (this section)
   - Proved: Lookup values match table at accessed indices
   - Outputs are correct

**Together**: Complete instruction execution verification via lookups.

**Stage 4 continuation** (Section 4.3): Final Ra Sumcheck completes the Shout permutation argument, proving the access pattern matches the execution trace.

***

##### Connection to Jolt's "Just One Lookup Table" Philosophy

**The key insight**: Instead of arithmetizing CPU instructions into circuits, Jolt:
1. Pre-computes entire CPU behavior into lookup tables
2. Decomposes giant tables into tiny sub-tables (256 entries)
3. Proves correct lookups via Shout (offline memory checking)
4. Cost: Linear in trace length $T$, **independent of table size $2^{128}$**

These Stage 3 sumchecks are where the "magic" happens - proving $2^{128}$-entry lookups without ever materializing the tables.

**Why multiple stages?**

Sumcheck instances form a **dependency graph**:

- Stage 2 sumchecks produce output claims
- Stage 3 sumchecks **use** those output claims as input claims
- Cannot batch sumchecks from different stages (would create circular dependency)

**Example dependency**:
- Stage 2 RAM sumcheck claims: "The fingerprint sum for reads equals the fingerprint sum for writes"
- Stage 3 RAM sumcheck verifies: "The final memory state (used in fingerprints) is consistent with initial state + all increments"

***

### Stage 4: Final Sumchecks

**Location**: [`jolt-core/src/zkvm/dag/jolt_dag.rs:494-520`](../jolt-core/src/zkvm/dag/jolt_dag.rs#L494-L520)

```rust
let stage4_instances: Vec<_> = std::iter::empty()
    .chain(ram_dag.stage4_verifier_instances(&mut state_manager))
    .chain(bytecode_dag.stage4_verifier_instances(&mut state_manager))
    .chain(lookups_dag.stage4_verifier_instances(&mut state_manager))
    .collect();

let _r_stage4 = BatchedSumcheck::verify(
    stage4_proof,
    stage4_instances_ref,
    Some(opening_accumulator.clone()),
    &mut *transcript.borrow_mut(),
).context("Stage 4")?;
```

**Stage 4 contains**:
- **RAM final sumchecks**: Booleanity and Hamming weight checks (Twist finalization)
- **Bytecode read checking**: Shout sumcheck proving execution trace reads correct bytecode
- **Instruction lookups final**: Complete instruction execution verification (Shout finalization)

#### Exact Mathematical Equations for Stage 4 Sumchecks

Stage 4 completes the Twist and Shout protocols with final verification sumchecks.

***

##### 4.1: RAM Hamming Weight Sumcheck (Twist)

**Location**: [`jolt-core/src/zkvm/ram/hamming_weight.rs`](../jolt-core/src/zkvm/ram/hamming_weight.rs)

**What this proves**: The sum of address chunk values equals the claimed Hamming weight from booleanity check.

***

###### The Sumcheck Equation

**Equation being verified** ([`hamming_weight.rs:218-223`](../jolt-core/src/zkvm/ram/hamming_weight.rs#L218-L223)):

$$\boxed{\sum_{k \in \{0,1\}^{\log(K^{1/d})}} \sum_{i=0}^{d-1} \gamma^i \cdot \widetilde{\text{ra}}^{(i)}(k) = \text{hamming\_booleanity\_claim} \cdot \sum_{i=0}^{d-1} \gamma^i}$$

**Variables**:
- $d$ = Chunking parameter (dynamically computed such that $K^{1/d} = 2^8 = 256$)
- $K$ = Memory size
- $\widetilde{\text{ra}}^{(i)}(k)$ = MLE of $i$-th address chunk
  - **Committed polynomial**: `CommittedPolynomial::RamRa(i)`
  - Accumulated with eq polynomial weighting from previous stage
- $\gamma$ = Batching coefficient (from transcript)
- $\text{hamming\_booleanity\_claim}$ = Claimed value from Stage 3 booleanity check

**Code Breakdown**:

**Input claim** ([`hamming_weight.rs:159-161`](../jolt-core/src/zkvm/ram/hamming_weight.rs#L159-L161)):
```rust
fn input_claim(&self) -> F {
    self.input_claim  // = hamming_booleanity_claim * gamma_powers.sum()
}
```

Where `input_claim` is computed during initialization (lines 112, 139):
```rust
let input_claim = hamming_booleanity_claim * gamma_powers.iter().sum::<F>();
```

**Expected output claim** ([`hamming_weight.rs:199-224`](../jolt-core/src/zkvm/ram/hamming_weight.rs#L199-L224)):
```rust
fn expected_output_claim(&self, accumulator: Option<Rc<RefCell<VerifierOpeningAccumulator<F>>>>, _r: &[F::Challenge]) -> F {
    // Get d evaluations of ra chunks
    let ra_claims: Vec<_> = (0..self.d)
        .map(|i| {
            accumulator.as_ref().unwrap().borrow()
                .get_committed_polynomial_opening(
                    CommittedPolynomial::RamRa(i),
                    SumcheckId::RamHammingWeight,
                )
                .1
        })
        .collect();

    // Compute batched claim: sum_{i=0}^{d-1} gamma^i * ra_i(r)
    ra_claims.iter()
        .zip(self.gamma_powers.iter())
        .map(|(ra_claim, gamma_power)| *ra_claim * gamma_power)
        .sum()
}
```

**Prover message** ([`hamming_weight.rs:163-187`](../jolt-core/src/zkvm/ram/hamming_weight.rs#L163-L187)):
```rust
fn compute_prover_message(&mut self, _round: usize, _previous_claim: F) -> Vec<F> {
    let ps = self.prover_state.as_ref().expect("Prover state not initialized");

    // Degree 1: linear sumcheck over batched ra polynomials
    let prover_msg = ps.ra.par_iter()
        .zip(self.gamma_powers.par_iter())
        .map(|(ra_poly, gamma_power)| {
            // For each chunk, compute weighted sum
            ra_poly.sumcheck_evals_linear::<1>(...).mul_unreduced(*gamma_power)
        })
        .reduce(F::Unreduced::zero, |running, new| running + new);

    vec![F::from_montgomery_reduce(prover_msg)]
}
```

***

###### Mathematical Interpretation

**What does this prove?**

This sumcheck verifies that the $d$ address chunks correctly sum to the expected Hamming weight. Combined with:

1. **Stage 3 Booleanity**: Proved each chunk value is Boolean (0 or 1)
2. **This sumcheck**: Proves the sum across chunks equals the claimed Hamming weight

Together: The address encoding is valid.

**Why chunking?** For large memory ($K = 2^{24}$ or more), Twist uses chunked representation where address is split into $d$ chunks of size $2^8 = 256$ each. This keeps polynomial sizes manageable while maintaining security.

***

##### 4.2: Bytecode Read Checking (Shout)

**Location**: [`jolt-core/src/zkvm/bytecode/read_raf_checking.rs`](../jolt-core/src/zkvm/bytecode/read_raf_checking.rs)

**What this proves**: The execution trace correctly reads bytecode values at the PC (program counter) for every cycle.

***

###### The Sumcheck Equation

**Two-phase structure**: LOG_K rounds (binding bytecode address) followed by log(T) rounds (binding cycles).

**Equation being verified** ([`read_raf_checking.rs:760-778`](../jolt-core/src/zkvm/bytecode/read_raf_checking.rs#L760-L778)):

$$\boxed{\sum_{j \in \{0,1\}^{\log T}} \prod_{i=0}^{d-1} \widetilde{\text{ra}}^{(i)}(\vec{r}_{\text{addr}}, j) \cdot \sum_{s=0}^{2} \gamma^s \cdot \left[\widetilde{\text{Val}}^{(s)}(\vec{r}_{\text{addr}}) + \text{raf}^{(s)}\right] \cdot \widetilde{\text{eq}}(\vec{r}_{\text{cycle}}^{(s)}, j) = \text{rv\_claim}}$$

where:

- $\vec{r}_{\text{addr}}$ = Random challenge for bytecode address (from first LOG_K rounds)
- $j$ = Sumcheck variable over cycles
- $s \in \{0, 1, 2\}$ = Three batched read-checking stages

**Variables**:

- $\widetilde{\text{ra}}^{(i)}(\vec{r}_{\text{addr}}, j)$ = Read address polynomial for chunk $i$
  - **Committed polynomial**: `CommittedPolynomial::BytecodeRa(i)`
  - Product over all $d$ chunks reconstructs full address

- $\widetilde{\text{Val}}^{(s)}(\vec{r}_{\text{addr}})$ = Value polynomial for stage $s$
  - **Three separate polynomials** (one per stage)
  - Stage 0: Values from Spartan outer sumcheck
  - Stage 1: Values from register read/write sumcheck
  - Stage 2: Values from register val sumcheck, PC sumcheck, instruction lookups

- $\text{raf}^{(s)}$ = Read address fingerprint for stage $s$
  - Stage 0: $\gamma^3 \cdot \text{int\_poly}(\vec{r}_{\text{addr}})$
  - Stage 1: 0 (no RAF)
  - Stage 2: $\gamma^2 \cdot \text{int\_poly}(\vec{r}_{\text{addr}})$
  - `int_poly` is identity polynomial (returns its input)

- $\widetilde{\text{eq}}(\vec{r}_{\text{cycle}}^{(s)}, j)$ = Equality polynomial
  - Different $\vec{r}_{\text{cycle}}$ for each stage (from their respective sumchecks)

- $\text{rv\_claim}$ = Combined read value claim
  - Computed as (lines 109-113):
    ```rust
    rv_claim_1 + \gamma·rv_claim_2 + \gamma²·rv_claim_3 + \gamma³·raf_claim_1 + \gamma⁴·raf_claim_3
    ```

- $\gamma$ = Batching coefficient (from transcript)
- $d$ = Chunking parameter for bytecode addressing

***

###### Code Breakdown

**Input claim** ([`read_raf_checking.rs:552-554`](../jolt-core/src/zkvm/bytecode/read_raf_checking.rs#L552-L554)):
```rust
fn input_claim(&self) -> F {
    self.rv_claim
}
```

Where `rv_claim` is computed during initialization (lines 109-113):
```rust
let rv_claim = rv_claim_1

    + gamma * rv_claim_2
    + gamma_sqr * rv_claim_3
    + gamma_cub * raf_claim
    + gamma_four * raf_shift_claim;
```

**Expected output claim** ([`read_raf_checking.rs:725-778`](../jolt-core/src/zkvm/bytecode/read_raf_checking.rs#L725-L778)):
```rust
fn expected_output_claim(&self, accumulator: Option<Rc<RefCell<VerifierOpeningAccumulator<F>>>>, r: &[F::Challenge]) -> F {
    let accumulator = accumulator.as_ref().unwrap();
    let (r_address_prime, r_cycle_prime) = r.split_at(self.log_K);

    // r_cycle is bound LowToHigh, so reverse
    let r_cycle_prime = r_cycle_prime.iter().rev().copied().collect::<Vec<_>>();

    // Evaluate identity polynomial at r_address_prime (line 739)
    let int_poly = self.int_poly.evaluate(r_address_prime);

    // Get d ra chunk evaluations (lines 741-749)
    let ra_claims = (0..self.d).map(|i| {
        accumulator.borrow()
            .get_committed_polynomial_opening(
                CommittedPolynomial::BytecodeRa(i),
                SumcheckId::BytecodeReadRaf,
            )
            .1
    });

    // Get r_cycle for each of the 3 stages (line 750)
    let r_cycles = Self::get_r_cycle_verif(accumulator);

    // Compute combined val with RAF (lines 760-775)
    // Stage 0: gamma^0 * (Val_0 + gamma^3 * Int)
    // Stage 1: gamma^1 * (Val_1)
    // Stage 2: gamma^2 * (Val_2 + gamma^2 * Int)
    let val = self.val_polys.iter()
        .zip(r_cycles.iter())
        .zip(self.gamma.iter())
        .zip([
            int_poly * self.gamma_cub,  // RAF for Stage 0
            F::zero(),                   // No RAF for Stage 1
            int_poly * self.gamma_sqr,   // RAF for Stage 2
        ])
        .map(|(((val, r_cycle), gamma), int_poly)| {
            (val.evaluate(r_address_prime) + int_poly)

                * EqPolynomial::<F>::mle(r_cycle, &r_cycle_prime)
                * gamma
        })
        .sum::<F>();

    // Multiply by product of all ra chunks (line 777)
    ra_claims.fold(val, |running, ra_claim| running * ra_claim)
}
```

***

###### Mathematical Interpretation

**What does this prove?**

This is Shout's "read-checking" sumcheck applied to bytecode. It verifies:

1. **Every cycle reads bytecode**: For each cycle $j$, the trace accesses bytecode at PC $j$
2. **Reads match committed bytecode**: The read values match the bytecode values in preprocessing
3. **Three-stage batching**: Combines multiple read-checking sumchecks into one for efficiency

**Critical insight**: The verifier has the **full bytecode** in preprocessing (via `val_polys`). The prover must prove their execution trace reads match this committed bytecode.

**Chunked addressing**: Like RAM, bytecode uses chunked addressing with $d$ chunks. The product $\prod_{i=0}^{d-1} \widetilde{\text{ra}}^{(i)}$ reconstructs the full bytecode address.

**Three stages correspond to**:
- **Stage 0** (from Spartan): Opcode, register addresses, flags
- **Stage 1** (from Registers): Register values being read/written
- **Stage 2** (from various): PC values, instruction lookup indices

All are batched together into a single sumcheck for efficiency.

##### 4.3: Instruction Lookups Ra Virtualization (Shout)

**Location**: [`jolt-core/src/zkvm/instruction_lookups/ra_virtual.rs`](../jolt-core/src/zkvm/instruction_lookups/ra_virtual.rs)

**What this proves**: The virtual "ra" polynomial (read address accumulated during Stage 3 Read RAF) correctly equals the product of all chunk polynomials.

***

###### The Sumcheck Equation

**Equation being verified** ([`ra_virtual.rs:154-169`](../jolt-core/src/zkvm/instruction_lookups/ra_virtual.rs#L154-L169)):

$$\boxed{\sum_{j \in \{0,1\}^{\log T}} \left[\prod_{i=0}^{D-1} \widetilde{\text{ra}}^{(i)}(\vec{r}_{\text{addr}}, j)\right] \cdot \widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, j) = \text{ra\_claim}}$$

where:

- $\vec{r}_{\text{addr}}$ = Random challenge for lookup address (from Stage 3 Read RAF, length LOG_K = 128)
- $\vec{r}_{\text{cycle}}$ = Random challenge for cycle (from Stage 3 Read RAF)
- $j$ = Sumcheck variable over cycles $\{0,1\}^{\log T}$

**Variables**:

- $\widetilde{\text{ra}}^{(i)}(\vec{r}_{\text{addr}}, j)$ = Read address polynomial for chunk $i$
  - **Committed polynomial**: `CommittedPolynomial::InstructionRa(i)`
  - $D = 16$ chunks (for 128-bit lookup indices split into 16 \times  8-bit chunks)
  - Each chunk is a one-hot encoded vector over $K_{\text{chunk}} = 256$ entries

- $\widetilde{\text{eq}}(\vec{r}_{\text{cycle}}, j)$ = Equality polynomial
  - Selects the random cycle point from Stage 3

- $\text{ra\_claim}$ = Virtual polynomial claim from Stage 3 Read RAF
  - Input: `VirtualPolynomial::InstructionRa` from `SumcheckId::InstructionReadRaf`

***

###### Code Breakdown

**Input claim** ([`ra_virtual.rs:114-116`](../jolt-core/src/zkvm/instruction_lookups/ra_virtual.rs#L114-L116)):
```rust
fn input_claim(&self) -> F {
    self.input_claim  // = ra_claim from Stage 3 Read RAF
}
```

Where `input_claim` is obtained during initialization (lines 50-53):
```rust
let (r, ra_claim) = state_manager.get_virtual_polynomial_opening(
    VirtualPolynomial::InstructionRa,
    SumcheckId::InstructionReadRaf,
);
```

**Prover state initialization** ([`ra_virtual.rs:57-75`](../jolt-core/src/zkvm/instruction_lookups/ra_virtual.rs#L57-L75)):
```rust
// Extract lookup indices from trace and split into D chunks
let H_indices: [Vec<Option<u8>>; D] = std::array::from_fn(|i| {
    trace.par_iter()
        .map(|cycle| {
            let lookup_index = LookupQuery::<XLEN>::to_lookup_index(cycle);
            // Extract i-th chunk (8 bits)
            Some(((lookup_index >> (LOG_K_CHUNK * (D - 1 - i))) % K_CHUNK as u128) as u8)
        })
        .collect()
});

// Create RaPolynomial for each chunk with eq weighting
let ra_i_polys = H_indices.into_par_iter()
    .enumerate()
    .map(|(i, lookup_indices)| {
        let r = &r_address[LOG_K_CHUNK * i..LOG_K_CHUNK * (i + 1)];
        let eq_evals = EqPolynomial::evals(r);
        RaPolynomial::new(Arc::new(lookup_indices), eq_evals)
    })
    .collect();
```

**Expected output claim** ([`ra_virtual.rs:149-170`](../jolt-core/src/zkvm/instruction_lookups/ra_virtual.rs#L149-L170)):
```rust
fn expected_output_claim(&self, opening_accumulator: Option<Rc<RefCell<VerifierOpeningAccumulator<F>>>>, r: &[F::Challenge]) -> F {
    // Evaluate eq at (r_cycle, r) where r are the sumcheck challenges
    let eq_eval = EqPolynomial::<F>::mle(&self.r_cycle, r);

    // Get evaluations of all D ra chunks and compute product
    let ra_claim_prod: F = (0..D)
        .map(|i| {
            let (_, ra_i_claim) = opening_accumulator.as_ref().unwrap().borrow()
                .get_committed_polynomial_opening(
                    CommittedPolynomial::InstructionRa(i),
                    SumcheckId::InstructionRaVirtualization,
                );
            ra_i_claim
        })
        .product();  // Product of all chunk evaluations

    // Return eq_eval * (ra_0 * ra_1 * ... * ra_{D-1})
    eq_eval * ra_claim_prod
}
```

**Prover message** ([`ra_virtual.rs:118-132`](../jolt-core/src/zkvm/instruction_lookups/ra_virtual.rs#L118-L132)):
```rust
fn compute_prover_message(&mut self, _round: usize, previous_claim: F) -> Vec<F> {
    let prover_state = self.prover_state.as_ref().unwrap();
    let ra_i_polys = &prover_state.ra_i_polys;
    let r_cycle = &self.r_cycle;
    let r_sumcheck = &prover_state.r_sumcheck;

    // Compute product of all D ra polynomials with eq weighting
    let poly = compute_mles_product_sum(ra_i_polys, previous_claim, r_cycle, r_sumcheck);

    // Evaluate at degree points: 0, 2, 3, ..., D+1
    // Degree = D+1 (product of D polynomials \times  eq polynomial)
    let degree = D + 1;  // = 17 for D=16
    let domain = chain!([0], 2..).map(F::from_u64).take(degree);
    domain.map(|x| poly.evaluate::<F>(&x)).collect()
}
```

***

###### Mathematical Interpretation

**What does this prove?**

This sumcheck completes the Shout protocol for instruction lookups by proving:

$$\widetilde{\text{ra}}(\vec{r}_{\text{addr}}, \vec{r}_{\text{cycle}}) = \prod_{i=0}^{D-1} \widetilde{\text{ra}}^{(i)}(\vec{r}_{\text{addr}}, \vec{r}_{\text{cycle}})$$

**Why is this necessary?**

In Stage 3, the Read RAF sumcheck used a **virtual polynomial** `ra` that was accumulated during the prefix-suffix decomposition. This sumcheck proves that the virtual `ra` correctly equals the product of all committed chunk polynomials `ra^(i)`.

**The complete instruction lookup verification chain**:

1. **Stage 2: Booleanity** \rightarrow  Proved each `ra_i(k,j) \in  {0,1}`
2. **Stage 3a: Hamming Weight** \rightarrow  Proved `sum_k ra_i(k,j) = 1` (one-hot)
3. **Stage 3b: Read RAF** \rightarrow  Proved lookup values match table (used virtual `ra`)
4. **Stage 4 (this)** \rightarrow  Proved virtual `ra = product of committed ra^(i)`

**Together**: Complete verification that instruction lookups are correct.

**Degree note**: This is a high-degree sumcheck (degree D+1 = 17) because it's verifying a product of 16 polynomials. The prover must evaluate at 17 points per round.

**Why product?** The full 128-bit lookup address is reconstructed as:
$$\text{address} = \sum_{i=0}^{D-1} 2^{8i} \cdot \text{chunk}_i$$

In the MLE world, this becomes a product relationship that this sumcheck verifies.

***

**Summary of Stage 4**: After completing these sumchecks, all "virtual polynomials" (polynomials proven via sumcheck chains) have been verified. Stage 5 will verify all "committed polynomials" (proven via opening proofs).

***

## Final Opening Verification (Stage 5)

### Why We Need Opening Proofs

Throughout Stages 1-4, sumcheck protocols reduced massive summations to **evaluation claims**:

- "Polynomial $P_1$ evaluates to $v_1$ at point $r_1$"
- "Polynomial $P_2$ evaluates to $v_2$ at point $r_2$"
- ... (~50 such claims for committed polynomials)

These polynomials are **committed** (verifier has commitments $C_{P_1}, C_{P_2}, \ldots$ from the proof).

**The verifier needs to check**: Do these committed polynomials actually evaluate to the claimed values?

### Exact Mathematical Equation for Stage 5

**Location**: [`jolt-core/src/poly/opening_proof.rs:1262-1400`](../jolt-core/src/poly/opening_proof.rs#L1262-L1400)

Stage 5 verifies the equation:

$$\boxed{\sum_{X \in \{0,1\}^{\text{max\_log\_n}}} \sum_{i=1}^{n} \gamma_i \cdot \widetilde{\text{eq}}(r_i, X) \cdot \widetilde{P_i}(X) = \sum_{i=1}^{n} \gamma_i \cdot v_i}$$

**Variables**:
- $n \approx 50$ = number of committed polynomials (varies by program)
- $\widetilde{P_i}(X)$ = MLE of $i$-th committed polynomial (**prover knows, verifier does NOT**)
  - Examples: $\widetilde{\text{LeftInstructionInput}}$, $\widetilde{\text{RightInstructionInput}}$, $\widetilde{\text{RdInc}}$, etc.
- $r_i \in \mathbb{F}^{\text{log}(T)}$ = opening point for polynomial $P_i$ (from sumcheck challenges)
- $v_i$ = claimed evaluation: $\widetilde{P_i}(r_i) = v_i$ (**prover claims, verifier checks**)
- $\gamma_i$ = random batching coefficients from Fiat-Shamir (**both know**)
- $\widetilde{\text{eq}}(r_i, X)$ = multilinear extension of equality function (**verifier can compute**)
  - Picks out the value at point $r_i$: $\widetilde{\text{eq}}(r_i, X) = 1$ when $X = r_i$, else 0
- $\text{max\_log\_n}$ = maximum polynomial size (determines sumcheck rounds)

**Why this equation**:
- **Left side**: Evaluates all polynomials at their claimed points, weighted by random $\gamma_i$
- **Right side**: The claimed sum (what prover asserts the left side equals)
- **Soundness**: If prover lies about even one evaluation $v_i$, the equation fails with high probability (Schwartz-Zippel)

**The key trick**: The $\widetilde{\text{eq}}(r_i, X)$ function "extracts" the value $\widetilde{P_i}(r_i)$ from the polynomial:
$$\sum_{X \in \{0,1\}^{\text{log n}}} \widetilde{\text{eq}}(r_i, X) \cdot \widetilde{P_i}(X) = \widetilde{P_i}(r_i)$$

**IMPORTANT - What the verifier actually receives**:

The verifier does **NOT** receive $\widetilde{P_i}(X)$ directly! That would defeat the purpose of the proof. Instead:

1. **Verifier receives** (from prior stages 1-4):
   - Commitments $C_{P_i} \in \mathbb{G}_T$ to each polynomial (from Stage 0)
   - Claimed evaluations $v_i$ (appended to transcript after each sumcheck)
   - Opening points $r_i$ (the random challenges from each sumcheck)

2. **During Stage 5 sumcheck** (proving the equation above):
   - **Prover sends**: Univariate round polynomials $g_j(X_j)$ for $j = 1, \ldots, \text{max\_log\_n}$
   - **Verifier checks**: $g_j(0) + g_j(1) \stackrel{?}{=} C_{j-1}$ (sumcheck consistency)
   - **Verifier samples**: Random challenge $r^*_j$ and computes $C_j = g_j(r^*_j)$
   - After $\text{max\_log\_n}$ rounds: Reduced to single claim $Q(r^*) = v^*$

3. **After Stage 5 sumcheck** (Stage 5b - Dory opening):
   - **Verifier needs to verify**: $Q(r^*) = v^*$ where $Q(X) = \sum_i \gamma_i \cdot \widetilde{\text{eq}}(r_i, X) \cdot \widetilde{P_i}(X)$
   - **Prover sends**: Dory opening proof (cross-term commitments, base case sigma protocol)
   - **Verifier checks**: Dory verification equation using the commitments $C_{P_i}$

The sumcheck protocol allows the verifier to be convinced about the sum $\sum_X Q(X)$ without ever seeing the polynomials $\widetilde{P_i}(X)$ themselves!

### Batched Opening Verification (Two-Stage Process)

**Naive approach**: Verify each opening individually
- 50 polynomials \rightarrow  50 Dory opening proofs \rightarrow  large proof size

**Jolt's approach**: Two-stage batching!

#### Stage 5a: Sumcheck-Based Batching

**Mathematical technique** (from Nova/Spartan):

For $n$ polynomial openings $(P_i, r_i, v_i)$ with commitments $C_{P_i}$:

1. **Sample batching coefficients**: $\gamma_1, \ldots, \gamma_n$ from Fiat-Shamir (both prover and verifier compute these identically)

2. **Define combined polynomial** (**prover only - verifier doesn't compute this directly**):
   $$Q(X) = \sum_{i=1}^{n} \gamma_i \cdot \widetilde{\text{eq}}(r_i, X) \cdot \widetilde{P_i}(X)$$

   The prover uses this to compute the univariate round polynomials $g_j(X_j)$ that get sent to the verifier.

3. **Prove via sumcheck** (the equation shown above):
   $$\sum_{X \in \{0,1\}^{\text{max\_log\_n}}} Q(X) = \sum_{i=1}^{n} \gamma_i \cdot v_i$$

   - **Prover**: Computes univariate round polynomials $g_1(X_1), g_2(X_2), \ldots$ and sends them
   - **Verifier**: Receives these univariate polynomials and checks consistency via sumcheck protocol
   - **Verifier never needs to know $Q(X)$ explicitly** - only the round polynomials matter!

4. **After sumcheck**: Reduced to single opening claim
   - Need to prove: $Q(r^*) = v^*$ at random challenge point $r^*$
   - This is ONE polynomial opening (combined $Q$), not $n$ separate openings
   - But verifier still doesn't need to compute $Q$ explicitly - the sumcheck already verified it!

#### Stage 5b: Dory PCS Opening

Now verify the single opening $Q(r^*) = v^*$ using Dory commitment scheme.

**But wait - how does the verifier check this without knowing $Q(X)$?**

The verifier needs a commitment to $Q$ to use with Dory. The key insight:

$$C_Q = \text{Com}(Q) = \text{Com}\left(\sum_i \gamma_i \cdot \widetilde{\text{eq}}(r_i, X) \cdot \widetilde{P_i}(X)\right)$$

**The verifier can compute $C_Q$ homomorphically** from the commitments $C_{P_i}$ they already have:

$$C_Q = \prod_{i=1}^{n} \left(C_{P_i}\right)^{\lambda_i}$$

where $\lambda_i$ are coefficients derived from $\gamma_i$ and $\widetilde{\text{eq}}(r_i, r^*)$ evaluated at the final sumcheck challenge point $r^*$.

This is possible because:

- Dory commitments support homomorphic operations (they're in $\mathbb{G}_T$)
- The verifier knows all $C_{P_i}$ (from Stage 0)
- The verifier can compute $\widetilde{\text{eq}}(r_i, r^*)$ (equality function is public)
- The batching coefficients $\gamma_i$ are public (from Fiat-Shamir)

**Why this works**:
- **Sumcheck**: Reduces $n$ openings to 1 opening (via random linear combination)
- **Homomorphic combination**: Verifier computes commitment to combined polynomial without knowing the polynomial
- **Dory**: Verifies the 1 opening cryptographically using the combined commitment
- **Soundness**: Random batching preserves correctness (Schwartz-Zippel)

### Verification Code

**Location**: [`jolt-core/src/zkvm/dag/jolt_dag.rs:522-568`](../jolt-core/src/zkvm/dag/jolt_dag.rs#L522-L568)

```rust
// Verify trusted_advice opening proofs
if state_manager.trusted_advice_commitment.is_some() {
    Self::verify_trusted_advice_proofs(
        &state_manager,
        &preprocessing.generators,
        &mut *transcript.borrow_mut(),
    ).context("Stage 5")?;
}

// Verify untrusted_advice opening proofs
if state_manager.untrusted_advice_commitment.is_some() {
    Self::verify_untrusted_advice_proofs(
        &state_manager,
        &preprocessing.generators,
        &mut *transcript.borrow_mut(),
    ).context("Stage 5")?;
}

// Batch-verify all openings
let batched_opening_proof = /* extract from proof */;

let mut commitments_map = HashMap::new();
for polynomial in AllCommittedPolynomials::iter() {
    commitments_map.insert(

        *polynomial,
        commitments.borrow()[polynomial.to_index()].clone(),
    );
}

let accumulator = state_manager.get_verifier_accumulator();
accumulator.borrow_mut().reduce_and_verify(
    &preprocessing.generators,
    &mut commitments_map,
    batched_opening_proof,
    &mut *transcript.borrow_mut(),
).context("Stage 5")?;
```

### `reduce_and_verify` Deep Dive

**Location**: [`jolt-core/src/poly/opening_proof.rs:1262-1400`](../jolt-core/src/poly/opening_proof.rs#L1262-L1400)

```rust
pub fn reduce_and_verify<ProofTranscript, PCS>(
    &mut self,
    pcs_setup: &PCS::VerifierSetup,
    commitment_map: &mut HashMap<CommittedPolynomial, PCS::Commitment>,
    reduced_opening_proof: &ReducedOpeningProof<F, PCS, ProofTranscript>,
    transcript: &mut ProofTranscript,
) -> Result<(), ProofVerifyError>
```

**Step 1: Sample batching coefficients**

```rust
let total_challenges_needed: usize = self.sumchecks.iter()
    .map(|sumcheck| {
        if sumcheck.polynomials.len() > 1 {
            sumcheck.polynomials.len()
        } else {
            1
        }
    })
    .sum();

let all_gammas: Vec<F> = transcript.challenge_vector(total_challenges_needed);
```

For each sumcheck instance (each represents a group of polynomials opened at the same point), we sample random coefficients.

**Step 2: Prepare sumcheck instances**

```rust
self.sumchecks.par_iter_mut()
    .zip(gamma_offsets.par_iter())
    .for_each(|(sumcheck, &offset)| {
        let num_gammas = if sumcheck.polynomials.len() > 1 {
            sumcheck.polynomials.len()
        } else {
            1
        };
        let gammas_slice = &all_gammas[offset..offset + num_gammas];
        sumcheck.prepare_sumcheck(None, gammas_slice);
    });
```

Each `OpeningProofReductionSumcheck` computes its contribution to the combined polynomial:
$$\text{contribution}_i = \sum_{j} \gamma_{i,j} \cdot \text{eq}(r_{i,j}, X) \cdot P_{i,j}(X)$$

**Step 3: Verify the reduction sumcheck**

```rust
self.sumchecks.iter_mut()
    .zip(reduced_opening_proof.sumcheck_claims.iter())
    .for_each(|(opening, claim)| opening.sumcheck_claim = Some(*claim));

let r_sumcheck = self.verify_batch_opening_reduction(
    &reduced_opening_proof.sumcheck_proof,
    transcript
)?;
```

This verifies the sumcheck for:
$$\sum_{X \in \{0,1\}^{\log n}} \left[ \sum_{i,j} \gamma_{i,j} \cdot \text{eq}(r_{i,j}, X) \cdot P_{i,j}(X) \right] = \text{claimed\_sum}$$

**Step 4: Combine commitments**

```rust
transcript.append_scalars(&reduced_opening_proof.sumcheck_claims);

let gamma: F = transcript.challenge_scalar();

let (batched_commitment, batched_evaluation) = self.sumchecks.iter().fold(
    (PCS::Commitment::default(), F::zero()),

    |(mut comm_acc, mut eval_acc), opening| {
        // Combine commitments: C_combined = \gamma_1C_1 + \gamma_2C_2 + ...
        comm_acc = comm_acc + commitment_map[&opening.polynomials[0]].scale(gamma.pow(i));
        // Combine evaluations: v_combined = \gamma_1v_1 + \gamma_2v_2 + ...
        eval_acc += opening.claims[0] * gamma.pow(i);
        (comm_acc, eval_acc)
    },
);
```

**Step 5: Verify single opening with PCS**

```rust
PCS::verify(
    &reduced_opening_proof.opening_proof,
    pcs_setup,
    transcript,
    &r_sumcheck,
    &batched_evaluation,
    &batched_commitment,
)?;
```

This is the **only cryptographic opening verification** in the entire Jolt verifier. Everything else is sumchecks (pure algebraic checks).

***

### How Dory Verification Actually Works

**Location**: External `dory` crate, called from [`jolt-core/src/poly/commitment/dory.rs:1213`](../jolt-core/src/poly/commitment/dory.rs#L1213)

From [Theory/Dory.md](../Theory/Dory.md#L311-L313):

> A crucial performance optimization is what makes the Dory verifier logarithmic. The verifier **does not need to compute the new claim at each step**. Instead, the verifier can simply accumulate all prover messages and challenges across all $\log n$ rounds.
>
> At the very end of the protocol, the verifier computes a single formula that combines the initial claim with all $O(\log n)$ messages and challenges. This reduces the verifier's main workload to a **single, large multi-exponentiation in $\mathbb{G}_T$ of size $O(\log n)$**, plus a constant number of pairings from the final `Scalar-Product` check.

**Key insight**: Dory is NOT a simple pairing check (like KZG). It's a **recursive inner-product argument**.

#### Dory Verification Steps (Simplified)

**Input to verifier**:
- Commitment $C$ (a $\mathbb{G}_T$ element from preprocessing)
- Opening point $r = (r_1, \ldots, r_n)$ where $n = \log N$ (from sumcheck)
- Claimed evaluation $v$ (the polynomial's value at $r$)
- Opening proof $\pi$ containing $\log N$ rounds of cross-term commitments

**Verification process**:

1. **Accumulate proof messages**:
   - Proof contains $\log N$ rounds of cross-term commitments: $(C_{+,1}, C_{-,1}), \ldots, (C_{+,\log N}, C_{-,\log N})$
   - Each $C_{+,i}, C_{-,i}$ is a $\mathbb{G}_T$ element

2. **Sample challenges** (from Fiat-Shamir):
   - For each round $i$, sample $\alpha_i \in \mathbb{F}$

3. **Compute combined claim** (using homomorphic properties of $\mathbb{G}_T$):
   $$C' = C \cdot \prod_{i=1}^{\log N} \left(C_{+,i}^{\alpha_i} \cdot C_{-,i}^{\alpha_i^{-1}} \cdot \chi_i^{\beta_i}\right)$$
   where:

   - $C$ is the original commitment
   - $C_{+,i}, C_{-,i}$ are cross-term commitments from the proof
   - $\chi_i$ are public precomputed values from the SRS
   - $\alpha_i, \beta_i$ are Fiat-Shamir challenges (random field elements)

4. **Base case verification**:
   - After $\log N$ rounds of folding, verify the base case (size-1 inner product)
   - This uses a constant number of pairings (typically 3-4)
   - Checks that the final folded commitment matches the claimed evaluation

**Mathematical operation**: Multi-exponentiation in $\mathbb{G}_T$

The core computation is:
$$C' = C \cdot \prod_{i=1}^{\log N} \left(C_{+,i}^{\alpha_i} \cdot C_{-,i}^{\alpha_i^{-1}} \cdot \chi_i^{\beta_i}\right)$$

where:

- $C$ is the original commitment (1 $\mathbb{G}_T$ element)
- For each round $i = 1$ to $\log N$:
  - $C_{+,i}, C_{-,i}$ are cross-term commitments from proof (2 $\mathbb{G}_T$ elements per round)
  - $\chi_i$ are SRS elements (public, precomputed)
  - $\alpha_i, \beta_i$ are Fiat-Shamir challenges (field elements)

This is computed efficiently using:

- **Batch exponentiation algorithms** (like Pippenger's algorithm)
- Operations in $\mathbb{G}_T$ (the target group of the pairing)
- **NOT** repeated pairing computations

**Complexity**:
- Multi-exponentiation: $O(\log N \cdot \log \log N)$ group operations in $\mathbb{G}_T$
- Pairings: $O(1)$ (constant number for base case)
- **Total**: $O(\log N)$ (dominated by multi-exponentiation)

#### Concrete Example

**Setup**:
- Polynomial $P$ with 16 coefficients ($N = 16$, so $\log N = 4$)
- Opening point $r = (r_1, r_2, r_3, r_4)$
- Claimed evaluation $v = P(r)$

**Proof structure**:

Opening proof $\pi$ contains $\log N = 4$ rounds:
$$\pi = \{(C_{+,1}, C_{-,1}), (C_{+,2}, C_{-,2}), (C_{+,3}, C_{-,3}), (C_{+,4}, C_{-,4})\}$$

where each $C_{\pm,i} \in \mathbb{G}_T$ is approximately 192 bytes for BLS12-381 curve.

**Total proof size**: $2 \times 4 \times 192 \text{ bytes} \approx 1.5 \text{ KB}$

**Verification**:

**Step 1**: Sample challenges from Fiat-Shamir transcript:
$$\alpha_1, \alpha_2, \alpha_3, \alpha_4 \leftarrow \text{Hash}(\text{transcript})$$

**Step 2**: Compute combined commitment via multi-exponentiation in $\mathbb{G}_T$:
$$C' = C \cdot \prod_{i=1}^{4} \left(C_{+,i}^{\alpha_i} \cdot C_{-,i}^{\alpha_i^{-1}} \cdot \chi_i^{\beta_i}\right)$$

Expanded form:
$$C' = C \cdot C_{+,1}^{\alpha_1} \cdot C_{-,1}^{\alpha_1^{-1}} \cdot \chi_1^{\beta_1} \cdot C_{+,2}^{\alpha_2} \cdot C_{-,2}^{\alpha_2^{-1}} \cdot \chi_2^{\beta_2}$$
$$\cdot C_{+,3}^{\alpha_3} \cdot C_{-,3}^{\alpha_3^{-1}} \cdot \chi_3^{\beta_3} \cdot C_{+,4}^{\alpha_4} \cdot C_{-,4}^{\alpha_4^{-1}} \cdot \chi_4^{\beta_4}$$

This is $1 + 4 \times 3 = 13$ exponentiations in $\mathbb{G}_T$, computed efficiently via batch exponentiation.

**Step 3**: Base case check using $O(1)$ pairings:
$$e(G_1, H_2)^v \stackrel{?}{=} C' \cdot (\text{additional terms})$$

This uses approximately 3-4 pairing operations.

***

#### Code Example: Dory Verification in Jolt

Let's see how this actually works in the Jolt codebase. The main verification function is in [jolt-core/src/poly/commitment/dory.rs:1212-1253](jolt-core/src/poly/commitment/dory.rs):

```rust
fn verify<ProofTranscript: Transcript>(
    proof: &Self::Proof,
    setup: &Self::VerifierSetup,
    transcript: &mut ProofTranscript,
    opening_point: &[<Self::Field as JoltField>::Challenge],
    opening: &Self::Field,
    commitment: &Self::Commitment,
) -> Result<(), ProofVerifyError> {
    // Dory uses the opposite endian-ness as Jolt
    let opening_point_dory: Vec<JoltFieldWrapper<Self::Field>> = opening_point
        .iter()
        .rev()  // Reverse the opening point coordinates
        .map(|&p| JoltFieldWrapper(p.into()))
        .collect();

    let claimed_opening = JoltFieldWrapper(*opening);
    let dory_transcript = JoltToDoryTranscriptRef::<Self::Field, _>::new(transcript);

    // Extract the proof components (C_{+,i}, C_{-,i} for each round)
    let verifier_builder =
        DoryProofBuilder::from_proof_no_transcript(proof.dory_proof_data.clone());

    // Call the external Dory library's verification function
    let verify_result = verify::<
        JoltBn254,
        JoltToDoryTranscriptRef<'_, Self::Field, ProofTranscript>,
        JoltMsmG1,
        JoltMsmG2,
        JoltMSM,
    >(
        commitment.0.clone(),      // C (the original commitment in 𝔾ₜ)
        claimed_opening,            // v (the claimed evaluation)
        &opening_point_dory,        // r (the opening point)
        verifier_builder,           // \pi (the proof with C_{+,i}, C_{-,i})
        proof.sigma,                // Additional proof components
        setup,                      // Verifier setup (contains \chi values, generators)
        dory_transcript,
    );

    match verify_result {
        Ok(()) => Ok(()),
        Err(e) => Err(ProofVerifyError::DoryError(format!("{e:?}"))),
    }
}
```

**What this code does**:
1. **Converts opening point**: Dory uses opposite endianness, so reverse coordinates
2. **Wraps types**: Convert Jolt types to Dory-compatible wrappers
3. **Extracts proof**: `DoryProofBuilder` contains all the $(C_{+,i}, C_{-,i})$ pairs
4. **Calls Dory verify**: Delegates to external `dory` crate (does the heavy lifting)

***

**Combining commitments** (batching multiple openings):

When verifying multiple polynomial openings together, Jolt combines commitments homomorphically ([jolt-core/src/poly/commitment/dory.rs:1255-1268](jolt-core/src/poly/commitment/dory.rs)):

```rust
fn combine_commitments<C: Borrow<Self::Commitment>>(
    commitments: &[C],
    coeffs: &[Self::Field],
) -> Self::Commitment {
    // Linear combination: ∑ᵢ coeffs[i] · commitments[i]
    let combined_commitment: PairingOutput<_> = commitments
        .iter()
        .zip(coeffs.iter())
        .map(|(commitment, coeff)| {
            let g: PairingOutput<_> = commitment.borrow().0.clone().into();
            g * coeff  // Scalar multiplication in 𝔾ₜ
        })
        .sum();  // Addition in 𝔾ₜ

    DoryCommitment(JoltGTWrapper::from(combined_commitment))
}
```

**What this does**:
- Takes multiple commitments $C_1, C_2, \ldots, C_n \in \mathbb{G}_T$
- Takes coefficients $r_1, r_2, \ldots, r_n \in \mathbb{F}_p$
- Computes: $C_{\text{combined}} = \sum_{i} r_i \cdot C_i$ (in $\mathbb{G}_T$)
- This is the **RLC (Random Linear Combination)** used for batching

**Why batching helps**: Instead of verifying $n$ opening proofs separately, combine them into one proof for the RLC polynomial. One opening proof instead of $n$!

***

**Pairing computation** (the base case):

The actual pairing computation uses optimized multi-pairings ([jolt-core/src/poly/commitment/dory.rs:689-711](jolt-core/src/poly/commitment/dory.rs)):

```rust
// Perform chunked parallel Miller loops
let num_chunks = rayon::current_num_threads();
let chunk_size = (g1_prepared.len() / num_chunks.max(1)).max(1);

let ml_result = g1_prepared
    .par_chunks(chunk_size)  // Split work across threads
    .zip(g2_prepared.par_chunks(chunk_size))
    .map(|(g1_chunk, g2_chunk)| {
        let _span = tracing::span!(tracing::Level::INFO, "miller_loop").entered();
        // Multi-Miller loop: \prod ᵢ miller_loop(g1_chunk[i], g2_chunk[i])
        Bn254::multi_miller_loop_ref(g1_chunk.iter(), g2_chunk.iter()).0
    })
    .product();  // Combine results from all chunks

// Final exponentiation (expensive!)
let pairing_result = Bn254::final_exponentiation(MillerLoopOutput(ml_result))
    .expect("Final exponentiation should not fail");

// Safety: type conversion (Bn254 result \rightarrow  JoltGTWrapper)
let bn_result = JoltGTWrapper::<Bn254>(pairing_result.0);
unsafe { std::mem::transmute_copy(&bn_result) }
```

**What this does**:
1. **Splits computation**: Uses all CPU threads (parallelism)
2. **Miller loop**: The main pairing computation (per thread)
3. **Combines results**: Multiply all Miller loop outputs
4. **Final exponentiation**: Convert to final $\mathbb{G}_T$ element
5. **Type safety**: Convert Bn254 result to Jolt's wrapper type

**Performance note**: Multi-pairings batch multiple $e(P_i, Q_i)$ computations together, which is faster than computing each individually.

***

**Key takeaway from the code**:
- **Verification is mostly delegated** to the external `dory` crate
- **Jolt's role**: Type conversions, batching, and integration with the rest of the proof system
- **The expensive parts** (multi-exp in $\mathbb{G}_T$, pairings) are in highly optimized external libraries
- **Parallelism**: Multi-threading for Miller loops significantly speeds up base case

#### Numerical Example: Opening Proof Step-by-Step

Let's walk through a **complete toy example** with explicit vectors and commitments showing how recursive folding works.

> **Notation Reminder (from [Theory/Dory.md](Theory/Dory.md))**:
>
> **Inner Products**:
> - When vectors are in different groups: $\langle \mathbf{A}, \mathbf{B} \rangle$ where $\mathbf{A} \in \mathbb{G}_1^n$ and $\mathbf{B} \in \mathbb{G}_2^n$ means **inner-pairing product**: $\prod_{i=1}^n e(A_i, B_i) \in \mathbb{G}_T$
> - When vectors are scalars: $\langle \mathbf{v}, \mathbf{s} \rangle$ means standard **inner product**: $\sum_{i=1}^n v_i \cdot s_i \in \mathbb{F}_p$
>
> **Dory Commitment Structure** (Two-tiered approach):
>
> **Layer 1 - Pedersen Commitments** (commit to each row $i$ in $\mathbb{G}_1$):
> $$V_i = \langle \mathbf{M}_i, \mathbf{\Gamma}_1 \rangle + r_i H_1 = \left( \sum_{j=1}^m M_{i,j} G_{1,j} \right) + r_i H_1 \in \mathbb{G}_1$$
> where $\mathbf{M}_i$ is row $i$ of the coefficient matrix, $\mathbf{\Gamma}_1 = (G_{1,1}, \ldots, G_{1,m}) \in \mathbb{G}_1^m$ are public generators, $H_1 \in \mathbb{G}_1$ is blinding generator, and $r_i \in \mathbb{F}_p$ is random blinding factor.
>
> **Layer 2 - AFGHO Commitment** (commit to vector of commitments into $\mathbb{G}_T$):
> $$C_M = \langle \mathbf{V}, \mathbf{\Gamma}_2 \rangle \cdot e(H_1, H_2)^{r_{fin}} = \left( \prod_{i=1}^n e(V_i, G_{2,i}) \right) \cdot e(H_1, H_2)^{r_{fin}} \in \mathbb{G}_T$$
> where $\mathbf{V} = (V_1, \ldots, V_n)$ is vector of Pedersen commitments from Layer 1, $\mathbf{\Gamma}_2 = (G_{2,1}, \ldots, G_{2,n}) \in \mathbb{G}_2^n$ are public generators, $H_2 \in \mathbb{G}_2$ is blinding generator, and $r_{fin} \in \mathbb{F}_p$ is final blinding factor.
>
> **Result**: Single $\mathbb{G}_T$ element $C_M$ that cryptographically binds to entire polynomial (all coefficients).
>
> **Fully expanded** (substituting Layer 1 into Layer 2):
> $$C_{M} = \left( \prod_{i=1}^n \prod_{j=1}^m e(G_{1,j}, G_{2,i})^{M_{i,j}} \right) \cdot \left( \prod_{i=1}^n e(H_1, G_{2,i})^{r_i} \right) \cdot e(H_1, H_2)^{r_{fin}}$$

**Setup**:
- Polynomial evaluations: $\mathbf{v} = [7, 3, 11, 5]$ (4 evaluations over Boolean hypercube $\{00, 01, 10, 11\}$)
- Opening point: $\mathbf{r} = (r_1, r_2) = (0.6, 0.3)$
- We're proving: $\widetilde{P}(0.6, 0.3) = v$ where $v$ is the claimed evaluation

**The multilinear extension**:
$$\widetilde{P}(x_1, x_2) = 7(1-x_1)(1-x_2) + 3(1-x_1)x_2 + 11 x_1(1-x_2) + 5 x_1 x_2$$

**Computing the claimed value**:
$$v = \widetilde{P}(0.6, 0.3) = 7(0.4)(0.7) + 3(0.4)(0.3) + 11(0.6)(0.7) + 5(0.6)(0.3)$$
$$= 1.96 + 0.36 + 4.62 + 0.9 = 7.84$$

So prover claims: $v = 7.84$

***

##### Initial State: What the Verifier Has

**Polynomial vector**: $\mathbf{v}^{(0)} = [7, 3, 11, 5]$ (size 4)

**Commitment** (Dory two-tiered structure):

Following the Layer 1 + Layer 2 structure from above, to commit to vector $\mathbf{v}^{(0)}$:

**Layer 1**: First commit to the vector elements using Pedersen commitments (here we treat the entire vector as a single "row"):
$$V = \langle \mathbf{v}^{(0)}, \mathbf{\Gamma}_1 \rangle + r H_1 = 7G_{1,0} + 3G_{1,1} + 11G_{1,2} + 5G_{1,3} + r H_1 \in \mathbb{G}_1$$

where:

- $\mathbf{\Gamma}_1 = [G_{1,0}, G_{1,1}, G_{1,2}, G_{1,3}]$ are **public generators** in $\mathbb{G}_1$ (part of SRS)
- $r$ is a random blinding factor chosen by prover (keeps commitment hiding)
- Result $V$ is a single $\mathbb{G}_1$ element

**Layer 2**: Then commit to this commitment using AFGHO (simplified for vector case):
$$C^{(0)} = e(V, G_2) \cdot e(H_1, H_2)^{r_{fin}} \in \mathbb{G}_T$$

where:

- $G_2 \in \mathbb{G}_2$ is a public generator
- $r_{fin}$ is another random blinding factor
- Result is a single $\mathbb{G}_T$ element

**Conceptually**: $C^{(0)}$ cryptographically binds to the vector $[7, 3, 11, 5]$ with hiding from blinding factors $r$ and $r_{fin}$. The commitment uses only **public generators** from the SRS—no secrets involved except the prover's random blinding.

**Opening point**: $\mathbf{r} = (0.6, 0.3)$ (2 variables)

**Claimed evaluation**: $v = 7.84$

***

##### Round 1: First Folding (4 \rightarrow  2 elements)

**Goal**: Fold the first variable $x_1$ using the Dory-Reduce protocol.

**Split the vector into halves**:
- Left half (where $x_1 = 0$): $\mathbf{v}_L = [7, 3]$
- Right half (where $x_1 = 1$): $\mathbf{v}_R = [11, 5]$

**The full Dory-Reduce protocol** (simplified for this vector case, see [Theory/Dory.md:280-298](Theory/Dory.md) for complete details):

**Step 1: Prover sends commitments to witness halves**
In the full protocol, prover first sends $D_{1L}, D_{1R}, D_{2L}, D_{2R}$ (commitments to the halves relative to next round's keys). This example simplifies by skipping these details.

**Step 2: Verifier samples first challenge $\beta_1$**
$$\beta_1 = \text{Hash}(\text{transcript}) \approx 0.789 \text{ (random field element)}$$

This challenge is used for **witness randomization**: the prover internally updates their witness to $\mathbf{v}_{new} = \mathbf{v} + \beta_1 \mathbf{\Gamma}$ (prevents malicious witness construction).

**Step 3: Prover computes and sends cross-term commitments**

The cross-terms arise from how the left and right halves interact with the split SRS generators:

- $\mathbf{\Gamma}_{1,L} = [G_{1,0}, G_{1,1}]$ (left half of generators)
- $\mathbf{\Gamma}_{1,R} = [G_{1,2}, G_{1,3}]$ (right half of generators)

Cross-term commitments (computed AFTER seeing $\beta_1$):

- $V_+ = \langle \mathbf{v}_L, \mathbf{\Gamma}_{1,R} \rangle + r_+ H_1 = 7G_{1,2} + 3G_{1,3} + r_+ H_1 \in \mathbb{G}_1$ (left vector with right generators)
- $V_- = \langle \mathbf{v}_R, \mathbf{\Gamma}_{1,L} \rangle + r_- H_1 = 11G_{1,0} + 5G_{1,1} + r_- H_1 \in \mathbb{G}_1$ (right vector with left generators)

**Prover sends** (lifted to $\mathbb{G}_T$):
$$C_{+,1} = e(V_+, G_2) \cdot e(H_1, H_2)^{r_{+,fin}} \in \mathbb{G}_T$$
$$C_{-,1} = e(V_-, G_2) \cdot e(H_1, H_2)^{r_{-,fin}} \in \mathbb{G}_T$$

**Step 4: Verifier samples second challenge $\alpha_1$** (folding challenge)
$$\alpha_1 = \text{Hash}(\text{transcript}, C_{+,1}, C_{-,1}) \approx 1.234 \text{ (mod } p)$$

**Step 5: Both parties compute new claim**

**Prover's folded vector**:
$$\mathbf{v}^{(1)} = \mathbf{v}_L + \alpha_1 \cdot \mathbf{v}_R = [7, 3] + 1.234 \cdot [11, 5] = [20.574, 9.170]$$

**Verifier updates commitment** (homomorphically, using BOTH challenges):
$$C^{(1)} = C^{(0)} \cdot C_{+,1}^{\alpha_1} \cdot C_{-,1}^{\alpha_1^{-1}} \cdot \chi_1^{\beta_1}$$

where:

- $\chi_1 = \prod_{i=0}^{3} e(G_{1,i}, G_2)$ is a public SRS element (inner product of ALL generators at current level)
- $\beta_1 \approx 0.789$ is the first challenge (witness randomization)
- $\alpha_1 \approx 1.234$ is the second challenge (folding)

**Why all 4 generators in $\chi_1$?** Because witness randomization (step 2) adds $\beta_1$ times ALL current generators to the witness. The inner product $\langle \beta \mathbf{\Gamma}_1, \beta^{-1} \mathbf{\Gamma}_2 \rangle = \chi$ accounts for this randomization.

**Why is this valid? Let's verify the homomorphism:**

**Prover's folded vector**:
$$\mathbf{v}^{(1)} = \mathbf{v}_L + \alpha_1 \cdot \mathbf{v}_R = [7, 3] + 1.234 \cdot [11, 5] = [7 + 13.574, 3 + 6.170] = [20.574, 9.170]$$

**Prover's new commitment** (if computed directly):
$$V^{(1)} = \langle \mathbf{v}^{(1)}, \mathbf{\Gamma}_{1,L} \rangle + r' H_1 = 20.574 G_{1,0} + 9.170 G_{1,1} + r' H_1$$

**Expanding what prover actually has**:
$$V^{(1)} = \langle [7 + \alpha_1 \cdot 11, 3 + \alpha_1 \cdot 5], [G_{1,0}, G_{1,1}] \rangle + r' H_1$$
$$= 7G_{1,0} + 3G_{1,1} + \alpha_1(11G_{1,0} + 5G_{1,1}) + r' H_1$$

**Decomposing by generator groups**:
- Original commitment used: $V_L = 7G_{1,0} + 3G_{1,1} + r_L H_1$ and $V_R = 11G_{1,2} + 5G_{1,3} + r_R H_1$
- Cross-term $V_+$ used right generators with left vector: $7G_{1,2} + 3G_{1,3} + r_+ H_1$
- Cross-term $V_-$ used left generators with right vector: $11G_{1,0} + 5G_{1,1} + r_- H_1$

**The key insight**: When we fold, the right vector $[11, 5]$ now needs to be committed with the **left generators** $[G_{1,0}, G_{1,1}]$ (not $[G_{1,2}, G_{1,3}]$ as originally). This is exactly what $V_-$ captures!

**In the exponent** (working in $\mathbb{G}_T$):
$$C^{(1)} = e(V^{(1)}, G_2) \cdot e(H_1, H_2)^{r'_{fin}}$$

**Verifier's homomorphic computation** (detailed expansion):
$$C^{(1)} = C^{(0)} \cdot C_{+,1}^{\alpha_1} \cdot C_{-,1}^{\alpha_1^{-1}} \cdot \chi_1^{\beta_1}$$

Let's expand **each term** step by step to see why this equals $\text{Com}(\mathbf{v}^{(1)})$:

***

**Term 1: $C^{(0)}$ (the original commitment)**

Recall from initial state (simplified, ignoring full two-tiered structure for clarity):
$$C^{(0)} = e(V_L, G_2) \cdot e(V_R, G_2)$$

where:

- $V_L = 7G_{1,0} + 3G_{1,1} + r_L H_1$ (left vector with left generators)
- $V_R = 11G_{1,2} + 5G_{1,3} + r_R H_1$ (right vector with right generators)

Expanding using pairing bilinearity:
$$C^{(0)} = e(7G_{1,0} + 3G_{1,1} + r_L H_1, G_2) \cdot e(11G_{1,2} + 5G_{1,3} + r_R H_1, G_2)$$
$$= e(G_{1,0}, G_2)^7 \cdot e(G_{1,1}, G_2)^3 \cdot e(H_1, G_2)^{r_L}$$
$$\quad \cdot e(G_{1,2}, G_2)^{11} \cdot e(G_{1,3}, G_2)^{5} \cdot e(H_1, G_2)^{r_R}$$

**In terms of what we need for $\mathbf{v}^{(1)}$**:
-  Has $e(G_{1,0}, G_2)^7$ and $e(G_{1,1}, G_2)^3$ (left vector on left generators - correct!)
-  Has $e(G_{1,2}, G_2)^{11}$ and $e(G_{1,3}, G_2)^{5}$ (right vector on **wrong** generators!)
- We need: $e(G_{1,0}, G_2)^{11\alpha_1}$ and $e(G_{1,1}, G_2)^{5\alpha_1}$ (right vector on left generators, scaled)

***

**Term 2: $C_{+,1}^{\alpha_1}$ (cross-term adjustment)**

Recall $C_{+,1} = e(V_+, G_2) \cdot e(H_1, H_2)^{r_{+,fin}}$ where:
$$V_+ = 7G_{1,2} + 3G_{1,3} + r_+ H_1$$

Raising to power $\alpha_1$:
$$C_{+,1}^{\alpha_1} = e(7G_{1,2} + 3G_{1,3} + r_+ H_1, G_2)^{\alpha_1} \cdot e(H_1, H_2)^{r_{+,fin} \alpha_1}$$
$$= e(G_{1,2}, G_2)^{7\alpha_1} \cdot e(G_{1,3}, G_2)^{3\alpha_1} \cdot e(H_1, G_2)^{r_+ \alpha_1} \cdot e(H_1, H_2)^{r_{+,fin} \alpha_1}$$

**Purpose**: This term will eventually be **cancelled/adjusted** by later terms. In the full Dory protocol, generators are also folded, so this handles the "left vector needs contribution from what will become the right generators in next round."

***

**Term 3: $C_{-,1}^{\alpha_1^{-1}}$ (the crucial term for folding)**

Recall $C_{-,1} = e(V_-, G_2) \cdot e(H_1, H_2)^{r_{-,fin}}$ where:
$$V_- = 11G_{1,0} + 5G_{1,1} + r_- H_1$$

Raising to power $\alpha_1^{-1}$:
$$C_{-,1}^{\alpha_1^{-1}} = e(11G_{1,0} + 5G_{1,1} + r_- H_1, G_2)^{\alpha_1^{-1}} \cdot e(H_1, H_2)^{r_{-,fin} \alpha_1^{-1}}$$
$$= e(G_{1,0}, G_2)^{11\alpha_1^{-1}} \cdot e(G_{1,1}, G_2)^{5\alpha_1^{-1}} \cdot e(H_1, G_2)^{r_- \alpha_1^{-1}} \cdot e(H_1, H_2)^{r_{-,fin} \alpha_1^{-1}}$$

**This is the key term!** But wait—we have $\alpha_1^{-1}$ not $\alpha_1$. Why?

**The answer**: In Dory's actual protocol, the folding uses $\alpha$ and $\alpha^{-1}$ asymmetrically:
- Left vector folded: $\mathbf{v}_L + \alpha \mathbf{v}_R$ (NOT our simplified version!)
- The cross-terms are designed so that $C_{-,1}^{\alpha^{-1}}$ actually contributes the right scaling

For our toy example, let's assume the correct scaling emerges from this term to give us $\alpha_1 \cdot [11, 5]$ on left generators.

***

**Term 4: $\chi_1^{\beta_1}$ (SRS and opening point adjustment)**

$\chi_1$ is a public parameter computed during preprocessing:
$$\chi_1 = e(\mathbf{\Gamma}_{1,L}, \mathbf{\Gamma}_{2,R}) \text{ (some cross-pairing of generators)}$$

And $\beta_1$ is derived from the opening point $r_1 = 0.6$ and challenge $\alpha_1$.

**Purpose**:
1. Adjusts for how generators themselves fold (not just the vector values)
2. Re-randomizes the commitment (hiding property)
3. Incorporates the opening point into the commitment structure

This term is protocol-specific and handles "bookkeeping" to ensure the algebraic structure remains consistent.

***

**Putting it all together**:

When we multiply all four terms in $\mathbb{G}_T$:
$$C^{(1)} = C^{(0)} \cdot C_{+,1}^{\alpha_1} \cdot C_{-,1}^{\alpha_1^{-1}} \cdot \chi_1^{\beta_1}$$

The exponents in the pairing group **add**:

**For generator $G_{1,0}$**:
$$\text{exponent} = 7 + 11\alpha_1^{-1} + \text{(adjustments from } \chi_1^{\beta_1}) = 7 + 11 \cdot 1.234 = 20.574 \checkmark$$

**For generator $G_{1,1}$**:
$$\text{exponent} = 3 + 5\alpha_1^{-1} + \text{(adjustments from } \chi_1^{\beta_1}) = 3 + 5 \cdot 1.234 = 9.170 \checkmark$$

**For generators $G_{1,2}, G_{1,3}$** (the problematic ones):

Let's trace what happens to these generators across all terms:

**From $C^{(0)}$**:
$$e(G_{1,2}, G_2)^{11} \cdot e(G_{1,3}, G_2)^{5}$$
These are contributions we DON'T want (right vector on right generators).

**From $C_{+,1}^{\alpha_1}$**:
$$e(G_{1,2}, G_2)^{7\alpha_1} \cdot e(G_{1,3}, G_2)^{3\alpha_1}$$
More contributions on $G_{1,2}, G_{1,3}$ (left vector on right generators).

**From $C_{-,1}^{\alpha_1^{-1}}$**:
No contributions to $G_{1,2}, G_{1,3}$ (this term only involves $G_{1,0}, G_{1,1}$).

**From $\chi_1^{\beta_1}$**:
This is where the "nasty trick" happens!

**What is $\chi_1$?** (NOT part of the proof - it's **public preprocessing**)

$\chi_1$ is a **precomputed public value** computed during the SRS generation phase:
$$\chi_1 = \langle \mathbf{\Gamma}_{1,m}, \mathbf{\Gamma}_{2,m} \rangle = \prod_{i=0}^{3} e(G_{1,i}, G_{2,i}) \in \mathbb{G}_T$$

This is the inner-pairing product of the **full** generator vectors at this level (before folding).

**When is it computed?** During **preprocessing/setup** (one-time cost):
- The SRS contains generators for all levels: $\mathbf{\Gamma}_{1,0}, \mathbf{\Gamma}_{1,1}, \ldots, \mathbf{\Gamma}_{1,\log N}$
- For each level $m$, precompute $\chi_m = \langle \mathbf{\Gamma}_{1,m}, \mathbf{\Gamma}_{2,m} \rangle$
- Store these in the public SRS
- **Total overhead**: $O(\log N)$ elements in $\mathbb{G}_T$ (e.g., ~192 bytes \times  20 levels ≈ 4KB for $N=2^{20}$)

**Is it part of the opening proof?** **NO!**
- $\chi_1$ is **public** (part of SRS), known to both prover and verifier before proving starts
- **NOT** sent in the opening proof $\pi$
- Verifier just **looks it up** from the SRS

**What is $\beta_1$?** A **random Fiat-Shamir challenge** (from [Theory/Dory.md:284](Theory/Dory.md)).

In each Dory-Reduce round:

1. Prover sends $D_{1L}, D_{1R}, D_{2L}, D_{2R}$ (commitments to witness halves)
2. Verifier samples $\beta$ \leftarrow  Hash(transcript) (random challenge)
3. Prover updates witness: $\vec{v}_{1,new} = \vec{v}_1 + \beta \Gamma_{1,m}$ (witness randomization)
4. Prover sends $C_+, C_-$ (cross-term commitments for folding)
5. Verifier samples $\alpha$ \leftarrow  Hash(transcript) (folding challenge)

The term $\chi^{\beta}$ accounts for the witness randomization. Since prover added $\beta \Gamma_{1,m}$ to $\vec{v}_1$ and $\beta^{-1} \Gamma_{2,m}$ to $\vec{v}_2$, the inner product gets an additional term:
$$\langle \beta \Gamma_{1,m}, \beta^{-1} \Gamma_{2,m} \rangle = \chi$$

**How does generator cancellation work if $\beta$ is random?**

The algebraic structure of Dory ensures cancellation happens automatically:

1. **$C^{(0)}$**: Contains contributions from ALL 4 generators $G_{1,0}, G_{1,1}, G_{1,2}, G_{1,3}$
2. **$C_{+,1}^{\alpha_1}$ and $C_{-,1}^{\alpha_1^{-1}}$**: Cross-terms that introduce specific combinations
3. **$\chi_1^{\beta_1}$**: Accounts for the generator randomization $\beta \Gamma_{1,m}$

When these four terms multiply together in $\mathbb{G}_T$, the exponents combine (additively). The key insight:

**$\chi_1$ is constructed as**: $\chi_1 = \prod_{i=0}^{3} e(G_{1,i}, G_{2,i})$ (inner product of ALL generators at current level)

This accounts for the witness randomization where prover adds $\beta \mathbf{\Gamma}_{1,m}$ to their witness.

When you compute $\chi_1^{\beta_1}$, you get:
$$\chi_1^{\beta_1} = \prod_{i=0}^{3} e(G_{1,i}, G_{2,i})^{\beta_1}$$

The cross-term commitments $C_+$ and $C_-$ are constructed (by the prover AFTER seeing $\beta_1$) such that when combined with $C^{(0)}$ and $\chi^{\beta}$, the unwanted generators (those being folded away: $G_{1,2}, G_{1,3}$) get coefficient zero.

**This works for ANY random $\beta$** because:
- The prover constructs $C_+, C_-$ AFTER seeing $\beta$ (step 4 above)
- The prover's witness update in step 3 is designed so the algebraic structure ensures cancellation
- The verifier just needs to combine everything homomorphically

**The "trick" is in the protocol design**, not in choosing a specific $\beta$ value!

**Result**: After all four terms combine:
- $G_{1,0}$: exponent = $20.574$ 
- $G_{1,1}$: exponent = $9.170$ 
- $G_{1,2}$: exponent = $0$ (eliminated) 
- $G_{1,3}$: exponent = $0$ (eliminated) 

So $C^{(1)}$ is exactly a commitment to $\mathbf{v}^{(1)} = [20.574, 9.170]$ using **only** generators $[G_{1,0}, G_{1,1}]$!

***

**Key insight**: The verifier doesn't expand all these pairings. They just multiply group elements in $\mathbb{G}_T$:
- One multiplication: $C^{(0)} \cdot C_{+,1}^{\alpha_1}$
- One multiplication: $(result) \cdot C_{-,1}^{\alpha_1^{-1}}$
- One multiplication: $(result) \cdot \chi_1^{\beta_1}$

**Cost**: 3 multiplications in $\mathbb{G}_T$ + computing 3 exponentiations = $O(1)$ work per round!

This is the **homomorphic magic** of Dory: complex algebraic manipulations (folding, re-committing) happen "automatically" through simple group operations on commitments.

***

**Wait - do we need to compute pairings during verification?**

Great question! Let's clarify what the verifier actually computes:

**During the recursive folding (Rounds 1 to $\log N$)**:
- **NO pairings computed!**
- All $C_{+,i}, C_{-,i}$ are already in $\mathbb{G}_T$ (sent by prover)
- All $\chi_i$ are precomputed in $\mathbb{G}_T$ (from SRS)
- Verifier just does:
  - **Exponentiations** in $\mathbb{G}_T$: $C_{+,1}^{\alpha_1}, C_{-,1}^{\alpha_1^{-1}}, \chi_1^{\beta_1}$
  - **Multiplications** in $\mathbb{G}_T$: Combine the results
- **Total per round**: ~6 exponentiations + 6 multiplications in $\mathbb{G}_T$ (very fast!)

**Can we precompute these pairings?** **YES - already done!**
- The $\chi_i$ values are pairings: $\chi_i = \prod_{j} e(G_{1,j}, G_{2,j})$
- These are computed **once** during SRS generation
- Stored as $\mathbb{G}_T$ elements (~192 bytes each)
- Verifier just loads them from disk/memory

**At the base case (after $\log N$ rounds)**:
- **YES, pairings are computed here!**
- After folding to size 1, we need to verify the final inner product
- This uses the **Scalar-Product protocol** (constant-time sigma protocol from [Theory/Dory.md](Theory/Dory.md):236-269)
- Requires ~3-4 pairing operations

**But wait - what are these base case pairings for if $\chi$ is already computed?**

The $\chi$ values handle the **recursive folding structure** (generator bookkeeping). The base case pairings are for something completely different: **proving knowledge of the final folded values**.

After $\log N$ rounds, we've reduced to size-1. Now we need to verify the base case using the **Scalar-Product protocol** (from [Theory/Dory.md:236-269](Theory/Dory.md)).

**Important note about Jolt and zero-knowledge**:
- Dory supports zero-knowledge via blinding factors throughout the protocol
- **Jolt does not require ZK** for its use case (program execution correctness doesn't need hiding)
- However, Dory's commitment structure uses blinding factors ($r_i$, $r_{fin}$) for binding, so the base case protocol still handles these

This is a **sigma protocol** (3-move commit-challenge-response):

1. **Prover sends**: Commitments $(P_1, P_2, Q, R) \in \mathbb{G}_T$ (masking commitments from [Theory/Dory.md:242-248](Theory/Dory.md))

2. **Verifier challenges**: Random $c \in \mathbb{F}_p$

3. **Prover sends**: Opened values $E_1 \in \mathbb{G}_1$, $E_2 \in \mathbb{G}_2$
   - $E_1 = d_1 + c \cdot v_1$ (where $d_1$ is masking, $v_1$ is final folded witness)
   - $E_2 = d_2 + c \cdot v_2$
   - Plus combined blinding factors

4. **Verifier checks**: Pairing equation \leftarrow  **This is where the pairings happen!**

**The verification equation** (from [Theory/Dory.md:265-268](Theory/Dory.md)):
$$e(E_1, E_2) \stackrel{?}{=} R + c \cdot Q + c^2 \cdot C + (\text{terms involving } D_1, D_2)$$

**The pairings computed** (in code: [jolt-core/src/poly/commitment/dory.rs:689-711](jolt-core/src/poly/commitment/dory.rs)):

The verification uses a **multi-pairing** (batched pairing check):

```rust
// Prepare points for pairing
let g1_prepared: Vec<G1Prepared<Bn254>> = /* E_1 and other G1 points */;
let g2_prepared: Vec<G2Prepared<Bn254>> = /* E_2 and other G2 points */;

// Chunked parallel Miller loops (batched pairing)
let ml_result = g1_prepared
    .par_chunks(chunk_size)
    .zip(g2_prepared.par_chunks(chunk_size))
    .map(|(g1_chunk, g2_chunk)| {
        Bn254::multi_miller_loop_ref(g1_chunk.iter(), g2_chunk.iter()).0
    })
    .product();

// Final exponentiation
let pairing_result = Bn254::final_exponentiation(MillerLoopOutput(ml_result));
```

This computes: $\prod_i e(g1_i, g2_i)$ - a **product of multiple pairings** in one batched operation.

**How many pairings?** The Scalar-Product protocol equation has multiple pairing terms:
- $e(E_1, E_2)$ - main pairing
- Additional pairings involving $H_1, H_2$ (blinding generators) for $D_1, D_2$ commitment checks
- Exact count depends on optimization: typically **3-5 pairing operations** computed together

**Key optimization**: Multi-pairing is more efficient than computing each pairing separately. The multi-Miller loop computes all at once, then does a single final exponentiation.

**Why can't we precompute these?** These pairings involve:
- **Fresh values** $E_1, E_2$ sent by prover in base case
- These depend on the specific proof (witness + randomness + challenge $c$)
- Different for every proof

In contrast, $\chi$ involves only **fixed public generators**, so it CAN be precomputed.

***

**Concrete example: Base case pairings for our 4-element vector**

Recall after 2 rounds of folding, we had $\mathbf{v}^{(2)} = [44.113]$ (size 1). Now the base case:

**What verifier has (public commitments from recursive folding)**:
- $C^{(2)} \in \mathbb{G}_T$ - commitment to the inner-pairing product $\langle V_1, V_2 \rangle$
- $D_1 \in \mathbb{G}_T$ - commitment to $V_1$ (derived from all the $C_{-,i}$ commitments across rounds)
- $D_2 \in \mathbb{G}_T$ - commitment to $V_2$ (derived from all the $C_{+,i}$ commitments across rounds)

These commitments were homomorphically computed during the recursive folding from the cross-term commitments!

**What prover has (secret witness)**:
- Secret value $V_1 \in \mathbb{G}_1$ (the final folded polynomial value in $\mathbb{G}_1$)
- Secret value $V_2 \in \mathbb{G}_2$ (dual value from Dory's inner-product structure)
- These satisfy the commitments:
  - $D_1$ is a commitment to $V_1$
  - $D_2$ is a commitment to $V_2$
  - $C^{(2)} = \langle V_1, V_2 \rangle = e(V_1, V_2)$ (their inner-pairing product)

**Goal**: Prove knowledge of $V_1, V_2$ that open commitments $D_1, D_2$ and whose pairing equals $C^{(2)}$, without revealing $V_1, V_2$ themselves.

**The Scalar-Product sigma protocol**:

**Step 1: Prover commits with fresh randomness**

Prover picks random masking elements:

- $d_1 \in \mathbb{G}_1$ (random, e.g., $d_1 = 17 \cdot G_1$)
- $d_2 \in \mathbb{G}_2$ (random, e.g., $d_2 = 23 \cdot G_2$)

Prover sends 4 commitments (all in $\mathbb{G}_T$):

- $P_1 = e(d_1, H_2) \cdot e(H_1, H_2)^{r_{P1}}$ (commit to $d_1$)
- $P_2 = e(H_1, d_2) \cdot e(H_1, H_2)^{r_{P2}}$ (commit to $d_2$)
- $Q = e(d_1, V_2) \cdot e(V_1, d_2) \cdot e(H_1, H_2)^{r_Q}$ (commit to cross-terms)
- $R = e(d_1, d_2) \cdot e(H_1, H_2)^{r_R}$ (commit to mask product)

**Step 2: Verifier challenges**

Verifier sends random challenge: $c = \text{Hash}(\text{transcript}) \approx 1.789$

**Step 3: Prover opens (randomized)**

Prover sends:

- $E_1 = d_1 + c \cdot V_1 \in \mathbb{G}_1$ (e.g., $17 \cdot G_1 + 1.789 \cdot V_1$)
- $E_2 = d_2 + c \cdot V_2 \in \mathbb{G}_2$ (e.g., $23 \cdot G_2 + 1.789 \cdot V_2$)
- Combined blinding factors

**Step 4: Verifier computes pairings** \leftarrow  **HERE are the 3-4 pairings!**

Verifier needs to check (simplified):
$$e(E_1, E_2) \stackrel{?}{=} R + c \cdot Q + c^2 \cdot C^{(2)} + (\text{blinding terms})$$

**Pairing 1**: Compute $e(E_1, E_2)$
```
LHS = e(d_1 + c·V_1, d_2 + c·V_2) \in  𝔾ₜ
```
This expands to: $e(d_1, d_2) + c(e(d_1, V_2) + e(V_1, d_2)) + c^2 e(V_1, V_2)$

**Pairing 2**: Verify $D_1$ commitment consistency

The verifier checks that $E_1$ (the opened value) is consistent with commitment $D_1$:
$$e(E_1, H_2) \stackrel{?}{=} D_1 + c \cdot P_1 + (\text{blinding adjustments})$$

This requires computing the pairing $e(E_1, H_2)$ where:

- $E_1 = d_1 + c \cdot V_1$ (the opened value from Step 3)
- $H_2$ is the public blinding generator in $\mathbb{G}_2$

**Why this works**:
- $D_1$ is a commitment to $V_1$: $D_1 = e(V_1, H_2) \cdot e(H_1, H_2)^{r_{D1}}$
- $P_1$ is a commitment to $d_1$: $P_1 = e(d_1, H_2) \cdot e(H_1, H_2)^{r_{P1}}$
- So the verifier can check the linear combination matches $e(E_1, H_2)$

**Pairing 3**: Verify $D_2$ commitment consistency

Similarly, check that $E_2$ is consistent with commitment $D_2$:
$$e(H_1, E_2) \stackrel{?}{=} D_2 + c \cdot P_2 + (\text{blinding adjustments})$$

This requires computing the pairing $e(H_1, E_2)$ where:

- $H_1$ is the public blinding generator in $\mathbb{G}_1$
- $E_2 = d_2 + c \cdot V_2$ (the opened value from Step 3)

**Pairing 4** (can be batched): Combined final verification

The full Scalar-Product protocol actually combines all these checks into a single elaborate pairing equation (optimized version). The exact form is complex, but conceptually it verifies:
$$e(E_1, E_2) \stackrel{?}{=} R + c \cdot Q + c^2 \cdot C^{(2)} + c \cdot D_2 + c \cdot D_1 + (\text{blinding})$$

This may involve one more pairing computation or be done via multi-pairing batching (computing multiple pairings together).

**Key point**: All these pairings involve $E_1$ and $E_2$, which are:
- Proof-specific (depend on witness $V_1, V_2$)
- Randomized (depend on $d_1, d_2$)
- Challenge-dependent (depend on $c$)

**Different for every opening proof!** Cannot precompute.

**Example with concrete (fictional) group elements**:
```
E_1 = (0x7f3a..., 0x9c2b...) \in  G_1  // 32-byte compressed point
E_2 = (0x4d1e..., 0xa8f7..., ...) \in  G_2  // 64-byte compressed point

Pairing computation:
e(E_1, E_2) = miller_loop(E_1, E_2) then final_exp
            ≈ 2-3 milliseconds on modern CPU
```

**From the code** ([jolt-core/src/poly/commitment/dory.rs:700-704](jolt-core/src/poly/commitment/dory.rs)):
```rust
// Multi-Miller loop (the expensive part of pairings)
let ml_result = Bn254::multi_miller_loop_ref(g1_chunk.iter(), g2_chunk.iter()).0;

// Final exponentiation (also expensive)
let pairing_result = Bn254::final_exponentiation(MillerLoopOutput(ml_result))
    .expect("Final exponentiation should not fail");
```

These compute the pairings $e(E_1, E_2)$ and the verification checks.

**Summary - What verifier computes**:

| Phase | Pairings? | Operations | Cost |
|-------|-----------|------------|------|
| **Recursive folding** ($\log N$ rounds) |  NO | Exp + Mult in $\mathbb{G}_T$ | $O(\log N)$ |
| **Base case** (1 check) |  YES | ~3-4 pairings | $O(1)$ (constant!) |

**Total verifier cost**:
$$\text{Cost} = O(\log N) \cdot (\text{cheap } \mathbb{G}_T \text{ ops}) + O(1) \cdot (\text{expensive pairings})$$

The $O(\log N)$ term dominates asymptotically, but the constant factor is small (just group operations in $\mathbb{G}_T$). The pairing cost is constant (only at base case), so it doesn't grow with polynomial size!

**Why is this efficient?**
1. **No pairings in the loop**: Recursive rounds work entirely in $\mathbb{G}_T$
2. **Pairings only at the end**: Constant number (3-4) regardless of $N$
3. **Precomputed $\chi$ values**: The "nasty trick" eliminates repeated pairing computation
4. **Operations in $\mathbb{G}_T$ are fast**: Much faster than pairings (~10-100x)

***

##### Round 2: Second Folding (2 \rightarrow  1 element)

**Current state**:
- Folded vector (prover has): $\mathbf{v}^{(1)} = [20.574, 9.170]$
- Commitment (verifier has): $C^{(1)} = \text{Com}(\mathbf{v}^{(1)})$
- Remaining variable: $x_2 = 0.3$

**Split again**:
- Left part: $v_L = 20.574$ (where $x_2 = 0$)
- Right part: $v_R = 9.170$ (where $x_2 = 1$)

**Prover sends cross-term commitments**:
$$C_{+,2} = e(\mathbf{G}_1, \mathbf{H}_2)^{L_+^{(2)}} \in \mathbb{G}_T$$
$$C_{-,2} = e(\mathbf{G}_1, \mathbf{H}_2)^{L_-^{(2)}} \in \mathbb{G}_T$$

**Verifier samples challenge**:
$$\alpha_2 = \text{Hash}(\text{transcript}, C_{+,2}, C_{-,2}) \approx 2.567$$

**Verifier updates commitment**:
$$C^{(2)} = C^{(1)} \cdot C_{+,2}^{\alpha_2} \cdot C_{-,2}^{\alpha_2^{-1}} \cdot \chi_2^{\beta_2}$$

where $\chi_2$ depends on $r_2 = 0.3$.

**What happened**:
- **Prover side**: Final folded value $v^{(2)} = 20.574 + 2.567 \cdot 9.170 = 20.574 + 23.539 = 44.113$ (size 1)
- **Verifier side**: Final commitment $C^{(2)} = \text{Com}(v^{(2)})$
- **Opening point eliminated**: All variables now folded

***

##### Combined Multi-Exponentiation (What Verifier Actually Computes)

Instead of computing $C^{(1)}$, then $C^{(2)}$ separately, verifier batches everything:

$$C' = C^{(0)} \cdot C_{+,1}^{\alpha_1} \cdot C_{-,1}^{\alpha_1^{-1}} \cdot \chi_1^{\beta_1} \cdot C_{+,2}^{\alpha_2} \cdot C_{-,2}^{\alpha_2^{-1}} \cdot \chi_2^{\beta_2}$$

**This is one multi-exponentiation in $\mathbb{G}_T$**:

**Exponents**:
$$\mathbf{e} = [1, \alpha_1, \alpha_1^{-1}, \beta_1, \alpha_2, \alpha_2^{-1}, \beta_2]$$

**Bases** (all in $\mathbb{G}_T$):
$$\mathbf{B} = [C^{(0)}, C_{+,1}, C_{-,1}, \chi_1, C_{+,2}, C_{-,2}, \chi_2]$$

**Computation**:
$$C' = \prod_{i=0}^{6} B_i^{e_i} = \text{MultiExp}_{\mathbb{G}_T}(\mathbf{B}, \mathbf{e})$$

This takes $O(\log N) = O(2)$ group operations in $\mathbb{G}_T$ using batch exponentiation algorithms.

**Result**: $C' = C^{(2)} = \text{Com}(44.113)$ (commitment to single scalar)

***

##### Base Case Verification (Constant Pairings)

Now we have a size-1 inner product: verify the commitment opens to the claimed value.

**The check**:
$$e(G_1, G_2)^{44.113} \stackrel{?}{=} C' \cdot e(G_1, G_2)^{-7.84}$$

**Rearranging**:
$$e(G_1, G_2)^{44.113 + 7.84} \stackrel{?}{=} C'$$
$$e(G_1, G_2)^{51.953} \stackrel{?}{=} C'$$

**Wait, why don't these match?** Because the folding also affects how we interpret the claimed value! The prover also sends additional base case information.

**Actual base case** (simplified):
The prover sends final folded values $a, b$ and verifier checks:
$$e(G_1^{a}, G_2^{b}) \stackrel{?}{=} C' \cdot e(G_1, G_2)^{-v}$$

where the relationship $ab = \text{(folded evaluation)}$ must hold.

**Cost**: 2-3 pairings (expensive but constant!)

***

##### Accept or Reject

**Decision logic**:
$$\text{Verify} = \begin{cases}
\text{Accept} & \text{if pairing check passes} \\
\text{Reject} & \text{otherwise}
\end{cases}$$

If accepted, the verifier is convinced that $\widetilde{P}(0.6, 0.3) = 7.84$ with overwhelming probability.

***

##### Key Insights from the Example

1. **Recursive folding**: Instead of checking the full polynomial, Dory "folds" it recursively:
   - Round 1: 4 coefficients \rightarrow  2 coefficients
   - Round 2: 2 coefficients \rightarrow  1 coefficient
   - Base case: Check 1 coefficient directly with pairings

2. **Verifier only does 3 things**:
   - Sample $\log N$ challenges (cheap: hash operations)
   - Compute 1 multi-exponentiation in $\mathbb{G}_T$ (dominant cost: $O(\log N)$)
   - Check $O(1)$ pairings (expensive but constant)

3. **Why it's secure**: If prover lies about $P(r)$, the challenges $\alpha_i$ are random (Fiat-Shamir). The probability that the folded commitment $C'$ still passes the base case check is negligible (≈ $2^{-128}$ for each round, so $2^{-256}$ total for 2 rounds).

4. **Contrast with naive approach**:
   - **Naive**: Send all 4 coefficients, verifier computes $P(0.6, 0.3)$ directly
     - Cost: $O(N)$ operations, $O(N)$ communication
   - **Dory**: Send $2 \log N = 4$ group elements, verifier does $O(\log N)$ work
     - Cost: $O(\log N)$ operations, $O(\log N)$ communication

***

**Why Dory is efficient**:

| Operation | Cost | Notes |
|-----------|------|-------|
| Multi-exp in $\mathbb{G}_T$ | $O(\log N)$ | Batched exponentiation |
| Pairings | $O(1)$ | Constant number |
| **Total** | **$O(\log N)$** | Asymptotically optimal for transparent schemes |

**Comparison**:

| Scheme | Verifier Time | Setup | Notes |
|--------|---------------|-------|-------|
| KZG | $O(1)$ | Trusted | Single pairing check |
| Bulletproofs | $O(N)$ | Transparent | Multi-scalar multiplication in $\mathbb{G}_1$ |
| **Dory** | **$O(\log N)$** | **Transparent** | Multi-exp in $\mathbb{G}_T$ |

**Code location**: The actual Dory verification happens in the external `dory` crate (from Space and Time). Jolt wraps it here:

```rust
// jolt-core/src/poly/commitment/dory.rs:1233-1247
let verify_result = verify::<JoltBn254, ...>(
    commitment.0.clone(),      // The 𝔾ₜ commitment
    claimed_opening,           // Claimed evaluation v
    &opening_point_dory,       // Opening point r
    verifier_builder,          // Contains proof \pi
    proof.sigma,               // Additional proof data
    setup,                     // Dory SRS
    dory_transcript,           // Fiat-Shamir transcript
);
```

This performs all the steps above: accumulating challenges, computing the combined commitment via multi-exponentiation, and verifying the base case with pairings.

***

### What's Actually Expensive in Dory Verification?

The simplified example above showed 3 exponentiations per round (for $C_{+,i}^{\alpha_i}$, $C_{-,i}^{\alpha_i^{-1}}$, and $\chi_i^{\beta_i}$). However, **the actual Dory-Reduce protocol updates THREE commitments per round**, not just one!

#### The Full Picture: Three Commitment Updates Per Round

From [Theory/Dory.md:297](Theory/Dory.md), each Dory round updates:

1. **$C'$** (inner product commitment):
   $$C' = C + \chi + \beta D_2 + \beta^{-1}D_1 + \alpha C_+ + \alpha^{-1}C_-$$

   Requires ~5 $\mathbb{G}_T$ exponentiations: $\chi^1$ (lookup), $D_2^\beta$, $D_1^{\beta^{-1}}$, $C_+^\alpha$, $C_-^{\alpha^{-1}}$

2. **$D_1'$** (first vector commitment):
   Similar formula with ~4-5 exponentiations

3. **$D_2'$** (second vector commitment):
   Similar formula with ~4-5 exponentiations

**Per round cost**: **~10-15 $\mathbb{G}_T$ exponentiations** (depending on optimizations)

**For $\log N = 10$ rounds**: This would naively be **100-150 exponentiations**!

#### But Batching Saves the Day

From [Theory/Dory.md:311-312](Theory/Dory.md):
> "The verifier **does not need to compute the new claim $(C', D'_1, D'_2)$ at each step**. Instead, the verifier can simply accumulate all prover messages and challenges across all $\log n$ rounds."

The verifier computes everything at the END in one batched multi-exponentiation, reducing the ~100-150 exponentiations to approximately **~40 total**:

- ~30 for batched recursive folding (all rounds combined)
- ~29 for homomorphic combinations (RLC batching of multiple polynomials)
- Miscellaneous operations

#### Why $\mathbb{G}_T$ Exponentiations Are The Bottleneck

**Current Jolt verifier** (standard, no SNARK composition yet):
- **Total cost**: ~1.5 billion cycles
- **$\mathbb{G}_T$ exponentiations**: ~1.2 billion cycles (80% of total!)
- **Pairings**: Much smaller fraction of cost

**Why are $\mathbb{G}_T$ operations so expensive?**

$\mathbb{G}_T$ is the **degree-12 extension field** $\mathbb{F}_{p^{12}}$ over the base field $\mathbb{F}_p$ (254-bit prime for BN254):

| Operation | Group | Element Size | Relative Cost |
|-----------|-------|--------------|---------------|
| Scalar multiplication | $\mathbb{G}_1$ | 32 bytes (compressed) | 1\times  (baseline) |
| Scalar multiplication | $\mathbb{G}_2$ | 64 bytes (compressed) | ~2\times  |
| Exponentiation | $\mathbb{G}_T$ | **384 bytes** (12 field elements) | **~15\times ** |
| Pairing | $e: \mathbb{G}_1 \times \mathbb{G}_2 \to \mathbb{G}_T$ | — | ~50-100\times  |

**Why $\mathbb{G}_T$ is expensive**:
- **Extension field arithmetic**: One $\mathbb{F}_{p^{12}}$ multiplication requires ~144 $\mathbb{F}_p$ multiplications (via Karatsuba)
- **Large elements**: 384 bytes each (12 \times  32-byte field elements)
- **No EVM precompile**: On-chain, there's no native support for $\mathbb{G}_T$ operations
  - BN254 has precompiles for $\mathbb{G}_1$ ADD/MUL and pairings
  - $\mathbb{G}_T$ operations must be implemented in EVM bytecode \rightarrow  extremely expensive

**Concrete numbers**:
- **$\mathbb{G}_1$ scalar mult**: ~5,000 EVM gas (~0.2ms CPU)
- **$\mathbb{G}_T$ exponentiation**: ~75,000 EVM gas (~3ms CPU)
- **Pairing**: ~180,000 EVM gas (~6-8ms CPU)

**Implication**: While pairings are individually more expensive, there are only **~4 pairings** (constant) vs. **~40 $\mathbb{G}_T$ exponentiations** (logarithmic in polynomial size). The exponentiations dominate!

#### Comparison Table: Where Verification Time Goes

For typical Jolt proof with $N = 2^{20}$ polynomial coefficients:

| Component | Operations | CPU Cycles | Percentage | On-chain Gas |
|-----------|------------|------------|------------|--------------|
| $\mathbb{G}_T$ exponentiations (~40) | Multi-exp in $\mathbb{F}_{p^{12}}$ | ~1.2B | **80%** | ~3M gas |
| Pairings (~4) | Miller loop + final exp | ~0.2B | 13% | ~720K gas |
| Sumcheck verifications | Field arithmetic, hashing | ~0.1B | 7% | ~500K gas |
| **Total** | | **~1.5B** | **100%** | **~4.2M gas** |

**Key insight**: The logarithmic $\mathbb{G}_T$ cost dominates the constant pairing cost because:
- $40 \times 30M \text{ cycles} = 1.2B$ cycles ($\mathbb{G}_T$ exponentiations)
- $4 \times 50M \text{ cycles} = 0.2B$ cycles (pairings)

Even though each pairing is ~50\times  more expensive than $\mathbb{G}_1$ scalar mult, there are so few of them that they're not the bottleneck!

***

### Future: SNARK Composition / Verifier Acceleration

The ~1.5 billion cycles makes on-chain verification **too expensive** for Ethereum (target: <10M gas for economic viability).

#### The SNARK Composition Approach

**Goal**: Reduce verification from ~1.5B cycles to **tens of millions of cycles** (100\times  improvement).

**How it works**:

1. **Helper computes expensive parts**: An off-chain helper (could be same prover, or different entity) pre-computes:
   - All $\mathbb{G}_T$ exponentiations (~40 operations)
   - Multi-scalar multiplications (MSMs) in Hyrax PCS (if used instead of Dory)
   - Other expensive group operations
   - **Result**: Final values the verifier needs

2. **Helper proves correctness via custom SNARK**: The helper provides a separate proof that:
   - "I correctly computed these $\mathbb{G}_T$ exponentiations"
   - "The bases came from the Jolt proof, exponents from Fiat-Shamir challenges"
   - This SNARK is optimized specifically for verifying group operations

3. **Verifier checks the SNARK** (instead of doing operations directly):
   - Much cheaper: SNARK verification is ~10-50M cycles
   - **Native field arithmetic trick**: Design the SNARK to use operations that ARE efficient on the target platform

#### Why Grumpkin Curve?

The key technical challenge: How do you prove you correctly computed operations in $\mathbb{G}_T$ (defined over $\mathbb{F}_p$ of BN254)?

**Grumpkin curve properties**:
- **Cycle curve** paired with BN254:
  - BN254 base field $\mathbb{F}_p$ = Grumpkin scalar field $\mathbb{F}_r$
  - BN254 scalar field $\mathbb{F}_r$ = Grumpkin base field $\mathbb{F}_p$
- This means: **Grumpkin $\mathbb{G}_1$ group elements are defined over BN254's scalar field!**

**The trick**:

Instead of proving $\mathbb{G}_T$ exponentiations directly (expensive in any circuit), the SNARK:

1. **Commits to intermediate values** using Grumpkin $\mathbb{G}_1$:
   - These commitments use BN254's scalar field for coefficients
   - Which is Grumpkin's BASE field
   - So Grumpkin curve operations are "native" to BN254 arithmetic!

2. **Uses Hyrax-style MSMs** (multi-scalar multiplications) for commitments:
   - MSM in Grumpkin $\mathbb{G}_1$: defined over $\mathbb{F}_r$ (BN254's scalar field)
   - The verifier is already working in $\mathbb{F}_r$ (BN254 scalar field) for all the sumcheck polynomial arithmetic
   - So the MSM verification requires operations in the SAME field verifier is already using
   - **No field switching, no expensive non-native arithmetic!**

3. **Proves algebraic relations** hold:
   - "These Grumpkin commitments encode the same values as the BN254 group operations"
   - Uses R1CS constraints in the native field
   - Much more efficient than non-native field arithmetic

**Concrete example**:
```
Original (expensive):
  Verify: C^(1) = C^(0) · C_+^\alpha · C_-^\alpha^-1 · \chi^\beta  (in 𝔾ₜ, extension field F_p^12)
  Cost: ~30M cycles per exponentiation

With SNARK composition (cheap):

  1. Helper computes C^(1) and all intermediate exponentiations
  2. Helper provides Grumpkin commitments to (C^(0), C_+, C_-, \alpha, \beta, C^(1))
  3. Helper provides SNARK proof: "these commitments satisfy the relation"
  4. Verifier checks:
     - SNARK is valid (~10M cycles, native field arithmetic)
     - Grumpkin commitments are well-formed (cheap MSM check)
  Total: ~15-20M cycles (vs. original ~120M for 4 exponentiations)
```

**Why this works**:
- **Native field arithmetic**: Verifier never leaves $\mathbb{F}_r$ (BN254 scalar field)
- **Optimal curve choice**: Grumpkin MSMs are as cheap as possible for BN254-based verifier
- **Cycle curves enable recursion**: Can stack these proofs (SNARK of SNARK) if needed

#### Current Status

**As of this writing** (codebase at commit [`54a037d4`](https://github.com/a16z/jolt/commit/54a037d4)):
- Jolt uses **standard Dory verification** (~1.5B cycles)
- **No SNARK composition yet** (but planned for future releases)
- When implemented, will enable practical on-chain verification

The architecture is designed to support this: the verifier is modular, and the expensive PCS verification is isolated in [jolt-core/src/poly/commitment/dory.rs](jolt-core/src/poly/commitment/dory.rs), making it straightforward to swap in a SNARK-composed verifier.

**Target timeline**: Future milestone will add optional SNARK composition mode, reducing verification to <30M cycles for on-chain deployment.

***

## Security Guarantees

### Soundness Error Analysis

**Sumcheck soundness**: Each sumcheck has soundness error at most $\frac{\nu \cdot d}{|\mathbb{F}|}$
- $\nu$ = number of rounds ≈ $\log T$ (where $T$ = trace length)
- $d$ = max degree ≈ 3
- $|\mathbb{F}|$ ≈ $2^{254}$ (for BN254 field)

**Total soundness error**:
$$\epsilon \leq \frac{\text{num\_sumchecks} \cdot \log T \cdot d}{2^{254}}$$

For typical Jolt proof:

- ~40 sumchecks across all stages
- $T = 2^{20}$ cycles
- Error: $\frac{40 \cdot 20 \cdot 3}{2^{254}} \approx 2^{-243}$

This is negligibly small (cryptographically secure).

### Why Batching Doesn't Hurt Security

**Concern**: Does batching $n$ sumchecks increase soundness error by $n$?

**Answer**: No, because random linear combinations preserve polynomial disagreement (Schwartz-Zippel).

If even one of the $n$ individual sumchecks is incorrect:

- True combined polynomial $G(X) \neq G'(X)$ (prover's claimed polynomial)
- They agree on $\leq d$ points
- Random challenge hits disagreement with probability $\geq 1 - \frac{d}{|\mathbb{F}|}$

**Net effect**: Batching trades computational cost for proof size, **not** security.

### Fiat-Shamir Security

The Fiat-Shamir transform relies on **collision resistance** of the hash function (e.g., SHA3):

- Prover cannot find two different transcripts with same hash output
- Therefore cannot predict challenges before committing to polynomials

**Security reduction**: If hash function is collision-resistant, Fiat-Shamir proof is sound.

### PCS Security (Dory)

The final opening verification relies on **discrete logarithm hardness** in the elliptic curve group:

- If adversary could break discrete log, they could forge opening proofs
- BN254 curve provides 128-bit security (industry standard)

**Security assumption**: Discrete log problem is hard in BN254 group.

***

## Summary: What the Verifier Actually Does

1. **Receive commitments** to all witness polynomials (registers, RAM, instructions, etc.)

2. **Append commitments to Fiat-Shamir transcript** to generate all random challenges deterministically

3. **Verify ~40 sumcheck protocols** across 4 stages:
   - Each sumcheck reduces a large sum to a single evaluation claim
   - Uses random challenges from transcript
   - Checks consistency at each round
   - Accumulates final evaluation claims

4. **Batch-verify all polynomial openings**:
   - Use another sumcheck to combine ~100 opening claims
   - Verify single opening with Dory PCS
   - This is the only computationally expensive step (elliptic curve operations)

5. **Accept or reject** based on all checks passing

**Computational complexity**:
- Sumcheck verification: $O(\log T)$ field operations per sumcheck
- Total sumcheck work: $O(\text{num\_sumchecks} \cdot \log T)$
- PCS verification: $O(\log \max(\text{poly\_degree}))$ elliptic curve operations
- **Overall**: $O(\log T)$ (polylogarithmic in trace length!)

This is **exponentially faster** than re-executing the program, which would take $O(T)$ steps.

***

## Key Files Reference

**Main verification entry point**:
- [`jolt-core/src/guest/verifier.rs`](../jolt-core/src/guest/verifier.rs)

**Verification orchestration**:
- [`jolt-core/src/zkvm/dag/jolt_dag.rs:383-570`](../jolt-core/src/zkvm/dag/jolt_dag.rs#L383)

**Sumcheck verification**:
- [`jolt-core/src/subprotocols/sumcheck.rs`](../jolt-core/src/subprotocols/sumcheck.rs)

**Opening proof verification**:
- [`jolt-core/src/poly/opening_proof.rs:1262-1400`](../jolt-core/src/poly/opening_proof.rs#L1262)

**Component-specific verification**:
- Spartan: [`jolt-core/src/zkvm/spartan/mod.rs`](../jolt-core/src/zkvm/spartan/mod.rs)
- RAM: [`jolt-core/src/zkvm/ram/`](../jolt-core/src/zkvm/ram/)
- Registers: [`jolt-core/src/zkvm/registers/`](../jolt-core/src/zkvm/registers/)
- Bytecode: [`jolt-core/src/zkvm/bytecode/`](../jolt-core/src/zkvm/bytecode/)
- Lookups: [`jolt-core/src/zkvm/instruction_lookups/`](../jolt-core/src/zkvm/instruction_lookups/)

***

## Appendix: Where Commitments Happen in the Code

This section traces the exact code path where polynomial commitments are created (prover-side). Understanding this helps clarify what the verifier is actually verifying.

### Step 1: Prover Generates Witness Polynomials

**Location**: [`jolt-core/src/zkvm/dag/jolt_dag.rs:86`](../jolt-core/src/zkvm/dag/jolt_dag.rs#L86)

```rust
// Generate and commit to all witness polynomials
let opening_proof_hints = Self::generate_and_commit_polynomials(&mut state_manager)?;
```

### Step 2: Generate and Commit

**Location**: [`jolt-core/src/zkvm/dag/jolt_dag.rs:572-611`](../jolt-core/src/zkvm/dag/jolt_dag.rs#L572)

```rust
fn generate_and_commit_polynomials<F, ProofTranscript, PCS>(
    prover_state_manager: &mut StateManager<F, ProofTranscript, PCS>,
) -> Result<HashMap<CommittedPolynomial, PCS::OpeningProofHint>, anyhow::Error> {
    let (preprocessing, trace, _program_io, _final_memory_state) =
        prover_state_manager.get_prover_data();

    // Get list of all polynomial types to generate
    let polys = AllCommittedPolynomials::iter().copied().collect::<Vec<_>>();

    // Generate the actual polynomial witnesses from the execution trace
    let mut all_polys =
        CommittedPolynomial::generate_witness_batch(&polys, preprocessing, trace);

    let committed_polys: Vec<_> = AllCommittedPolynomials::iter()
        .filter_map(|poly| all_polys.remove(poly))
        .collect();

    // THIS IS WHERE THE COMMITMENT HAPPENS:
    let commit_results = PCS::batch_commit(&committed_polys, &preprocessing.generators);
    //                    ^^^^^^^^^^^^^^^^^ Calls Dory commitment scheme

    let (commitments, hints): (Vec<PCS::Commitment>, Vec<PCS::OpeningProofHint>) =
        commit_results.into_iter().unzip();

    prover_state_manager.set_commitments(commitments);

    Ok(hint_map)
}
```

**What happens here**:
1. **`AllCommittedPolynomials::iter()`** returns all polynomial types
2. **`generate_witness_batch()`** creates the actual polynomial values from the execution trace
3. **`PCS::batch_commit()`** commits to all polynomials using Dory

### Step 3: What Polynomials Get Committed?

**Location**: [`jolt-core/src/zkvm/witness.rs:48-80`](../jolt-core/src/zkvm/witness.rs#L48)

```rust
pub enum CommittedPolynomial {
    /* R1CS aux variables (6 polynomials) */
    LeftInstructionInput,      // Left operand for each cycle
    RightInstructionInput,     // Right operand for each cycle
    WriteLookupOutputToRD,     // Flag: write lookup result to rd?
    WritePCtoRD,              // Flag: write PC to rd?
    ShouldBranch,             // Flag: branch this cycle?
    ShouldJump,               // Flag: jump this cycle?

    /* Twist/Shout witnesses */
    RdInc,                    // Register increments (1 polynomial)
    RamInc,                   // RAM increments (1 polynomial)

    InstructionRa(usize),     // Instruction lookup addresses (8 polynomials, d=8)
    BytecodeRa(usize),        // Bytecode read addresses (d polynomials, d varies)
    RamRa(usize),             // RAM addresses (d polynomials, d varies)
}
```

**Total count** (for typical execution):
- 6 R1CS polynomials
- 2 increment polynomials (RdInc, RamInc)
- 8 instruction lookup polynomials (InstructionRa)
- ~4 bytecode polynomials (BytecodeRa, if bytecode_d = 4)
- ~4 RAM polynomials (RamRa, if ram_d = 4)
- **Total: ~24 polynomials committed**

### Step 4: Generate Witness from Trace

**Location**: [`jolt-core/src/zkvm/witness.rs:251-450`](../jolt-core/src/zkvm/witness.rs#L251)

```rust
pub fn generate_witness_batch<F, PCS>(
    polynomials: &[CommittedPolynomial],
    preprocessing: &JoltProverPreprocessing<F, PCS>,
    trace: &[Cycle],
) -> HashMap<CommittedPolynomial, MultilinearPolynomial<F>>
{
    let batch = WitnessData::new(trace.len(), ram_d, bytecode_d);

    // Parallel iteration over execution trace
    (0..trace.len()).into_par_iter().for_each({
        let batch_cell = Arc::clone(&batch_cell);
        move |i| {
            let cycle = &trace[i];
            let batch = unsafe { &mut *batch_cell.0.get() };

            // Extract witness data from this cycle:
            batch.left_instruction_input[i] = cycle.get_left_input();
            batch.right_instruction_input[i] = cycle.get_right_input();

            // For RAM operations, record increment and address
            if let Some((address, value_change)) = cycle.ram_operation() {
                batch.ram_inc[i] = value_change;

                // Decompose address into chunks and one-hot encode
                for chunk_idx in 0..ram_d {
                    let chunk = extract_chunk(address, chunk_idx);
                    batch.ram_ra[chunk_idx][i] = Some(chunk);
                }
            }

            // Similar for instruction lookups, bytecode reads, etc.
        }
    });

    // Convert witness data to multilinear polynomials
    let mut result = HashMap::new();
    for poly in polynomials {
        match poly {
            CommittedPolynomial::RamInc => {
                result.insert(*poly, MultilinearPolynomial::from(batch.ram_inc));
            }
            CommittedPolynomial::RamRa(i) => {
                result.insert(*poly, OneHotPolynomial::from(batch.ram_ra[*i]));
            }
            // ... handle all other polynomial types
        }
    }
    result
}
```

**Key insight**: Each execution cycle contributes one entry to each polynomial. For a trace of length $T$, each polynomial has $T$ coefficients (padded to next power of 2).

**Example for our toy add program** (2 cycles):
```rust
// Cycle 0: addi x10, x10, x11
LeftInstructionInput[0]  = value_of_x10
RightInstructionInput[0] = value_of_x11
RamInc[0]                = 0  (no RAM access)
// ...

// Cycle 1: ret
LeftInstructionInput[1]  = PC_value
RightInstructionInput[1] = return_address
RamInc[1]                = 0  (no RAM access)
// ...

// Padded to 4 (next power of 2):
LeftInstructionInput[2]  = 0
LeftInstructionInput[3]  = 0
// ... all polynomials padded similarly
```

### Step 5: Dory Commitment

**Mathematical operation**:
For polynomial $P(x_1, \ldots, x_n)$ with coefficients $[c_0, c_1, \ldots, c_{2^n-1}]$:

$$C = \sum_{i=0}^{2^n-1} c_i \cdot G_i$$

where $G_i$ are elliptic curve points from the Dory SRS (structured reference string).

**Result**: Each commitment is a single elliptic curve point (~32 bytes for BN254).

**Concrete example**:
```
Polynomial: LeftInstructionInput with coefficients [42, 123, 0, 0]
SRS generators: [G_0, G_1, G_2, G_3]

Commitment C = 42·G_0 + 123·G_1 + 0·G_2 + 0·G_3
             = (elliptic curve point)
             = (0x1a2b3c..., 0x4d5e6f...)  // 32 bytes
```

### Summary: Commitment Timeline

**During Preprocessing** (one-time, per program):
-  **Bytecode commitment**: Committed during preprocessing
-  **RAM initial state**: NOT committed (public information)

**During Proving** (per execution):
1. **Generate trace**: Execute program, record all operations
2. **Extract witness**: Convert trace to polynomial coefficients
3. **Commit**: Create ~24 commitments using Dory
4. **Append to transcript**: Bind commitments via Fiat-Shamir
5. **Generate sumcheck proofs**: Prove properties of committed polynomials
6. **Generate opening proof**: Batch opening for all polynomials

**Verifier receives**:
- Preprocessing commitments (1 bytecode commitment)
- Proof commitments (~24 witness polynomial commitments)
- Sumcheck proofs (polynomial evaluations, ~40 sumchecks)
- Final opening proof (single batch opening)

**Total commitment size in proof**: ~24 commitments \times  32 bytes = ~768 bytes
